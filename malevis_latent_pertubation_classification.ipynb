{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535b76d4-545d-445f-bfc1-e4e15f7e2063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with fixed label mapping...\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n",
      "Classes: 26\n",
      "First 5 classes: ['Adposhel', 'Agent', 'Allaple', 'Amonetize', 'Androm']\n",
      "Shapes: (9100, 128, 128, 3) (9100,) (5126, 128, 128, 3) (5126,)\n",
      "\n",
      "================================================================================\n",
      "Starting latent dimension sweep...\n",
      "\n",
      "================================================================================\n",
      "[SWEEP] Latent dim k=8\n",
      "Epoch 1/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 931ms/step - loss: 0.0554 - val_loss: 0.0350\n",
      "Epoch 2/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 842ms/step - loss: 0.2998 - val_loss: 0.0540\n",
      "Epoch 3/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 825ms/step - loss: 0.0560 - val_loss: 0.0539\n",
      "Epoch 4/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 791ms/step - loss: 0.0559 - val_loss: 0.0539\n",
      "Epoch 5/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 810ms/step - loss: 0.0559 - val_loss: 0.0539\n",
      "Epoch 1/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.3296 - loss: 2.4250 - val_accuracy: 0.5665 - val_loss: 1.6003\n",
      "Epoch 2/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6756 - loss: 1.1777 - val_accuracy: 0.6666 - val_loss: 1.3951\n",
      "Epoch 3/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7505 - loss: 0.9075 - val_accuracy: 0.6740 - val_loss: 1.3413\n",
      "Epoch 4/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7836 - loss: 0.7830 - val_accuracy: 0.7031 - val_loss: 1.3159\n",
      "Epoch 5/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7958 - loss: 0.7155 - val_accuracy: 0.7060 - val_loss: 1.2573\n",
      "Epoch 6/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8071 - loss: 0.6674 - val_accuracy: 0.7218 - val_loss: 1.1679\n",
      "Epoch 7/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8181 - loss: 0.6189 - val_accuracy: 0.7113 - val_loss: 1.2524\n",
      "Epoch 8/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8315 - loss: 0.5801 - val_accuracy: 0.7199 - val_loss: 1.1678\n",
      "Epoch 9/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8342 - loss: 0.5521 - val_accuracy: 0.7407 - val_loss: 1.1134\n",
      "Epoch 10/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8352 - loss: 0.5345 - val_accuracy: 0.7298 - val_loss: 1.0879\n",
      "Epoch 11/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8439 - loss: 0.4966 - val_accuracy: 0.7376 - val_loss: 1.1219\n",
      "Epoch 12/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8484 - loss: 0.4822 - val_accuracy: 0.7405 - val_loss: 1.1377\n",
      "Epoch 1/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.2949 - loss: 2.5748 - val_accuracy: 0.4733 - val_loss: 1.9942\n",
      "Epoch 2/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6256 - loss: 1.2974 - val_accuracy: 0.6087 - val_loss: 1.6243\n",
      "Epoch 3/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7166 - loss: 1.0049 - val_accuracy: 0.6479 - val_loss: 1.4795\n",
      "Epoch 4/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7599 - loss: 0.8588 - val_accuracy: 0.6611 - val_loss: 1.4137\n",
      "Epoch 5/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7769 - loss: 0.7721 - val_accuracy: 0.6656 - val_loss: 1.3791\n",
      "Epoch 6/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7923 - loss: 0.7152 - val_accuracy: 0.6779 - val_loss: 1.4098\n",
      "Epoch 7/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8053 - loss: 0.6757 - val_accuracy: 0.6941 - val_loss: 1.3012\n",
      "Epoch 8/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8104 - loss: 0.6303 - val_accuracy: 0.7023 - val_loss: 1.3042\n",
      "Epoch 9/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8228 - loss: 0.6009 - val_accuracy: 0.7019 - val_loss: 1.2824\n",
      "Epoch 10/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8259 - loss: 0.5799 - val_accuracy: 0.7136 - val_loss: 1.2353\n",
      "Epoch 11/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8388 - loss: 0.5315 - val_accuracy: 0.7173 - val_loss: 1.2235\n",
      "Epoch 12/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8406 - loss: 0.5268 - val_accuracy: 0.7111 - val_loss: 1.2196\n",
      "Epoch 13/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8508 - loss: 0.4961 - val_accuracy: 0.7290 - val_loss: 1.2236\n",
      "Epoch 14/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8501 - loss: 0.4819 - val_accuracy: 0.7294 - val_loss: 1.2077\n",
      "Epoch 15/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8554 - loss: 0.4600 - val_accuracy: 0.7308 - val_loss: 1.2332\n",
      "[DONE k=8] clean_avg=0.7834 robust_avg=0.7723 recon=0.058543 kl=85.30 time=22.6 min\n",
      "\n",
      "================================================================================\n",
      "[SWEEP] Latent dim k=16\n",
      "Epoch 1/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 817ms/step - loss: 0.0560 - val_loss: 0.0348\n",
      "Epoch 2/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 792ms/step - loss: 0.4230 - val_loss: 0.0539\n",
      "Epoch 3/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 809ms/step - loss: 0.0560 - val_loss: 0.0539\n",
      "Epoch 4/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 790ms/step - loss: 0.0560 - val_loss: 0.0539\n",
      "Epoch 5/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 801ms/step - loss: 0.0560 - val_loss: 0.0539\n",
      "Epoch 1/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.3605 - loss: 2.3482 - val_accuracy: 0.6100 - val_loss: 1.5639\n",
      "Epoch 2/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7138 - loss: 1.0567 - val_accuracy: 0.6746 - val_loss: 1.3010\n",
      "Epoch 3/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7731 - loss: 0.8345 - val_accuracy: 0.7043 - val_loss: 1.1981\n",
      "Epoch 4/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7973 - loss: 0.7166 - val_accuracy: 0.7273 - val_loss: 1.1235\n",
      "Epoch 5/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8158 - loss: 0.6493 - val_accuracy: 0.7304 - val_loss: 1.0560\n",
      "Epoch 6/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8267 - loss: 0.5858 - val_accuracy: 0.7199 - val_loss: 1.1495\n",
      "Epoch 7/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8393 - loss: 0.5520 - val_accuracy: 0.7339 - val_loss: 1.0943\n",
      "Epoch 8/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8468 - loss: 0.5223 - val_accuracy: 0.7499 - val_loss: 1.0882\n",
      "Epoch 9/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8548 - loss: 0.4805 - val_accuracy: 0.7442 - val_loss: 1.0947\n",
      "Epoch 10/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8654 - loss: 0.4592 - val_accuracy: 0.7481 - val_loss: 1.0770\n",
      "Epoch 11/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8733 - loss: 0.4301 - val_accuracy: 0.7462 - val_loss: 1.1272\n",
      "Epoch 1/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.2781 - loss: 2.4967 - val_accuracy: 0.5347 - val_loss: 2.0625\n",
      "Epoch 2/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6368 - loss: 1.2558 - val_accuracy: 0.5743 - val_loss: 1.8482\n",
      "Epoch 3/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.7148 - loss: 1.0149 - val_accuracy: 0.6258 - val_loss: 1.5537\n",
      "Epoch 4/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.7471 - loss: 0.8784 - val_accuracy: 0.6405 - val_loss: 1.5182\n",
      "Epoch 5/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7629 - loss: 0.8082 - val_accuracy: 0.6572 - val_loss: 1.4629\n",
      "Epoch 6/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.7884 - loss: 0.7267 - val_accuracy: 0.6619 - val_loss: 1.4294\n",
      "Epoch 7/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7946 - loss: 0.6932 - val_accuracy: 0.6715 - val_loss: 1.3950\n",
      "Epoch 8/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8039 - loss: 0.6664 - val_accuracy: 0.6996 - val_loss: 1.2928\n",
      "Epoch 9/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8146 - loss: 0.6185 - val_accuracy: 0.7004 - val_loss: 1.2717\n",
      "Epoch 10/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.8205 - loss: 0.5972 - val_accuracy: 0.7070 - val_loss: 1.2818\n",
      "Epoch 11/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8248 - loss: 0.5740 - val_accuracy: 0.7232 - val_loss: 1.2141\n",
      "Epoch 12/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8259 - loss: 0.5563 - val_accuracy: 0.7183 - val_loss: 1.2220\n",
      "Epoch 13/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8406 - loss: 0.5206 - val_accuracy: 0.7226 - val_loss: 1.2609\n",
      "Epoch 14/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8430 - loss: 0.5034 - val_accuracy: 0.7208 - val_loss: 1.2663\n",
      "[DONE k=16] clean_avg=0.7843 robust_avg=0.7759 recon=0.059056 kl=157.52 time=21.5 min\n",
      "\n",
      "================================================================================\n",
      "[SWEEP] Latent dim k=32\n",
      "Epoch 1/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 806ms/step - loss: 0.0546 - val_loss: 0.0343\n",
      "Epoch 2/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 818ms/step - loss: 0.6586 - val_loss: 0.0540\n",
      "Epoch 3/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 798ms/step - loss: 0.0561 - val_loss: 0.0539\n",
      "Epoch 4/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 806ms/step - loss: 0.0560 - val_loss: 0.0539\n",
      "Epoch 5/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 800ms/step - loss: 0.0560 - val_loss: 0.0540\n",
      "Epoch 1/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.3335 - loss: 2.4285 - val_accuracy: 0.5669 - val_loss: 1.5946\n",
      "Epoch 2/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.6970 - loss: 1.0932 - val_accuracy: 0.6568 - val_loss: 1.3293\n",
      "Epoch 3/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7649 - loss: 0.8446 - val_accuracy: 0.6851 - val_loss: 1.2567\n",
      "Epoch 4/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7891 - loss: 0.7389 - val_accuracy: 0.7023 - val_loss: 1.1738\n",
      "Epoch 5/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8105 - loss: 0.6621 - val_accuracy: 0.7173 - val_loss: 1.1458\n",
      "Epoch 6/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8255 - loss: 0.5963 - val_accuracy: 0.7323 - val_loss: 1.1029\n",
      "Epoch 7/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8320 - loss: 0.5649 - val_accuracy: 0.7144 - val_loss: 1.1142\n",
      "Epoch 8/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8446 - loss: 0.5279 - val_accuracy: 0.7171 - val_loss: 1.1081\n",
      "Epoch 9/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8527 - loss: 0.5026 - val_accuracy: 0.7306 - val_loss: 1.1142\n",
      "Epoch 1/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 25ms/step - accuracy: 0.2518 - loss: 2.6579 - val_accuracy: 0.4754 - val_loss: 1.9310\n",
      "Epoch 2/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.6184 - loss: 1.3836 - val_accuracy: 0.5819 - val_loss: 1.6521\n",
      "Epoch 3/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6955 - loss: 1.0990 - val_accuracy: 0.5948 - val_loss: 1.5451\n",
      "Epoch 4/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.7412 - loss: 0.9368 - val_accuracy: 0.6467 - val_loss: 1.4999\n",
      "Epoch 5/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.7602 - loss: 0.8341 - val_accuracy: 0.6284 - val_loss: 1.5370\n",
      "Epoch 6/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7808 - loss: 0.7673 - val_accuracy: 0.6836 - val_loss: 1.3775\n",
      "Epoch 7/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.7994 - loss: 0.7041 - val_accuracy: 0.6845 - val_loss: 1.3730\n",
      "Epoch 8/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.8031 - loss: 0.6805 - val_accuracy: 0.6998 - val_loss: 1.3638\n",
      "Epoch 9/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.8148 - loss: 0.6274 - val_accuracy: 0.7011 - val_loss: 1.3920\n",
      "Epoch 10/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - accuracy: 0.8206 - loss: 0.6006 - val_accuracy: 0.6935 - val_loss: 1.3449\n",
      "Epoch 11/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.8270 - loss: 0.5798 - val_accuracy: 0.7107 - val_loss: 1.2975\n",
      "Epoch 12/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.8354 - loss: 0.5552 - val_accuracy: 0.7054 - val_loss: 1.2809\n",
      "Epoch 13/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.8398 - loss: 0.5371 - val_accuracy: 0.7142 - val_loss: 1.2231\n",
      "Epoch 14/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.8415 - loss: 0.5194 - val_accuracy: 0.7093 - val_loss: 1.2375\n",
      "Epoch 15/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.8499 - loss: 0.5043 - val_accuracy: 0.7013 - val_loss: 1.2934\n",
      "[DONE k=32] clean_avg=0.7742 robust_avg=0.7578 recon=0.059228 kl=210.91 time=22.4 min\n",
      "\n",
      "================================================================================\n",
      "[SWEEP] Latent dim k=64\n",
      "Epoch 1/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 926ms/step - loss: 0.0541 - val_loss: 0.0342\n",
      "Epoch 2/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 813ms/step - loss: 0.9515 - val_loss: 0.0540\n",
      "Epoch 3/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 822ms/step - loss: 0.0561 - val_loss: 0.0540\n",
      "Epoch 4/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 794ms/step - loss: 0.0560 - val_loss: 0.0540\n",
      "Epoch 5/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 793ms/step - loss: 0.0560 - val_loss: 0.0540\n",
      "Epoch 1/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.3266 - loss: 2.4772 - val_accuracy: 0.5759 - val_loss: 1.5365\n",
      "Epoch 2/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.6934 - loss: 1.0892 - val_accuracy: 0.6773 - val_loss: 1.1976\n",
      "Epoch 3/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.7545 - loss: 0.8562 - val_accuracy: 0.6912 - val_loss: 1.1694\n",
      "Epoch 4/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.7869 - loss: 0.7244 - val_accuracy: 0.7173 - val_loss: 1.1294\n",
      "Epoch 5/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.8150 - loss: 0.6335 - val_accuracy: 0.7234 - val_loss: 1.1116\n",
      "Epoch 6/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8284 - loss: 0.5825 - val_accuracy: 0.7257 - val_loss: 1.1083\n",
      "Epoch 7/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8377 - loss: 0.5567 - val_accuracy: 0.7536 - val_loss: 0.9916\n",
      "Epoch 8/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8539 - loss: 0.5090 - val_accuracy: 0.7306 - val_loss: 1.1254\n",
      "Epoch 9/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8586 - loss: 0.4817 - val_accuracy: 0.7497 - val_loss: 0.9747\n",
      "Epoch 10/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8624 - loss: 0.4557 - val_accuracy: 0.7581 - val_loss: 1.0149\n",
      "Epoch 11/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8674 - loss: 0.4440 - val_accuracy: 0.7655 - val_loss: 1.0058\n",
      "Epoch 12/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.8771 - loss: 0.4097 - val_accuracy: 0.7491 - val_loss: 1.1392\n",
      "Epoch 13/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8771 - loss: 0.4013 - val_accuracy: 0.7634 - val_loss: 0.9834\n",
      "Epoch 14/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8808 - loss: 0.3810 - val_accuracy: 0.7563 - val_loss: 1.1171\n",
      "Epoch 1/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 50ms/step - accuracy: 0.2993 - loss: 2.5414 - val_accuracy: 0.4705 - val_loss: 1.9545\n",
      "Epoch 2/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - accuracy: 0.6520 - loss: 1.2577 - val_accuracy: 0.6092 - val_loss: 1.5618\n",
      "Epoch 3/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - accuracy: 0.7274 - loss: 0.9742 - val_accuracy: 0.6444 - val_loss: 1.4391\n",
      "Epoch 4/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - accuracy: 0.7455 - loss: 0.8693 - val_accuracy: 0.6479 - val_loss: 1.4592\n",
      "Epoch 5/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 48ms/step - accuracy: 0.7776 - loss: 0.7866 - val_accuracy: 0.6410 - val_loss: 1.5000\n",
      "Epoch 6/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 48ms/step - accuracy: 0.7864 - loss: 0.7456 - val_accuracy: 0.6652 - val_loss: 1.4042\n",
      "Epoch 7/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 48ms/step - accuracy: 0.8014 - loss: 0.6943 - val_accuracy: 0.6726 - val_loss: 1.3375\n",
      "Epoch 8/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - accuracy: 0.8077 - loss: 0.6520 - val_accuracy: 0.6662 - val_loss: 1.4032\n",
      "Epoch 9/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - accuracy: 0.7986 - loss: 0.6787 - val_accuracy: 0.6890 - val_loss: 1.3205\n",
      "Epoch 10/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 50ms/step - accuracy: 0.8195 - loss: 0.6016 - val_accuracy: 0.7072 - val_loss: 1.2499\n",
      "Epoch 11/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - accuracy: 0.8291 - loss: 0.5776 - val_accuracy: 0.6883 - val_loss: 1.3043\n",
      "Epoch 12/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 50ms/step - accuracy: 0.8320 - loss: 0.5544 - val_accuracy: 0.6830 - val_loss: 1.3418\n",
      "Epoch 13/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.8342 - loss: 0.5335 - val_accuracy: 0.7089 - val_loss: 1.3149\n",
      "Epoch 14/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 54ms/step - accuracy: 0.8431 - loss: 0.5086 - val_accuracy: 0.7181 - val_loss: 1.2167\n",
      "Epoch 15/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 54ms/step - accuracy: 0.8482 - loss: 0.5030 - val_accuracy: 0.7099 - val_loss: 1.2677\n",
      "[DONE k=64] clean_avg=0.7955 robust_avg=0.7761 recon=0.059476 kl=355.28 time=25.4 min\n",
      "\n",
      "================================================================================\n",
      "[SWEEP] Latent dim k=128\n",
      "Epoch 1/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 843ms/step - loss: 0.0535 - val_loss: 0.0346\n",
      "Epoch 2/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 835ms/step - loss: 1.1478 - val_loss: 0.0542\n",
      "Epoch 3/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 823ms/step - loss: 0.0563 - val_loss: 0.0539\n",
      "Epoch 4/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 818ms/step - loss: 0.0561 - val_loss: 0.0541\n",
      "Epoch 5/25\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 804ms/step - loss: 0.0561 - val_loss: 0.0541\n",
      "Epoch 1/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.2951 - loss: 2.5358 - val_accuracy: 0.5870 - val_loss: 1.5973\n",
      "Epoch 2/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.6959 - loss: 1.1186 - val_accuracy: 0.6422 - val_loss: 1.3599\n",
      "Epoch 3/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.7660 - loss: 0.8512 - val_accuracy: 0.6567 - val_loss: 1.3154\n",
      "Epoch 4/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.7907 - loss: 0.7492 - val_accuracy: 0.6797 - val_loss: 1.2108\n",
      "Epoch 5/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.8096 - loss: 0.6581 - val_accuracy: 0.6951 - val_loss: 1.1524\n",
      "Epoch 6/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.8272 - loss: 0.6064 - val_accuracy: 0.6992 - val_loss: 1.1934\n",
      "Epoch 7/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.8372 - loss: 0.5639 - val_accuracy: 0.6906 - val_loss: 1.2542\n",
      "Epoch 8/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.8439 - loss: 0.5387 - val_accuracy: 0.7314 - val_loss: 1.0752\n",
      "Epoch 9/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.8574 - loss: 0.4939 - val_accuracy: 0.7320 - val_loss: 1.1069\n",
      "Epoch 10/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.8578 - loss: 0.4792 - val_accuracy: 0.7085 - val_loss: 1.2060\n",
      "Epoch 11/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.8686 - loss: 0.4323 - val_accuracy: 0.7000 - val_loss: 1.2650\n",
      "Epoch 12/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.8739 - loss: 0.4250 - val_accuracy: 0.7199 - val_loss: 1.2310\n",
      "Epoch 1/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 95ms/step - accuracy: 0.2492 - loss: 2.6423 - val_accuracy: 0.4694 - val_loss: 2.1384\n",
      "Epoch 2/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 93ms/step - accuracy: 0.6095 - loss: 1.4207 - val_accuracy: 0.5392 - val_loss: 1.8374\n",
      "Epoch 3/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 94ms/step - accuracy: 0.6849 - loss: 1.1119 - val_accuracy: 0.5790 - val_loss: 1.6811\n",
      "Epoch 4/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 94ms/step - accuracy: 0.7090 - loss: 1.0097 - val_accuracy: 0.6149 - val_loss: 1.5295\n",
      "Epoch 5/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 94ms/step - accuracy: 0.7465 - loss: 0.8969 - val_accuracy: 0.6011 - val_loss: 1.6280\n",
      "Epoch 6/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 94ms/step - accuracy: 0.7562 - loss: 0.8523 - val_accuracy: 0.6426 - val_loss: 1.4773\n",
      "Epoch 7/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 95ms/step - accuracy: 0.7579 - loss: 0.8382 - val_accuracy: 0.6465 - val_loss: 1.5079\n",
      "Epoch 8/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 96ms/step - accuracy: 0.7847 - loss: 0.7416 - val_accuracy: 0.6451 - val_loss: 1.4827\n",
      "Epoch 9/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - accuracy: 0.7809 - loss: 0.7333 - val_accuracy: 0.6535 - val_loss: 1.5767\n",
      "Epoch 10/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 94ms/step - accuracy: 0.8024 - loss: 0.6782 - val_accuracy: 0.6580 - val_loss: 1.4472\n",
      "Epoch 11/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 96ms/step - accuracy: 0.8076 - loss: 0.6580 - val_accuracy: 0.6789 - val_loss: 1.4382\n",
      "Epoch 12/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 97ms/step - accuracy: 0.8170 - loss: 0.6360 - val_accuracy: 0.6801 - val_loss: 1.2996\n",
      "Epoch 13/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 95ms/step - accuracy: 0.8148 - loss: 0.6080 - val_accuracy: 0.6785 - val_loss: 1.4301\n",
      "Epoch 14/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 95ms/step - accuracy: 0.8221 - loss: 0.5885 - val_accuracy: 0.6945 - val_loss: 1.3584\n",
      "Epoch 15/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 94ms/step - accuracy: 0.8247 - loss: 0.5841 - val_accuracy: 0.6842 - val_loss: 1.3428\n",
      "[DONE k=128] clean_avg=0.7739 robust_avg=0.7574 recon=0.059428 kl=453.44 time=28.8 min\n",
      "\n",
      "=== SWEEP RESULTS ===\n",
      "  k  recon_mse_val(sub256)  kl_val(sub256)  clean_macroF1_avg  robust_macroF1_avg  time_min\n",
      "  8               0.058543       85.303406           0.783440            0.772314 22.588615\n",
      " 16               0.059056      157.518250           0.784303            0.775916 21.510953\n",
      " 32               0.059228      210.905777           0.774243            0.757765 22.424136\n",
      " 64               0.059476      355.282867           0.795546            0.776145 25.407922\n",
      "128               0.059428      453.440613           0.773903            0.757400 28.845904\n",
      "\n",
      "Saved: latent_dim_sweep_results.csv\n",
      "\n",
      "=== SELECTED k ===\n",
      "k                         64.000000\n",
      "recon_mse_val(sub256)      0.059476\n",
      "kl_val(sub256)           355.282867\n",
      "clean_macroF1_avg          0.795546\n",
      "robust_macroF1_avg         0.776145\n",
      "time_min                  25.407922\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# >>> CHANGE THIS to your local folder that contains \"train\" and \"val\"\n",
    "PATH_ROOT = r\"C:\\Users\\ajayi\\Downloads\\malevis_train_val_300x300\\malevis_train_val_300x300\"\n",
    "TRAIN_DIR = os.path.join(PATH_ROOT, \"train\")\n",
    "VAL_DIR   = os.path.join(PATH_ROOT, \"val\")\n",
    "\n",
    "IMG_H, IMG_W, CH = 128, 128, 3\n",
    "\n",
    "LATENT_DIMS = [8, 16, 32, 64, 128]\n",
    "\n",
    "VAE_EPOCHS = 25\n",
    "VAE_BS = 32\n",
    "VAE_LR = 3e-4\n",
    "\n",
    "BETA_MAX = 1.0\n",
    "BETA_WARMUP_EPOCHS = 8\n",
    "\n",
    "CLS_EPOCHS = 15\n",
    "CLS_BS = 32\n",
    "CLS_LR = 1e-3\n",
    "\n",
    "NOISE_GAUSS_SIG_LAT = 0.10\n",
    "\n",
    "# GPU memory growth\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Dataset loader with FIXED label mapping\n",
    "# -----------------------------\n",
    "def make_ds_fixed(train_dir, val_dir, img_size, batch_size, seed):\n",
    "    # 1) Build train first to get canonical class order\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "    # 2) Force VAL to use SAME mapping (THIS is the correct API)\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        class_names=class_names,\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    def norm(x, y):\n",
    "        x = tf.cast(x, tf.float32) / 255.0\n",
    "        return x, y\n",
    "\n",
    "    train_ds = train_ds.map(norm, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = val_ds.map(norm,   num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds, class_names, len(class_names)\n",
    "\n",
    "def ds_to_numpy(ds):\n",
    "    xs, ys = [], []\n",
    "    for xb, yb in ds:\n",
    "        xs.append(xb.numpy())\n",
    "        ys.append(yb.numpy())\n",
    "    X = np.concatenate(xs, axis=0).astype(np.float32)\n",
    "    y = np.concatenate(ys, axis=0).astype(np.int64)\n",
    "    return X, y\n",
    "\n",
    "# -----------------------------\n",
    "# 2) VAE pieces (Keras-safe KL via add_loss)\n",
    "# -----------------------------\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "class KLDivergenceLayer(layers.Layer):\n",
    "    def __init__(self, beta_init=0.0, logvar_clip=10.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.beta = tf.Variable(beta_init, trainable=False, dtype=tf.float32, name=\"beta\")\n",
    "        self.logvar_clip = float(logvar_clip)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        z_log_var = tf.clip_by_value(z_log_var, -self.logvar_clip, self.logvar_clip)\n",
    "\n",
    "        kl_per_sample = -0.5 * tf.reduce_sum(\n",
    "            1.0 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var),\n",
    "            axis=1\n",
    "        )\n",
    "        kl = tf.reduce_mean(kl_per_sample)\n",
    "        self.add_loss(self.beta * kl)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "class BetaWarmup(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, beta_max=1.0, warmup_epochs=10, layer_name=\"kl\"):\n",
    "        super().__init__()\n",
    "        self.beta_max = float(beta_max)\n",
    "        self.warmup_epochs = int(max(1, warmup_epochs))\n",
    "        self.layer_name = layer_name\n",
    "        self._kl_layer = None\n",
    "\n",
    "    def _find_layer_recursive(self, model, name):\n",
    "        # Search direct children\n",
    "        for layer in model.layers:\n",
    "            if layer.name == name:\n",
    "                return layer\n",
    "        # Search nested models\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, tf.keras.Model):\n",
    "                found = self._find_layer_recursive(layer, name)\n",
    "                if found is not None:\n",
    "                    return found\n",
    "        return None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self._kl_layer = self._find_layer_recursive(self.model, self.layer_name)\n",
    "        if self._kl_layer is None:\n",
    "            raise ValueError(\n",
    "                f\"Could not find layer '{self.layer_name}' in model or nested submodels. \"\n",
    "                f\"Top-level layers: {[l.name for l in self.model.layers]}\"\n",
    "            )\n",
    "        # Sanity: must have beta variable\n",
    "        if not hasattr(self._kl_layer, \"beta\"):\n",
    "            raise ValueError(f\"Layer '{self.layer_name}' found but has no attribute 'beta'.\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        beta = self.beta_max * min(1.0, epoch / float(self.warmup_epochs))\n",
    "        self._kl_layer.beta.assign(beta)\n",
    "\n",
    "\n",
    "def build_cnn_vae(input_shape, latent_dim, beta_init=0.0):\n",
    "    enc_in = layers.Input(shape=input_shape, name=\"vae_input\")\n",
    "\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\", activation=\"relu\")(enc_in)\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "\n",
    "    z_mean, z_log_var = KLDivergenceLayer(beta_init=beta_init, name=\"kl\")([z_mean, z_log_var])\n",
    "    z = Sampling(name=\"z_sample\")([z_mean, z_log_var])\n",
    "\n",
    "    encoder = Model(enc_in, [z_mean, z_log_var, z], name=f\"encoder_k{latent_dim}\")\n",
    "\n",
    "    # Decoder\n",
    "    h4, w4 = input_shape[0] // 16, input_shape[1] // 16\n",
    "    dec_in = layers.Input(shape=(latent_dim,), name=\"dec_input\")\n",
    "\n",
    "    x = layers.Dense(h4 * w4 * 256, activation=\"relu\")(dec_in)\n",
    "    x = layers.Reshape((h4, w4, 256))(x)\n",
    "    x = layers.Conv2DTranspose(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    dec_out = layers.Conv2D(input_shape[2], 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "\n",
    "    decoder = Model(dec_in, dec_out, name=f\"decoder_k{latent_dim}\")\n",
    "\n",
    "    z_mean_, z_log_var_, z_ = encoder(enc_in)\n",
    "    recon = decoder(z_)\n",
    "    vae = Model(enc_in, recon, name=f\"vae_k{latent_dim}\")\n",
    "\n",
    "    return encoder, decoder, vae\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Latent classifiers\n",
    "# -----------------------------\n",
    "def build_latent_cnn(latent_dim, n_classes, lr=1e-3):\n",
    "    inp = layers.Input(shape=(latent_dim, 1))\n",
    "    x = layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    m = Model(inp, out, name=f\"latent_cnn_k{latent_dim}\")\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
    "              loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "def build_latent_rnn(latent_dim, n_classes, lr=1e-3):\n",
    "    inp = layers.Input(shape=(latent_dim, 1))\n",
    "    x = layers.Bidirectional(layers.LSTM(96, return_sequences=False))(inp)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(192, activation=\"relu\")(x)\n",
    "    out = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    m = Model(inp, out, name=f\"latent_rnn_k{latent_dim}\")\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
    "              loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Helpers\n",
    "# -----------------------------\n",
    "def get_latent_zmean(encoder, X, batch_size=64):\n",
    "    z_list = []\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        z_mean, z_logv, z_samp = encoder.predict(X[i:i+batch_size], verbose=0)\n",
    "        z_list.append(z_mean.astype(np.float32))\n",
    "    return np.vstack(z_list)\n",
    "\n",
    "def eval_macro_f1(model, X, y_true):\n",
    "    probs = model.predict(X, verbose=0)\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return acc, f1m\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Load ds + convert to numpy\n",
    "# -----------------------------\n",
    "print(\"Loading datasets with fixed label mapping...\")\n",
    "train_ds, val_ds, class_names, num_classes = make_ds_fixed(\n",
    "    TRAIN_DIR, VAL_DIR, (IMG_H, IMG_W), batch_size=VAE_BS, seed=SEED\n",
    ")\n",
    "print(\"Classes:\", num_classes)\n",
    "print(\"First 5 classes:\", class_names[:5])\n",
    "\n",
    "X_train, y_train = ds_to_numpy(train_ds)\n",
    "X_val, y_val     = ds_to_numpy(val_ds)\n",
    "print(\"Shapes:\", X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Sweep\n",
    "# -----------------------------\n",
    "def train_one_k(latent_dim):\n",
    "    t0 = time.time()\n",
    "\n",
    "    encoder, decoder, vae = build_cnn_vae((IMG_H, IMG_W, CH), latent_dim, beta_init=0.0)\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(VAE_LR),\n",
    "                loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "    vae.fit(\n",
    "        X_train, X_train,\n",
    "        validation_data=(X_val, X_val),\n",
    "        epochs=VAE_EPOCHS, batch_size=VAE_BS, verbose=1,\n",
    "        callbacks=[\n",
    "            BetaWarmup(beta_max=BETA_MAX, warmup_epochs=BETA_WARMUP_EPOCHS, layer_name=\"kl\"),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # quick recon/kl on subset\n",
    "    sub = min(256, len(X_val))\n",
    "    z_mean_val, z_logv_val, _ = encoder.predict(X_val[:sub], verbose=0)\n",
    "    z_logv_val = np.clip(z_logv_val, -10, 10)\n",
    "    kl_val = float(np.mean(-0.5 * np.sum(1.0 + z_logv_val - z_mean_val**2 - np.exp(z_logv_val), axis=1)))\n",
    "\n",
    "    X_recon = vae.predict(X_val[:sub], verbose=0)\n",
    "    recon_mse = float(np.mean((X_val[:sub] - X_recon) ** 2))\n",
    "\n",
    "    # latent\n",
    "    z_train = get_latent_zmean(encoder, X_train, batch_size=64)\n",
    "    z_val   = get_latent_zmean(encoder, X_val,   batch_size=64)\n",
    "\n",
    "    z_train_seq = z_train.reshape((-1, latent_dim, 1)).astype(np.float32)\n",
    "    z_val_seq   = z_val.reshape((-1, latent_dim, 1)).astype(np.float32)\n",
    "\n",
    "    z_lo = float(np.percentile(z_train, 0.5))\n",
    "    z_hi = float(np.percentile(z_train, 99.5))\n",
    "\n",
    "    # heads\n",
    "    m_cnn = build_latent_cnn(latent_dim, num_classes, lr=CLS_LR)\n",
    "    m_rnn = build_latent_rnn(latent_dim, num_classes, lr=CLS_LR)\n",
    "\n",
    "    m_cnn.fit(\n",
    "        z_train_seq, y_train,\n",
    "        validation_data=(z_val_seq, y_val),\n",
    "        epochs=CLS_EPOCHS, batch_size=CLS_BS, verbose=1,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True)]\n",
    "    )\n",
    "    m_rnn.fit(\n",
    "        z_train_seq, y_train,\n",
    "        validation_data=(z_val_seq, y_val),\n",
    "        epochs=CLS_EPOCHS, batch_size=CLS_BS, verbose=1,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True)]\n",
    "    )\n",
    "\n",
    "    # clean\n",
    "    _, f1_cnn = eval_macro_f1(m_cnn, z_val_seq, y_val)\n",
    "    _, f1_rnn = eval_macro_f1(m_rnn, z_val_seq, y_val)\n",
    "    clean_avg = 0.5 * (f1_cnn + f1_rnn)\n",
    "\n",
    "    # robust (latent gaussian)\n",
    "    z_val_noisy = z_val_seq + np.random.normal(0, NOISE_GAUSS_SIG_LAT, size=z_val_seq.shape).astype(np.float32)\n",
    "    z_val_noisy = np.clip(z_val_noisy, z_lo, z_hi).astype(np.float32)\n",
    "\n",
    "    _, f1_cnn_r = eval_macro_f1(m_cnn, z_val_noisy, y_val)\n",
    "    _, f1_rnn_r = eval_macro_f1(m_rnn, z_val_noisy, y_val)\n",
    "    robust_avg = 0.5 * (f1_cnn_r + f1_rnn_r)\n",
    "\n",
    "    elapsed_min = (time.time() - t0) / 60.0\n",
    "\n",
    "    return {\n",
    "        \"k\": latent_dim,\n",
    "        \"recon_mse_val(sub256)\": recon_mse,\n",
    "        \"kl_val(sub256)\": kl_val,\n",
    "        \"clean_macroF1_avg\": clean_avg,\n",
    "        \"robust_macroF1_avg\": robust_avg,\n",
    "        \"time_min\": elapsed_min\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting latent dimension sweep...\")\n",
    "for k in LATENT_DIMS:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"[SWEEP] Latent dim k={k}\")\n",
    "    row = train_one_k(k)\n",
    "    rows.append(row)\n",
    "    print(f\"[DONE k={k}] clean_avg={row['clean_macroF1_avg']:.4f} \"\n",
    "          f\"robust_avg={row['robust_macroF1_avg']:.4f} \"\n",
    "          f\"recon={row['recon_mse_val(sub256)']:.6f} \"\n",
    "          f\"kl={row['kl_val(sub256)']:.2f} time={row['time_min']:.1f} min\")\n",
    "\n",
    "sweep_df = pd.DataFrame(rows).sort_values(\"k\")\n",
    "print(\"\\n=== SWEEP RESULTS ===\")\n",
    "print(sweep_df.to_string(index=False))\n",
    "sweep_df.to_csv(\"latent_dim_sweep_results.csv\", index=False)\n",
    "print(\"\\nSaved: latent_dim_sweep_results.csv\")\n",
    "\n",
    "best = sweep_df.sort_values([\"robust_macroF1_avg\", \"clean_macroF1_avg\"], ascending=[False, False]).iloc[0]\n",
    "print(\"\\n=== SELECTED k ===\")\n",
    "print(best.to_string())\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(sweep_df[\"k\"], sweep_df[\"clean_macroF1_avg\"], marker=\"o\", label=\"Clean Macro-F1 (avg)\")\n",
    "plt.plot(sweep_df[\"k\"], sweep_df[\"robust_macroF1_avg\"], marker=\"o\", label=\"Robust Macro-F1 (avg)\")\n",
    "plt.xscale(\"log\", base=2)\n",
    "plt.xticks(LATENT_DIMS, LATENT_DIMS)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Latent dim sweep: clean vs robust Macro-F1\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5283e1-3769-4317-b8ed-bbb0ec05c588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with fixed label mapping...\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n",
      "Classes: 26\n",
      "First 5 classes: ['Adposhel', 'Agent', 'Allaple', 'Amonetize', 'Androm']\n",
      "\n",
      "Training VAE...\n",
      "Epoch 1/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 353ms/step - kl: 2126.3662 - loss: 2210.0642 - recon_sse: 2210.0642 - val_kl: 855.5303 - val_loss: 1460.1569 - val_recon_sse: 1460.1569\n",
      "Epoch 2/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 360ms/step - kl: 214.6332 - loss: 1423.1990 - recon_sse: 1401.7358 - val_kl: 129.7855 - val_loss: 1331.0577 - val_recon_sse: 1318.0806\n",
      "Epoch 3/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 346ms/step - kl: 118.5630 - loss: 1226.2263 - recon_sse: 1202.5138 - val_kl: 111.7611 - val_loss: 1209.7183 - val_recon_sse: 1187.3665\n",
      "Epoch 4/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 342ms/step - kl: 99.6874 - loss: 1110.9768 - recon_sse: 1081.0704 - val_kl: 94.6058 - val_loss: 1137.2623 - val_recon_sse: 1108.8807\n",
      "Epoch 5/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 346ms/step - kl: 87.8507 - loss: 1044.6086 - recon_sse: 1009.4686 - val_kl: 79.4689 - val_loss: 1126.4835 - val_recon_sse: 1094.6957\n",
      "Epoch 6/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 345ms/step - kl: 78.5246 - loss: 1008.7897 - recon_sse: 969.5273 - val_kl: 72.7032 - val_loss: 1121.5265 - val_recon_sse: 1085.1752\n",
      "Epoch 7/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 383ms/step - kl: 71.2377 - loss: 984.2439 - recon_sse: 941.5012 - val_kl: 67.2911 - val_loss: 1124.8201 - val_recon_sse: 1084.4462\n",
      "Epoch 8/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 357ms/step - kl: 67.8685 - loss: 981.6104 - recon_sse: 934.1022 - val_kl: 66.8811 - val_loss: 1105.6581 - val_recon_sse: 1058.8422\n",
      "Epoch 9/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 344ms/step - kl: 62.1042 - loss: 962.1017 - recon_sse: 912.4185 - val_kl: 58.7025 - val_loss: 1105.9420 - val_recon_sse: 1058.9803\n",
      "Epoch 10/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 346ms/step - kl: 58.5160 - loss: 961.2640 - recon_sse: 908.5997 - val_kl: 56.0474 - val_loss: 1087.6200 - val_recon_sse: 1037.1782\n",
      "Epoch 11/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 352ms/step - kl: 55.4901 - loss: 949.3594 - recon_sse: 893.8696 - val_kl: 51.8345 - val_loss: 1085.8669 - val_recon_sse: 1034.0322\n",
      "Epoch 12/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 349ms/step - kl: 53.0630 - loss: 937.4901 - recon_sse: 884.4274 - val_kl: 52.8691 - val_loss: 1081.2987 - val_recon_sse: 1028.4296\n",
      "Epoch 13/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 347ms/step - kl: 51.7850 - loss: 930.6597 - recon_sse: 878.8749 - val_kl: 50.1282 - val_loss: 1065.8478 - val_recon_sse: 1015.7196\n",
      "Epoch 14/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 349ms/step - kl: 51.5495 - loss: 925.2817 - recon_sse: 873.7322 - val_kl: 49.0336 - val_loss: 1064.7202 - val_recon_sse: 1015.6860\n",
      "Epoch 15/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 347ms/step - kl: 50.2459 - loss: 913.4731 - recon_sse: 863.2268 - val_kl: 47.2346 - val_loss: 1069.0813 - val_recon_sse: 1021.8463\n",
      "Epoch 16/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 346ms/step - kl: 49.6355 - loss: 906.1553 - recon_sse: 856.5194 - val_kl: 50.1734 - val_loss: 1058.7301 - val_recon_sse: 1008.5576\n",
      "Epoch 17/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 349ms/step - kl: 49.3047 - loss: 900.8084 - recon_sse: 851.5036 - val_kl: 47.8227 - val_loss: 1048.3409 - val_recon_sse: 1000.5173\n",
      "Epoch 18/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 350ms/step - kl: 48.6373 - loss: 893.2634 - recon_sse: 844.6262 - val_kl: 50.9898 - val_loss: 1055.9086 - val_recon_sse: 1004.9184\n",
      "Epoch 19/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 350ms/step - kl: 48.2412 - loss: 887.1561 - recon_sse: 838.9148 - val_kl: 47.4009 - val_loss: 1046.9197 - val_recon_sse: 999.5184\n",
      "Epoch 20/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 352ms/step - kl: 47.9293 - loss: 886.4802 - recon_sse: 838.5510 - val_kl: 46.3018 - val_loss: 1047.1411 - val_recon_sse: 1000.8399\n",
      "Epoch 21/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 356ms/step - kl: 47.7956 - loss: 880.7002 - recon_sse: 832.9048 - val_kl: 47.7696 - val_loss: 1039.5240 - val_recon_sse: 991.7544\n",
      "Epoch 22/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 350ms/step - kl: 47.2057 - loss: 873.5887 - recon_sse: 826.3833 - val_kl: 46.9716 - val_loss: 1036.5714 - val_recon_sse: 989.5992\n",
      "Epoch 23/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 354ms/step - kl: 47.2065 - loss: 873.4264 - recon_sse: 826.2200 - val_kl: 46.6919 - val_loss: 1032.2368 - val_recon_sse: 985.5452\n",
      "Epoch 24/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 351ms/step - kl: 47.0107 - loss: 869.1382 - recon_sse: 822.1276 - val_kl: 46.5537 - val_loss: 1034.8579 - val_recon_sse: 988.3032\n",
      "Epoch 25/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 357ms/step - kl: 46.9236 - loss: 870.3353 - recon_sse: 823.4119 - val_kl: 45.6607 - val_loss: 1023.7556 - val_recon_sse: 978.0955\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n",
      "\n",
      "Latent clip bounds: -2.4984171390533447 2.536536604166031\n",
      "\n",
      "Training Latent CNN...\n",
      "Epoch 1/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 190ms/step - accuracy: 0.1944 - loss: 2.9683 - val_accuracy: 0.4370 - val_loss: 2.0764\n",
      "Epoch 2/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.5758 - loss: 1.6013 - val_accuracy: 0.6182 - val_loss: 1.4646\n",
      "Epoch 3/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6972 - loss: 1.1371 - val_accuracy: 0.6403 - val_loss: 1.2942\n",
      "Epoch 4/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7360 - loss: 0.9608 - val_accuracy: 0.6951 - val_loss: 1.1366\n",
      "Epoch 5/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.7653 - loss: 0.8403 - val_accuracy: 0.6750 - val_loss: 1.2094\n",
      "Epoch 6/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.7951 - loss: 0.7445 - val_accuracy: 0.6978 - val_loss: 1.1500\n",
      "Epoch 7/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.8091 - loss: 0.6746 - val_accuracy: 0.7298 - val_loss: 1.0218\n",
      "Epoch 8/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.8180 - loss: 0.6538 - val_accuracy: 0.7148 - val_loss: 1.0907\n",
      "Epoch 9/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.8291 - loss: 0.6018 - val_accuracy: 0.7070 - val_loss: 1.0694\n",
      "Epoch 10/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.8344 - loss: 0.5605 - val_accuracy: 0.7142 - val_loss: 1.0287\n",
      "Epoch 11/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - accuracy: 0.8452 - loss: 0.5299 - val_accuracy: 0.7583 - val_loss: 0.8660\n",
      "Epoch 12/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.8548 - loss: 0.4995 - val_accuracy: 0.7614 - val_loss: 0.9009\n",
      "Epoch 13/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.8619 - loss: 0.4698 - val_accuracy: 0.7558 - val_loss: 0.9489\n",
      "Epoch 14/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.8624 - loss: 0.4650 - val_accuracy: 0.7583 - val_loss: 0.9083\n",
      "Epoch 15/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8693 - loss: 0.4299 - val_accuracy: 0.7474 - val_loss: 0.9716\n",
      "\n",
      "Training Latent RNN...\n",
      "Epoch 1/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 173ms/step - accuracy: 0.1523 - loss: 3.0255 - val_accuracy: 0.4366 - val_loss: 2.3362\n",
      "Epoch 2/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 165ms/step - accuracy: 0.4990 - loss: 1.8816 - val_accuracy: 0.5558 - val_loss: 1.9402\n",
      "Epoch 3/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 165ms/step - accuracy: 0.6071 - loss: 1.4535 - val_accuracy: 0.5874 - val_loss: 1.8475\n",
      "Epoch 4/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 174ms/step - accuracy: 0.6746 - loss: 1.2198 - val_accuracy: 0.6336 - val_loss: 1.5857\n",
      "Epoch 5/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.6977 - loss: 1.1182 - val_accuracy: 0.6262 - val_loss: 1.5519\n",
      "Epoch 6/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 171ms/step - accuracy: 0.7163 - loss: 0.9957 - val_accuracy: 0.6221 - val_loss: 1.5195\n",
      "Epoch 7/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 176ms/step - accuracy: 0.7345 - loss: 0.9454 - val_accuracy: 0.6438 - val_loss: 1.4676\n",
      "Epoch 8/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 177ms/step - accuracy: 0.7441 - loss: 0.9019 - val_accuracy: 0.6662 - val_loss: 1.3586\n",
      "Epoch 9/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 176ms/step - accuracy: 0.7575 - loss: 0.8308 - val_accuracy: 0.6693 - val_loss: 1.3803\n",
      "Epoch 10/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 169ms/step - accuracy: 0.7743 - loss: 0.8000 - val_accuracy: 0.6703 - val_loss: 1.3283\n",
      "Epoch 11/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 155ms/step - accuracy: 0.7811 - loss: 0.7693 - val_accuracy: 0.6680 - val_loss: 1.3096\n",
      "Epoch 12/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.7874 - loss: 0.7359 - val_accuracy: 0.6828 - val_loss: 1.2677\n",
      "Epoch 13/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 175ms/step - accuracy: 0.8022 - loss: 0.6891 - val_accuracy: 0.7021 - val_loss: 1.2278\n",
      "Epoch 14/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 175ms/step - accuracy: 0.8047 - loss: 0.6716 - val_accuracy: 0.7044 - val_loss: 1.1905\n",
      "Epoch 15/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 163ms/step - accuracy: 0.8116 - loss: 0.6592 - val_accuracy: 0.6726 - val_loss: 1.3012\n",
      "\n",
      "Evaluating Latent-CNN ...\n",
      "  Clean         acc=0.7614  macroF1=0.8057  time=1.1s\n",
      "  Gaussian      acc=0.7290  macroF1=0.7796  time=1.2s\n",
      "  Uniform       acc=0.7483  macroF1=0.7939  time=1.1s\n",
      "  Dropout       acc=0.5115  macroF1=0.5465  time=1.1s\n",
      "  Salt-Pepper   acc=0.5884  macroF1=0.6296  time=1.3s\n",
      "  FGSM          acc=0.0977  macroF1=0.1375  time=4.0s\n",
      "  PGD           acc=0.0008  macroF1=0.0015  time=30.5s\n",
      "\n",
      "Evaluating Latent-RNN ...\n",
      "  Clean         acc=0.7044  macroF1=0.7482  time=27.2s\n",
      "  Gaussian      acc=0.6208  macroF1=0.6515  time=26.7s\n",
      "  Uniform       acc=0.6789  macroF1=0.7139  time=26.5s\n",
      "  Dropout       acc=0.3984  macroF1=0.4032  time=25.0s\n",
      "  Salt-Pepper   acc=0.4618  macroF1=0.4812  time=26.7s\n",
      "  FGSM          acc=0.0265  macroF1=0.0133  time=93.6s\n",
      "  PGD           acc=0.0004  macroF1=0.0001  time=560.1s\n",
      "\n",
      "================================================================================\n",
      "RESULTS:\n",
      "     Model   Condition  Accuracy  Macro-F1    Time(s)\n",
      "Latent-CNN       Clean  0.761412  0.805730   1.116000\n",
      "Latent-RNN       Clean  0.704448  0.748181  27.232263\n",
      "Latent-CNN     Dropout  0.511510  0.546532   1.103086\n",
      "Latent-RNN     Dropout  0.398361  0.403190  25.048946\n",
      "Latent-CNN        FGSM  0.097737  0.137482   4.017928\n",
      "Latent-RNN        FGSM  0.026531  0.013272  93.603895\n",
      "Latent-CNN    Gaussian  0.729028  0.779550   1.165957\n",
      "Latent-RNN    Gaussian  0.620757  0.651524  26.733493\n",
      "Latent-CNN         PGD  0.000780  0.001494  30.455906\n",
      "Latent-RNN         PGD  0.000390  0.000094 560.128690\n",
      "Latent-CNN Salt-Pepper  0.588373  0.629558   1.307001\n",
      "Latent-RNN Salt-Pepper  0.461764  0.481243  26.657062\n",
      "Latent-CNN     Uniform  0.748342  0.793856   1.144748\n",
      "Latent-RNN     Uniform  0.678892  0.713855  26.525929\n",
      "\n",
      "Saved: malevis_latent_cnn_rnn_results.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "MaleVis: VAE Latent -> (CNN + RNN) with Clean + Perturbations + Adversarial (Latent Space)\n",
    "\n",
    "Key features:\n",
    "- Uses local directory with train/ and val/\n",
    "- FIXED class mapping: val uses class_names from train (prevents label mismatch)\n",
    "- Streaming tf.data (NO ds_to_numpy)\n",
    "- VAE uses KL warmup via model.beta (no missing 'kl' layer)\n",
    "- Classifiers use deterministic z_mean (stable)\n",
    "- Evaluates: Clean + Gaussian/Uniform/Dropout/SaltPepper + FGSM/PGD on latent\n",
    "\n",
    "Output:\n",
    "- Prints results table\n",
    "- Saves: malevis_latent_cnn_rnn_results.csv\n",
    "\"\"\"\n",
    "\n",
    "import os, gc, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = r\"C:\\Users\\ajayi\\Downloads\\malevis_train_val_300x300\\malevis_train_val_300x300\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n",
    "\n",
    "IMG_H, IMG_W, CH = 128, 128, 3\n",
    "LATENT_DIM = 64  # set to your chosen k (or best_k from sweep)\n",
    "\n",
    "# VAE\n",
    "VAE_EPOCHS = 25\n",
    "VAE_BS     = 8\n",
    "VAE_LR     = 1e-3\n",
    "\n",
    "# KL warmup\n",
    "BETA_MAX = 1.0\n",
    "BETA_WARMUP_EPOCHS = 10\n",
    "\n",
    "# Classifiers\n",
    "CLS_EPOCHS = 15\n",
    "CLS_BS     = 128\n",
    "CLS_LR     = 1e-3\n",
    "\n",
    "# Latent perturbations\n",
    "NOISE_GAUSS_SIG = 0.15\n",
    "NOISE_UNIF_RNG  = 0.15\n",
    "NOISE_DROPOUT   = 0.25\n",
    "NOISE_SP_PROB   = 0.02\n",
    "\n",
    "# Adversarial (latent, Linf)\n",
    "FGSM_EPS_LAT   = 0.25\n",
    "PGD_EPS_LAT    = 0.35\n",
    "PGD_ALPHA_LAT  = 0.06\n",
    "PGD_ITERS_LAT  = 10\n",
    "\n",
    "# GPU memory growth (safe)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Dataset loading (FIXED mapping)\n",
    "# -----------------------------\n",
    "def make_ds_fixed(train_dir, val_dir, img_size, batch_size, seed):\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names = train_ds.class_names  # canonical mapping\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_names=class_names   # <-- critical fix\n",
    "    )\n",
    "\n",
    "    def norm(x, y):\n",
    "        x = tf.cast(x, tf.float32) / 255.0\n",
    "        return x, y\n",
    "\n",
    "    # DON'T cache images in RAM for 300x300; just prefetch\n",
    "    train_ds = train_ds.map(norm, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = val_ds.map(norm,   num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds, class_names, len(class_names)\n",
    "\n",
    "print(\"Loading datasets with fixed label mapping...\")\n",
    "train_ds_vae, val_ds_vae, class_names, num_classes = make_ds_fixed(\n",
    "    TRAIN_DIR, VAL_DIR, (IMG_H, IMG_W), batch_size=VAE_BS, seed=SEED\n",
    ")\n",
    "print(\"Classes:\", num_classes)\n",
    "print(\"First 5 classes:\", class_names[:5])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Conv VAE (subclassed, beta warmup)\n",
    "# -----------------------------\n",
    "def _compute_decoder_grid(img_h, img_w, downsamples=4):\n",
    "    div = 2 ** downsamples\n",
    "    gh = int(math.ceil(img_h / div))\n",
    "    gw = int(math.ceil(img_w / div))\n",
    "    out_h = gh * div\n",
    "    out_w = gw * div\n",
    "    return gh, gw, out_h, out_w\n",
    "\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    inp = layers.Input(shape=input_shape, name=f\"enc_in_k{latent_dim}\")\n",
    "    x = layers.Conv2D(32, 4, strides=2, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.Conv2D(64, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(128, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(256, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=f\"z_mean_k{latent_dim}\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=f\"z_log_var_k{latent_dim}\")(x)\n",
    "    return tf.keras.Model(inp, [z_mean, z_log_var], name=f\"encoder_k{latent_dim}\")\n",
    "\n",
    "def build_decoder(output_shape, latent_dim):\n",
    "    img_h, img_w, ch = output_shape\n",
    "    gh, gw, out_h, out_w = _compute_decoder_grid(img_h, img_w, downsamples=4)\n",
    "\n",
    "    inp = layers.Input(shape=(latent_dim,), name=f\"dec_in_k{latent_dim}\")\n",
    "    x = layers.Dense(gh * gw * 256, activation=\"relu\")(inp)\n",
    "    x = layers.Reshape((gh, gw, 256))(x)\n",
    "    x = layers.Conv2DTranspose(256, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(128, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(64, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(32, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(ch, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "\n",
    "    crop_h = max(0, out_h - img_h)\n",
    "    crop_w = max(0, out_w - img_w)\n",
    "    if crop_h > 0 or crop_w > 0:\n",
    "        top = crop_h // 2\n",
    "        bot = crop_h - top\n",
    "        left = crop_w // 2\n",
    "        right = crop_w - left\n",
    "        x = layers.Cropping2D(cropping=((top, bot), (left, right)))(x)\n",
    "\n",
    "    return tf.keras.Model(inp, x, name=f\"decoder_k{latent_dim}\")\n",
    "\n",
    "class ConvVAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, logvar_clip=10.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = tf.Variable(0.0, trainable=False, dtype=tf.float32, name=\"beta\")\n",
    "        self.logvar_clip = float(logvar_clip)\n",
    "\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.recon_tracker = tf.keras.metrics.Mean(name=\"recon_sse\")\n",
    "        self.kl_tracker = tf.keras.metrics.Mean(name=\"kl\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.recon_tracker, self.kl_tracker]\n",
    "\n",
    "    def _sample(self, z_mean, z_log_var):\n",
    "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(x, training=True)\n",
    "            z_log_var = tf.clip_by_value(z_log_var, -self.logvar_clip, self.logvar_clip)\n",
    "            z = self._sample(z_mean, z_log_var)\n",
    "            x_hat = self.decoder(z, training=True)\n",
    "\n",
    "            recon_sse = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=[1,2,3]))\n",
    "            kl = tf.reduce_mean(\n",
    "                -0.5 * tf.reduce_sum(1.0 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "            )\n",
    "            loss = recon_sse + self.beta * kl\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.recon_tracker.update_state(recon_sse)\n",
    "        self.kl_tracker.update_state(kl)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x = data\n",
    "        z_mean, z_log_var = self.encoder(x, training=False)\n",
    "        z_log_var = tf.clip_by_value(z_log_var, -self.logvar_clip, self.logvar_clip)\n",
    "        z = self._sample(z_mean, z_log_var)\n",
    "        x_hat = self.decoder(z, training=False)\n",
    "\n",
    "        recon_sse = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=[1,2,3]))\n",
    "        kl = tf.reduce_mean(\n",
    "            -0.5 * tf.reduce_sum(1.0 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        loss = recon_sse + self.beta * kl\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.recon_tracker.update_state(recon_sse)\n",
    "        self.kl_tracker.update_state(kl)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def encode_mean(self, x):\n",
    "        z_mean, _ = self.encoder(x, training=False)\n",
    "        return z_mean\n",
    "\n",
    "class BetaWarmup(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, beta_max=1.0, warmup_epochs=10):\n",
    "        super().__init__()\n",
    "        self.beta_max = float(beta_max)\n",
    "        self.warmup_epochs = int(max(1, warmup_epochs))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        beta = self.beta_max * min(1.0, epoch / float(self.warmup_epochs))\n",
    "        self.model.beta.assign(beta)\n",
    "\n",
    "\n",
    "# Train VAE (images only)\n",
    "print(\"\\nTraining VAE...\")\n",
    "tf.keras.backend.clear_session()\n",
    "encoder = build_encoder((IMG_H, IMG_W, CH), LATENT_DIM)\n",
    "decoder = build_decoder((IMG_H, IMG_W, CH), LATENT_DIM)\n",
    "vae = ConvVAE(encoder, decoder, name=f\"vae_k{LATENT_DIM}\")\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(VAE_LR))\n",
    "\n",
    "vae_train_img = train_ds_vae.map(lambda x, y: x)\n",
    "vae_val_img   = val_ds_vae.map(lambda x, y: x)\n",
    "\n",
    "vae.fit(\n",
    "    vae_train_img,\n",
    "    validation_data=vae_val_img,\n",
    "    epochs=VAE_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        BetaWarmup(beta_max=BETA_MAX, warmup_epochs=BETA_WARMUP_EPOCHS),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build latent datasets (cache latents, safe)\n",
    "# -----------------------------\n",
    "def make_image_ds(dir_path, class_names, img_size, batch_size, shuffle, seed):\n",
    "    ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dir_path,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=seed if shuffle else None,\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    ds = ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds_cls = make_image_ds(TRAIN_DIR, class_names, (IMG_H, IMG_W), CLS_BS, shuffle=True,  seed=SEED)\n",
    "val_ds_cls   = make_image_ds(VAL_DIR,   class_names, (IMG_H, IMG_W), CLS_BS, shuffle=False, seed=SEED)\n",
    "\n",
    "def make_latent_ds(image_ds, vae, cache_latents=True, shuffle=False):\n",
    "    def to_latent(x, y):\n",
    "        z = vae.encode_mean(x)          # (B,k)\n",
    "        z = tf.expand_dims(z, axis=-1)  # (B,k,1)\n",
    "        return z, y\n",
    "\n",
    "    out = image_ds.map(to_latent, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if cache_latents:\n",
    "        out = out.cache()\n",
    "    if shuffle:\n",
    "        out = out.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
    "    return out.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "lat_train_ds = make_latent_ds(train_ds_cls, vae, cache_latents=True, shuffle=True)\n",
    "lat_val_ds   = make_latent_ds(val_ds_cls,   vae, cache_latents=True, shuffle=False)\n",
    "\n",
    "def estimate_latent_bounds(latent_ds, q_lo=0.5, q_hi=99.5, take_batches=50):\n",
    "    zs = []\n",
    "    for zb, _ in latent_ds.take(take_batches):\n",
    "        zs.append(tf.reshape(zb, [-1]).numpy())\n",
    "    zs = np.concatenate(zs, axis=0)\n",
    "    return float(np.percentile(zs, q_lo)), float(np.percentile(zs, q_hi))\n",
    "\n",
    "z_min, z_max = estimate_latent_bounds(lat_train_ds)\n",
    "print(\"\\nLatent clip bounds:\", z_min, z_max)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build latent CNN + latent RNN\n",
    "# -----------------------------\n",
    "def build_latent_cnn(latent_dim, n_classes):\n",
    "    inp = layers.Input(shape=(latent_dim, 1))\n",
    "    x = layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    m = tf.keras.Model(inp, out, name=\"latent_cnn\")\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(CLS_LR),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "def build_latent_rnn(latent_dim, n_classes):\n",
    "    inp = layers.Input(shape=(latent_dim, 1))\n",
    "    x = layers.Bidirectional(layers.LSTM(96, return_sequences=False))(inp)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(192, activation=\"relu\")(x)\n",
    "    out = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    m = tf.keras.Model(inp, out, name=\"latent_rnn\")\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(CLS_LR),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "cnn_lat = build_latent_cnn(LATENT_DIM, num_classes)\n",
    "rnn_lat = build_latent_rnn(LATENT_DIM, num_classes)\n",
    "\n",
    "print(\"\\nTraining Latent CNN...\")\n",
    "cnn_lat.fit(\n",
    "    lat_train_ds,\n",
    "    validation_data=lat_val_ds,\n",
    "    epochs=CLS_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Latent RNN...\")\n",
    "rnn_lat.fit(\n",
    "    lat_train_ds,\n",
    "    validation_data=lat_val_ds,\n",
    "    epochs=CLS_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Latent perturbations + attacks\n",
    "# -----------------------------\n",
    "def clip_latent(z):\n",
    "    return tf.clip_by_value(z, z_min, z_max)\n",
    "\n",
    "def latent_noise(z, kind):\n",
    "    if kind == \"Gaussian\":\n",
    "        z2 = z + tf.random.normal(tf.shape(z), stddev=NOISE_GAUSS_SIG)\n",
    "    elif kind == \"Uniform\":\n",
    "        z2 = z + tf.random.uniform(tf.shape(z), minval=-NOISE_UNIF_RNG, maxval=NOISE_UNIF_RNG)\n",
    "    elif kind == \"Dropout\":\n",
    "        mask = tf.cast(tf.random.uniform(tf.shape(z)) > NOISE_DROPOUT, tf.float32)\n",
    "        z2 = z * mask\n",
    "    elif kind == \"Salt-Pepper\":\n",
    "        rnd = tf.random.uniform(tf.shape(z))\n",
    "        z2 = tf.where(rnd < (NOISE_SP_PROB/2.0), tf.cast(z_min, tf.float32), z)\n",
    "        z2 = tf.where(rnd > 1.0 - (NOISE_SP_PROB/2.0), tf.cast(z_max, tf.float32), z2)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown noise kind\")\n",
    "    return clip_latent(z2)\n",
    "\n",
    "cce_sparse = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def fgsm_latent(model, z, y, eps):\n",
    "    z = tf.identity(z)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(z)\n",
    "        probs = model(z, training=False)\n",
    "        loss = cce_sparse(y, probs)\n",
    "    grad = tape.gradient(loss, z)\n",
    "    z_adv = z + eps * tf.sign(grad)\n",
    "    return clip_latent(z_adv)\n",
    "\n",
    "def pgd_latent(model, z0, y, eps, alpha, iters):\n",
    "    z_adv = z0 + tf.random.uniform(tf.shape(z0), minval=-eps, maxval=eps)\n",
    "    z_adv = clip_latent(z_adv)\n",
    "    for _ in range(iters):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(z_adv)\n",
    "            probs = model(z_adv, training=False)\n",
    "            loss = cce_sparse(y, probs)\n",
    "        grad = tape.gradient(loss, z_adv)\n",
    "        z_adv = z_adv + alpha * tf.sign(grad)\n",
    "\n",
    "        delta = tf.clip_by_value(z_adv - z0, -eps, eps)\n",
    "        z_adv = clip_latent(z0 + delta)\n",
    "    return z_adv\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Evaluation on Clean + Perturbations + Adversarial\n",
    "# -----------------------------\n",
    "def eval_model_on_condition(model, latent_ds, condition):\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for zb, yb in latent_ds:\n",
    "        if condition == \"Clean\":\n",
    "            z_eval = zb\n",
    "        elif condition in [\"Gaussian\", \"Uniform\", \"Dropout\", \"Salt-Pepper\"]:\n",
    "            z_eval = latent_noise(zb, condition)\n",
    "        elif condition == \"FGSM\":\n",
    "            z_eval = fgsm_latent(model, zb, yb, FGSM_EPS_LAT)\n",
    "        elif condition == \"PGD\":\n",
    "            z_eval = pgd_latent(model, zb, yb, PGD_EPS_LAT, PGD_ALPHA_LAT, PGD_ITERS_LAT)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown condition\")\n",
    "\n",
    "        probs = model(z_eval, training=False)\n",
    "        pred = tf.argmax(probs, axis=1)\n",
    "\n",
    "        y_true_all.append(yb.numpy())\n",
    "        y_pred_all.append(pred.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    macro_f1 = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    elapsed = float(time.time() - t0)\n",
    "\n",
    "    return acc, macro_f1, elapsed\n",
    "\n",
    "conditions = [\"Clean\", \"Gaussian\", \"Uniform\", \"Dropout\", \"Salt-Pepper\", \"FGSM\", \"PGD\"]\n",
    "\n",
    "results = []\n",
    "for mname, model in [(\"Latent-CNN\", cnn_lat), (\"Latent-RNN\", rnn_lat)]:\n",
    "    print(f\"\\nEvaluating {mname} ...\")\n",
    "    for cond in conditions:\n",
    "        acc, mf1, sec = eval_model_on_condition(model, lat_val_ds, cond)\n",
    "        results.append({\"Model\": mname, \"Condition\": cond, \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "        print(f\"  {cond:12s}  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "df = pd.DataFrame(results).sort_values([\"Condition\", \"Model\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "df.to_csv(\"malevis_latent_cnn_rnn_results.csv\", index=False)\n",
    "print(\"\\nSaved: malevis_latent_cnn_rnn_results.csv\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a1f6145-fd53-4762-9cf8-9860e95c2e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with fixed label mapping...\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n",
      "Classes: 26\n",
      "First 5 classes: ['Adposhel', 'Agent', 'Allaple', 'Amonetize', 'Androm']\n",
      "\n",
      "Training VAE...\n",
      "Epoch 1/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2589s\u001b[0m 2s/step - kl: 1430.3793 - loss: 17699.2090 - recon_sse: 17699.2090 - val_kl: 523.2974 - val_loss: 12347.1885 - val_recon_sse: 12347.1885\n",
      "Epoch 2/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2585s\u001b[0m 2s/step - kl: 407.2189 - loss: 12004.5361 - recon_sse: 11963.8184 - val_kl: 301.9803 - val_loss: 10930.0068 - val_recon_sse: 10899.8076\n",
      "Epoch 3/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2588s\u001b[0m 2s/step - kl: 263.2910 - loss: 10512.8809 - recon_sse: 10460.2217 - val_kl: 208.0231 - val_loss: 10480.4053 - val_recon_sse: 10438.8047\n",
      "Epoch 4/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2606s\u001b[0m 2s/step - kl: 203.1589 - loss: 9889.6133 - recon_sse: 9828.6650 - val_kl: 182.3593 - val_loss: 10133.3496 - val_recon_sse: 10078.6348\n",
      "Epoch 5/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2591s\u001b[0m 2s/step - kl: 173.7014 - loss: 9547.3809 - recon_sse: 9477.9023 - val_kl: 159.9792 - val_loss: 9930.9873 - val_recon_sse: 9867.0029\n",
      "Epoch 6/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2593s\u001b[0m 2s/step - kl: 160.3491 - loss: 9369.1904 - recon_sse: 9289.0137 - val_kl: 157.5636 - val_loss: 9971.8848 - val_recon_sse: 9893.1055\n",
      "Epoch 7/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2606s\u001b[0m 2s/step - kl: 151.6973 - loss: 9135.4619 - recon_sse: 9044.4453 - val_kl: 147.2139 - val_loss: 9833.5293 - val_recon_sse: 9745.2070\n",
      "Epoch 8/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2599s\u001b[0m 2s/step - kl: 146.6311 - loss: 9077.9736 - recon_sse: 8975.3311 - val_kl: 143.9232 - val_loss: 9658.9980 - val_recon_sse: 9558.2520\n",
      "Epoch 9/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2614s\u001b[0m 2s/step - kl: 143.0558 - loss: 8960.2900 - recon_sse: 8845.8438 - val_kl: 143.4442 - val_loss: 9636.9287 - val_recon_sse: 9522.1729\n",
      "Epoch 10/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2606s\u001b[0m 2s/step - kl: 141.3586 - loss: 8901.9307 - recon_sse: 8774.7012 - val_kl: 144.5184 - val_loss: 9790.2725 - val_recon_sse: 9660.2148\n",
      "Epoch 11/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2599s\u001b[0m 2s/step - kl: 139.4279 - loss: 8801.7715 - recon_sse: 8662.3467 - val_kl: 139.6304 - val_loss: 9632.9873 - val_recon_sse: 9493.3535\n",
      "Epoch 12/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2657s\u001b[0m 2s/step - kl: 140.1742 - loss: 8729.0820 - recon_sse: 8588.9072 - val_kl: 138.0236 - val_loss: 9611.5420 - val_recon_sse: 9473.5244\n",
      "Epoch 13/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2645s\u001b[0m 2s/step - kl: 141.2019 - loss: 8696.0459 - recon_sse: 8554.8428 - val_kl: 140.8303 - val_loss: 9573.4863 - val_recon_sse: 9432.6533\n",
      "Epoch 14/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2613s\u001b[0m 2s/step - kl: 142.9191 - loss: 8654.4365 - recon_sse: 8511.5156 - val_kl: 141.7761 - val_loss: 9526.5664 - val_recon_sse: 9384.7891\n",
      "Epoch 15/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2576s\u001b[0m 2s/step - kl: 144.7975 - loss: 8624.0117 - recon_sse: 8479.2168 - val_kl: 144.9855 - val_loss: 9470.3896 - val_recon_sse: 9325.4014\n",
      "Epoch 16/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2625s\u001b[0m 2s/step - kl: 146.0570 - loss: 8570.2402 - recon_sse: 8424.1758 - val_kl: 138.8310 - val_loss: 9522.5488 - val_recon_sse: 9383.7285\n",
      "Epoch 17/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2575s\u001b[0m 2s/step - kl: 147.1092 - loss: 8488.9150 - recon_sse: 8341.8047 - val_kl: 145.6764 - val_loss: 9385.8047 - val_recon_sse: 9240.1279\n",
      "Epoch 18/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2578s\u001b[0m 2s/step - kl: 148.6587 - loss: 8448.0752 - recon_sse: 8299.4189 - val_kl: 153.3770 - val_loss: 9571.9482 - val_recon_sse: 9418.5664\n",
      "Epoch 19/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2578s\u001b[0m 2s/step - kl: 149.6183 - loss: 8434.8594 - recon_sse: 8285.2402 - val_kl: 149.9288 - val_loss: 9382.6934 - val_recon_sse: 9232.7607\n",
      "Epoch 20/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2582s\u001b[0m 2s/step - kl: 150.9102 - loss: 8430.9111 - recon_sse: 8279.9971 - val_kl: 147.2505 - val_loss: 9347.0215 - val_recon_sse: 9199.7744\n",
      "Epoch 21/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2672s\u001b[0m 2s/step - kl: 151.4624 - loss: 8378.3516 - recon_sse: 8226.8887 - val_kl: 152.6337 - val_loss: 9376.2910 - val_recon_sse: 9223.6455\n",
      "Epoch 22/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2653s\u001b[0m 2s/step - kl: 152.2520 - loss: 8357.6201 - recon_sse: 8205.3672 - val_kl: 153.5295 - val_loss: 9325.7471 - val_recon_sse: 9172.2148\n",
      "Epoch 23/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2668s\u001b[0m 2s/step - kl: 154.2780 - loss: 8339.8076 - recon_sse: 8185.5283 - val_kl: 153.1412 - val_loss: 9349.0742 - val_recon_sse: 9195.9307\n",
      "Epoch 24/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2683s\u001b[0m 2s/step - kl: 154.6201 - loss: 8303.7764 - recon_sse: 8149.1577 - val_kl: 152.8173 - val_loss: 9411.1172 - val_recon_sse: 9258.2959\n",
      "Epoch 25/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2696s\u001b[0m 2s/step - kl: 155.3078 - loss: 8322.3457 - recon_sse: 8167.0381 - val_kl: 149.2747 - val_loss: 9289.4316 - val_recon_sse: 9140.1689\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n",
      "\n",
      "Latent clip bounds: -2.5877618193626404 2.393813282251358\n",
      "\n",
      "Training Latent CNN...\n",
      "Epoch 1/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 761ms/step - accuracy: 0.1669 - loss: 3.0591 - val_accuracy: 0.4124 - val_loss: 2.2605\n",
      "Epoch 2/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.5795 - loss: 1.6616 - val_accuracy: 0.5392 - val_loss: 1.5961\n",
      "Epoch 3/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.7013 - loss: 1.1152 - val_accuracy: 0.5989 - val_loss: 1.3322\n",
      "Epoch 4/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.7435 - loss: 0.9220 - val_accuracy: 0.7124 - val_loss: 1.1112\n",
      "Epoch 5/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.7723 - loss: 0.8161 - val_accuracy: 0.7048 - val_loss: 1.1240\n",
      "Epoch 6/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.8032 - loss: 0.7141 - val_accuracy: 0.7195 - val_loss: 1.0702\n",
      "Epoch 7/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.8098 - loss: 0.6591 - val_accuracy: 0.7378 - val_loss: 1.0378\n",
      "Epoch 8/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.8232 - loss: 0.6235 - val_accuracy: 0.6836 - val_loss: 1.1260\n",
      "Epoch 9/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.8288 - loss: 0.5931 - val_accuracy: 0.7105 - val_loss: 1.0915\n",
      "Epoch 10/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.8378 - loss: 0.5404 - val_accuracy: 0.7017 - val_loss: 1.0827\n",
      "Epoch 11/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.8471 - loss: 0.5147 - val_accuracy: 0.6867 - val_loss: 1.2273\n",
      "\n",
      "Training Latent RNN...\n",
      "Epoch 1/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 323ms/step - accuracy: 0.1739 - loss: 3.0780 - val_accuracy: 0.3467 - val_loss: 2.4407\n",
      "Epoch 2/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 295ms/step - accuracy: 0.4718 - loss: 1.9989 - val_accuracy: 0.4717 - val_loss: 1.8105\n",
      "Epoch 3/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 305ms/step - accuracy: 0.6047 - loss: 1.4982 - val_accuracy: 0.5306 - val_loss: 1.5883\n",
      "Epoch 4/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 287ms/step - accuracy: 0.6675 - loss: 1.2163 - val_accuracy: 0.5714 - val_loss: 1.4485\n",
      "Epoch 5/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 304ms/step - accuracy: 0.7121 - loss: 1.0571 - val_accuracy: 0.5722 - val_loss: 1.4701\n",
      "Epoch 6/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 290ms/step - accuracy: 0.7388 - loss: 0.9679 - val_accuracy: 0.6239 - val_loss: 1.2198\n",
      "Epoch 7/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 284ms/step - accuracy: 0.7486 - loss: 0.9023 - val_accuracy: 0.6311 - val_loss: 1.2277\n",
      "Epoch 8/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 283ms/step - accuracy: 0.7724 - loss: 0.7981 - val_accuracy: 0.6691 - val_loss: 1.1384\n",
      "Epoch 9/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 286ms/step - accuracy: 0.7752 - loss: 0.7694 - val_accuracy: 0.6787 - val_loss: 1.0789\n",
      "Epoch 10/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 280ms/step - accuracy: 0.8052 - loss: 0.6850 - val_accuracy: 0.6440 - val_loss: 1.2668\n",
      "Epoch 11/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 294ms/step - accuracy: 0.7957 - loss: 0.7043 - val_accuracy: 0.6674 - val_loss: 1.1544\n",
      "Epoch 12/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 283ms/step - accuracy: 0.8085 - loss: 0.6651 - val_accuracy: 0.6844 - val_loss: 1.0817\n",
      "Epoch 13/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 278ms/step - accuracy: 0.8183 - loss: 0.6171 - val_accuracy: 0.6826 - val_loss: 1.0609\n",
      "Epoch 14/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 286ms/step - accuracy: 0.8252 - loss: 0.5928 - val_accuracy: 0.6855 - val_loss: 1.0944\n",
      "Epoch 15/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 285ms/step - accuracy: 0.8332 - loss: 0.5741 - val_accuracy: 0.6966 - val_loss: 1.0974\n",
      "\n",
      "Evaluating Latent-CNN ...\n",
      "  Clean         acc=0.7378  macroF1=0.7802  time=1.2s\n",
      "  Gaussian      acc=0.7202  macroF1=0.7658  time=1.3s\n",
      "  Uniform       acc=0.7314  macroF1=0.7738  time=1.4s\n",
      "  Dropout       acc=0.5579  macroF1=0.5721  time=1.3s\n",
      "  Salt-Pepper   acc=0.5872  macroF1=0.6542  time=1.4s\n",
      "  FGSM          acc=0.0784  macroF1=0.0929  time=4.7s\n",
      "  PGD           acc=0.0000  macroF1=0.0000  time=34.1s\n",
      "\n",
      "Evaluating Latent-RNN ...\n",
      "  Clean         acc=0.6966  macroF1=0.7648  time=30.7s\n",
      "  Gaussian      acc=0.6522  macroF1=0.7222  time=31.1s\n",
      "  Uniform       acc=0.6834  macroF1=0.7522  time=30.5s\n",
      "  Dropout       acc=0.3997  macroF1=0.4274  time=30.1s\n",
      "  Salt-Pepper   acc=0.5332  macroF1=0.5975  time=30.3s\n",
      "  FGSM          acc=0.0505  macroF1=0.0530  time=93.9s\n",
      "  PGD           acc=0.0000  macroF1=0.0000  time=636.6s\n",
      "\n",
      "================================================================================\n",
      "RESULTS:\n",
      "     Model   Condition  Accuracy  Macro-F1    Time(s)\n",
      "Latent-CNN       Clean  0.737807  0.780246   1.232479\n",
      "Latent-RNN       Clean  0.696645  0.764756  30.686296\n",
      "Latent-CNN     Dropout  0.557940  0.572097   1.331813\n",
      "Latent-RNN     Dropout  0.399727  0.427405  30.056458\n",
      "Latent-CNN        FGSM  0.078424  0.092881   4.734471\n",
      "Latent-RNN        FGSM  0.050527  0.053047  93.931171\n",
      "Latent-CNN    Gaussian  0.720250  0.765840   1.338542\n",
      "Latent-RNN    Gaussian  0.652165  0.722156  31.148269\n",
      "Latent-CNN         PGD  0.000000  0.000000  34.095389\n",
      "Latent-RNN         PGD  0.000000  0.000000 636.564801\n",
      "Latent-CNN Salt-Pepper  0.587202  0.654179   1.404393\n",
      "Latent-RNN Salt-Pepper  0.533164  0.597452  30.311387\n",
      "Latent-CNN     Uniform  0.731369  0.773756   1.375677\n",
      "Latent-RNN     Uniform  0.683379  0.752191  30.471151\n",
      "\n",
      "Saved: malevis_latent_cnn_rnn_results.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "MaleVis: VAE Latent -> (CNN + RNN) with Clean + Perturbations + Adversarial (Latent Space)\n",
    "\n",
    "Key features:\n",
    "- Uses local directory with train/ and val/\n",
    "- FIXED class mapping: val uses class_names from train (prevents label mismatch)\n",
    "- Streaming tf.data (NO ds_to_numpy)\n",
    "- VAE uses KL warmup via model.beta (no missing 'kl' layer)\n",
    "- Classifiers use deterministic z_mean (stable)\n",
    "- Evaluates: Clean + Gaussian/Uniform/Dropout/SaltPepper + FGSM/PGD on latent\n",
    "\n",
    "Output:\n",
    "- Prints results table\n",
    "- Saves: malevis_latent_cnn_rnn_results.csv\n",
    "\"\"\"\n",
    "\n",
    "import os, gc, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = r\"C:\\Users\\ajayi\\Downloads\\malevis_train_val_300x300\\malevis_train_val_300x300\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n",
    "\n",
    "IMG_H, IMG_W, CH = 300, 300, 3\n",
    "LATENT_DIM = 64  # set to your chosen k (or best_k from sweep)\n",
    "\n",
    "# VAE\n",
    "VAE_EPOCHS = 25\n",
    "VAE_BS     = 8\n",
    "VAE_LR     = 1e-3\n",
    "\n",
    "# KL warmup\n",
    "BETA_MAX = 1.0\n",
    "BETA_WARMUP_EPOCHS = 10\n",
    "\n",
    "# Classifiers\n",
    "CLS_EPOCHS = 15\n",
    "CLS_BS     = 128\n",
    "CLS_LR     = 1e-3\n",
    "\n",
    "# Latent perturbations\n",
    "NOISE_GAUSS_SIG = 0.15\n",
    "NOISE_UNIF_RNG  = 0.15\n",
    "NOISE_DROPOUT   = 0.25\n",
    "NOISE_SP_PROB   = 0.02\n",
    "\n",
    "# Adversarial (latent, Linf)\n",
    "FGSM_EPS_LAT   = 0.25\n",
    "PGD_EPS_LAT    = 0.35\n",
    "PGD_ALPHA_LAT  = 0.06\n",
    "PGD_ITERS_LAT  = 10\n",
    "\n",
    "# GPU memory growth (safe)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Dataset loading (FIXED mapping)\n",
    "# -----------------------------\n",
    "def make_ds_fixed(train_dir, val_dir, img_size, batch_size, seed):\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names = train_ds.class_names  # canonical mapping\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_names=class_names   # <-- critical fix\n",
    "    )\n",
    "\n",
    "    def norm(x, y):\n",
    "        x = tf.cast(x, tf.float32) / 255.0\n",
    "        return x, y\n",
    "\n",
    "    # DON'T cache images in RAM for 300x300; just prefetch\n",
    "    train_ds = train_ds.map(norm, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = val_ds.map(norm,   num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds, class_names, len(class_names)\n",
    "\n",
    "print(\"Loading datasets with fixed label mapping...\")\n",
    "train_ds_vae, val_ds_vae, class_names, num_classes = make_ds_fixed(\n",
    "    TRAIN_DIR, VAL_DIR, (IMG_H, IMG_W), batch_size=VAE_BS, seed=SEED\n",
    ")\n",
    "print(\"Classes:\", num_classes)\n",
    "print(\"First 5 classes:\", class_names[:5])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Conv VAE (subclassed, beta warmup)\n",
    "# -----------------------------\n",
    "def _compute_decoder_grid(img_h, img_w, downsamples=4):\n",
    "    div = 2 ** downsamples\n",
    "    gh = int(math.ceil(img_h / div))\n",
    "    gw = int(math.ceil(img_w / div))\n",
    "    out_h = gh * div\n",
    "    out_w = gw * div\n",
    "    return gh, gw, out_h, out_w\n",
    "\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    inp = layers.Input(shape=input_shape, name=f\"enc_in_k{latent_dim}\")\n",
    "    x = layers.Conv2D(32, 4, strides=2, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.Conv2D(64, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(128, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(256, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=f\"z_mean_k{latent_dim}\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=f\"z_log_var_k{latent_dim}\")(x)\n",
    "    return tf.keras.Model(inp, [z_mean, z_log_var], name=f\"encoder_k{latent_dim}\")\n",
    "\n",
    "def build_decoder(output_shape, latent_dim):\n",
    "    img_h, img_w, ch = output_shape\n",
    "    gh, gw, out_h, out_w = _compute_decoder_grid(img_h, img_w, downsamples=4)\n",
    "\n",
    "    inp = layers.Input(shape=(latent_dim,), name=f\"dec_in_k{latent_dim}\")\n",
    "    x = layers.Dense(gh * gw * 256, activation=\"relu\")(inp)\n",
    "    x = layers.Reshape((gh, gw, 256))(x)\n",
    "    x = layers.Conv2DTranspose(256, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(128, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(64, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(32, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(ch, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "\n",
    "    crop_h = max(0, out_h - img_h)\n",
    "    crop_w = max(0, out_w - img_w)\n",
    "    if crop_h > 0 or crop_w > 0:\n",
    "        top = crop_h // 2\n",
    "        bot = crop_h - top\n",
    "        left = crop_w // 2\n",
    "        right = crop_w - left\n",
    "        x = layers.Cropping2D(cropping=((top, bot), (left, right)))(x)\n",
    "\n",
    "    return tf.keras.Model(inp, x, name=f\"decoder_k{latent_dim}\")\n",
    "\n",
    "class ConvVAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, logvar_clip=10.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = tf.Variable(0.0, trainable=False, dtype=tf.float32, name=\"beta\")\n",
    "        self.logvar_clip = float(logvar_clip)\n",
    "\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.recon_tracker = tf.keras.metrics.Mean(name=\"recon_sse\")\n",
    "        self.kl_tracker = tf.keras.metrics.Mean(name=\"kl\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.recon_tracker, self.kl_tracker]\n",
    "\n",
    "    def _sample(self, z_mean, z_log_var):\n",
    "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(x, training=True)\n",
    "            z_log_var = tf.clip_by_value(z_log_var, -self.logvar_clip, self.logvar_clip)\n",
    "            z = self._sample(z_mean, z_log_var)\n",
    "            x_hat = self.decoder(z, training=True)\n",
    "\n",
    "            recon_sse = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=[1,2,3]))\n",
    "            kl = tf.reduce_mean(\n",
    "                -0.5 * tf.reduce_sum(1.0 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "            )\n",
    "            loss = recon_sse + self.beta * kl\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.recon_tracker.update_state(recon_sse)\n",
    "        self.kl_tracker.update_state(kl)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x = data\n",
    "        z_mean, z_log_var = self.encoder(x, training=False)\n",
    "        z_log_var = tf.clip_by_value(z_log_var, -self.logvar_clip, self.logvar_clip)\n",
    "        z = self._sample(z_mean, z_log_var)\n",
    "        x_hat = self.decoder(z, training=False)\n",
    "\n",
    "        recon_sse = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=[1,2,3]))\n",
    "        kl = tf.reduce_mean(\n",
    "            -0.5 * tf.reduce_sum(1.0 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        loss = recon_sse + self.beta * kl\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.recon_tracker.update_state(recon_sse)\n",
    "        self.kl_tracker.update_state(kl)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def encode_mean(self, x):\n",
    "        z_mean, _ = self.encoder(x, training=False)\n",
    "        return z_mean\n",
    "\n",
    "class BetaWarmup(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, beta_max=1.0, warmup_epochs=10):\n",
    "        super().__init__()\n",
    "        self.beta_max = float(beta_max)\n",
    "        self.warmup_epochs = int(max(1, warmup_epochs))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        beta = self.beta_max * min(1.0, epoch / float(self.warmup_epochs))\n",
    "        self.model.beta.assign(beta)\n",
    "\n",
    "\n",
    "# Train VAE (images only)\n",
    "print(\"\\nTraining VAE...\")\n",
    "tf.keras.backend.clear_session()\n",
    "encoder = build_encoder((IMG_H, IMG_W, CH), LATENT_DIM)\n",
    "decoder = build_decoder((IMG_H, IMG_W, CH), LATENT_DIM)\n",
    "vae = ConvVAE(encoder, decoder, name=f\"vae_k{LATENT_DIM}\")\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(VAE_LR))\n",
    "\n",
    "vae_train_img = train_ds_vae.map(lambda x, y: x)\n",
    "vae_val_img   = val_ds_vae.map(lambda x, y: x)\n",
    "\n",
    "vae.fit(\n",
    "    vae_train_img,\n",
    "    validation_data=vae_val_img,\n",
    "    epochs=VAE_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        BetaWarmup(beta_max=BETA_MAX, warmup_epochs=BETA_WARMUP_EPOCHS),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build latent datasets (cache latents, safe)\n",
    "# -----------------------------\n",
    "def make_image_ds(dir_path, class_names, img_size, batch_size, shuffle, seed):\n",
    "    ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dir_path,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=seed if shuffle else None,\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    ds = ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds_cls = make_image_ds(TRAIN_DIR, class_names, (IMG_H, IMG_W), CLS_BS, shuffle=True,  seed=SEED)\n",
    "val_ds_cls   = make_image_ds(VAL_DIR,   class_names, (IMG_H, IMG_W), CLS_BS, shuffle=False, seed=SEED)\n",
    "\n",
    "def make_latent_ds(image_ds, vae, cache_latents=True, shuffle=False):\n",
    "    def to_latent(x, y):\n",
    "        z = vae.encode_mean(x)          # (B,k)\n",
    "        z = tf.expand_dims(z, axis=-1)  # (B,k,1)\n",
    "        return z, y\n",
    "\n",
    "    out = image_ds.map(to_latent, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if cache_latents:\n",
    "        out = out.cache()\n",
    "    if shuffle:\n",
    "        out = out.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
    "    return out.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "lat_train_ds = make_latent_ds(train_ds_cls, vae, cache_latents=True, shuffle=True)\n",
    "lat_val_ds   = make_latent_ds(val_ds_cls,   vae, cache_latents=True, shuffle=False)\n",
    "\n",
    "def estimate_latent_bounds(latent_ds, q_lo=0.5, q_hi=99.5, take_batches=50):\n",
    "    zs = []\n",
    "    for zb, _ in latent_ds.take(take_batches):\n",
    "        zs.append(tf.reshape(zb, [-1]).numpy())\n",
    "    zs = np.concatenate(zs, axis=0)\n",
    "    return float(np.percentile(zs, q_lo)), float(np.percentile(zs, q_hi))\n",
    "\n",
    "z_min, z_max = estimate_latent_bounds(lat_train_ds)\n",
    "print(\"\\nLatent clip bounds:\", z_min, z_max)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build latent CNN + latent RNN\n",
    "# -----------------------------\n",
    "def build_latent_cnn(latent_dim, n_classes):\n",
    "    inp = layers.Input(shape=(latent_dim, 1))\n",
    "    x = layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    m = tf.keras.Model(inp, out, name=\"latent_cnn\")\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(CLS_LR),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "def build_latent_rnn(latent_dim, n_classes):\n",
    "    inp = layers.Input(shape=(latent_dim, 1))\n",
    "    x = layers.Bidirectional(layers.LSTM(96, return_sequences=False))(inp)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(192, activation=\"relu\")(x)\n",
    "    out = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    m = tf.keras.Model(inp, out, name=\"latent_rnn\")\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(CLS_LR),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "cnn_lat = build_latent_cnn(LATENT_DIM, num_classes)\n",
    "rnn_lat = build_latent_rnn(LATENT_DIM, num_classes)\n",
    "\n",
    "print(\"\\nTraining Latent CNN...\")\n",
    "cnn_lat.fit(\n",
    "    lat_train_ds,\n",
    "    validation_data=lat_val_ds,\n",
    "    epochs=CLS_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Latent RNN...\")\n",
    "rnn_lat.fit(\n",
    "    lat_train_ds,\n",
    "    validation_data=lat_val_ds,\n",
    "    epochs=CLS_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Latent perturbations + attacks\n",
    "# -----------------------------\n",
    "def clip_latent(z):\n",
    "    return tf.clip_by_value(z, z_min, z_max)\n",
    "\n",
    "def latent_noise(z, kind):\n",
    "    if kind == \"Gaussian\":\n",
    "        z2 = z + tf.random.normal(tf.shape(z), stddev=NOISE_GAUSS_SIG)\n",
    "    elif kind == \"Uniform\":\n",
    "        z2 = z + tf.random.uniform(tf.shape(z), minval=-NOISE_UNIF_RNG, maxval=NOISE_UNIF_RNG)\n",
    "    elif kind == \"Dropout\":\n",
    "        mask = tf.cast(tf.random.uniform(tf.shape(z)) > NOISE_DROPOUT, tf.float32)\n",
    "        z2 = z * mask\n",
    "    elif kind == \"Salt-Pepper\":\n",
    "        rnd = tf.random.uniform(tf.shape(z))\n",
    "        z2 = tf.where(rnd < (NOISE_SP_PROB/2.0), tf.cast(z_min, tf.float32), z)\n",
    "        z2 = tf.where(rnd > 1.0 - (NOISE_SP_PROB/2.0), tf.cast(z_max, tf.float32), z2)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown noise kind\")\n",
    "    return clip_latent(z2)\n",
    "\n",
    "cce_sparse = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def fgsm_latent(model, z, y, eps):\n",
    "    z = tf.identity(z)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(z)\n",
    "        probs = model(z, training=False)\n",
    "        loss = cce_sparse(y, probs)\n",
    "    grad = tape.gradient(loss, z)\n",
    "    z_adv = z + eps * tf.sign(grad)\n",
    "    return clip_latent(z_adv)\n",
    "\n",
    "def pgd_latent(model, z0, y, eps, alpha, iters):\n",
    "    z_adv = z0 + tf.random.uniform(tf.shape(z0), minval=-eps, maxval=eps)\n",
    "    z_adv = clip_latent(z_adv)\n",
    "    for _ in range(iters):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(z_adv)\n",
    "            probs = model(z_adv, training=False)\n",
    "            loss = cce_sparse(y, probs)\n",
    "        grad = tape.gradient(loss, z_adv)\n",
    "        z_adv = z_adv + alpha * tf.sign(grad)\n",
    "\n",
    "        delta = tf.clip_by_value(z_adv - z0, -eps, eps)\n",
    "        z_adv = clip_latent(z0 + delta)\n",
    "    return z_adv\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Evaluation on Clean + Perturbations + Adversarial\n",
    "# -----------------------------\n",
    "def eval_model_on_condition(model, latent_ds, condition):\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for zb, yb in latent_ds:\n",
    "        if condition == \"Clean\":\n",
    "            z_eval = zb\n",
    "        elif condition in [\"Gaussian\", \"Uniform\", \"Dropout\", \"Salt-Pepper\"]:\n",
    "            z_eval = latent_noise(zb, condition)\n",
    "        elif condition == \"FGSM\":\n",
    "            z_eval = fgsm_latent(model, zb, yb, FGSM_EPS_LAT)\n",
    "        elif condition == \"PGD\":\n",
    "            z_eval = pgd_latent(model, zb, yb, PGD_EPS_LAT, PGD_ALPHA_LAT, PGD_ITERS_LAT)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown condition\")\n",
    "\n",
    "        probs = model(z_eval, training=False)\n",
    "        pred = tf.argmax(probs, axis=1)\n",
    "\n",
    "        y_true_all.append(yb.numpy())\n",
    "        y_pred_all.append(pred.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    macro_f1 = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    elapsed = float(time.time() - t0)\n",
    "\n",
    "    return acc, macro_f1, elapsed\n",
    "\n",
    "conditions = [\"Clean\", \"Gaussian\", \"Uniform\", \"Dropout\", \"Salt-Pepper\", \"FGSM\", \"PGD\"]\n",
    "\n",
    "results = []\n",
    "for mname, model in [(\"Latent-CNN\", cnn_lat), (\"Latent-RNN\", rnn_lat)]:\n",
    "    print(f\"\\nEvaluating {mname} ...\")\n",
    "    for cond in conditions:\n",
    "        acc, mf1, sec = eval_model_on_condition(model, lat_val_ds, cond)\n",
    "        results.append({\"Model\": mname, \"Condition\": cond, \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "        print(f\"  {cond:12s}  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "df = pd.DataFrame(results).sort_values([\"Condition\", \"Model\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "df.to_csv(\"malevis_latent_cnn_rnn_results.csv\", index=False)\n",
    "print(\"\\nSaved: malevis_latent_cnn_rnn_results.csv\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e384648c-2a7c-41ea-9a4c-6a2384f7ee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with fixed label mapping...\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n",
      "Classes: 26\n",
      "First 5 classes: ['Adposhel', 'Agent', 'Allaple', 'Amonetize', 'Androm']\n",
      "\n",
      "Training CNN...\n",
      "Epoch 1/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m804s\u001b[0m 3s/step - accuracy: 0.1802 - loss: 2.7967 - val_accuracy: 0.4278 - val_loss: 2.2236\n",
      "Epoch 2/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m782s\u001b[0m 3s/step - accuracy: 0.5491 - loss: 1.6089 - val_accuracy: 0.6030 - val_loss: 1.5905\n",
      "Epoch 3/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m780s\u001b[0m 3s/step - accuracy: 0.6708 - loss: 1.1940 - val_accuracy: 0.6528 - val_loss: 1.4124\n",
      "Epoch 4/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m781s\u001b[0m 3s/step - accuracy: 0.7408 - loss: 0.9129 - val_accuracy: 0.7298 - val_loss: 1.3171\n",
      "Epoch 5/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 3s/step - accuracy: 0.7938 - loss: 0.7511 - val_accuracy: 0.7284 - val_loss: 1.3466\n",
      "Epoch 6/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m791s\u001b[0m 3s/step - accuracy: 0.8201 - loss: 0.6493 - val_accuracy: 0.7522 - val_loss: 1.0414\n",
      "Epoch 7/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m789s\u001b[0m 3s/step - accuracy: 0.8487 - loss: 0.5321 - val_accuracy: 0.7608 - val_loss: 1.0487\n",
      "Epoch 8/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m795s\u001b[0m 3s/step - accuracy: 0.8660 - loss: 0.4860 - val_accuracy: 0.7673 - val_loss: 1.0943\n",
      "Epoch 9/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m794s\u001b[0m 3s/step - accuracy: 0.8719 - loss: 0.4440 - val_accuracy: 0.8034 - val_loss: 0.9406\n",
      "Epoch 10/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m792s\u001b[0m 3s/step - accuracy: 0.8877 - loss: 0.4095 - val_accuracy: 0.8090 - val_loss: 0.8844\n",
      "Epoch 11/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m794s\u001b[0m 3s/step - accuracy: 0.8973 - loss: 0.3639 - val_accuracy: 0.8010 - val_loss: 0.8941\n",
      "Epoch 12/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m788s\u001b[0m 3s/step - accuracy: 0.9008 - loss: 0.3359 - val_accuracy: 0.8104 - val_loss: 0.9201\n",
      "Epoch 13/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m791s\u001b[0m 3s/step - accuracy: 0.9022 - loss: 0.3272 - val_accuracy: 0.8176 - val_loss: 0.9629\n",
      "Epoch 14/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m795s\u001b[0m 3s/step - accuracy: 0.9108 - loss: 0.2923 - val_accuracy: 0.8250 - val_loss: 0.8611\n",
      "Epoch 15/15\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m789s\u001b[0m 3s/step - accuracy: 0.9186 - loss: 0.2878 - val_accuracy: 0.8311 - val_loss: 0.8655\n",
      "\n",
      "Sanity check (Keras evaluate on CLEAN val): [0.8655463457107544, 0.8310573697090149]\n",
      "\n",
      "Evaluating CNN (noise / clean) ...\n",
      "  Clean         acc=0.8311  macroF1=0.8658  time=95.8s\n",
      "  Gaussian      acc=0.7019  macroF1=0.6866  time=111.7s\n",
      "  Uniform       acc=0.8174  macroF1=0.8487  time=105.6s\n",
      "  Dropout       acc=0.2236  macroF1=0.1853  time=102.3s\n",
      "  Salt-Pepper   acc=0.6966  macroF1=0.6891  time=110.0s\n",
      "\n",
      "Evaluating CNN (adversarial on first 64 samples) ...\n",
      "  FGSM         acc=0.8438  macroF1=0.3051  time=9.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037c5736673a466a801a954dd82336c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PGD          acc=0.6406  macroF1=0.2603  time=66.6s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997e03443f434ebbb4148e1c4b103b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HopSkipJump:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  HopSkipJump  acc=0.0000  macroF1=0.0000  time=9625.5s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2685030be13d4f8283e5d65736923697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf0401d841f445c84b2e3b649f3ede8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b22931114ac4aa8a17baf2a43ca5b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b6b4b4745c47498f26776d4b325c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8254e6cced8a4a19b3228223ca40c091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c6e11d867b4a8ab0cc0b54bf614701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff1d44a73734860a60cecd7991b91e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7043b87932c345c1b2c8a6b96dc726b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dd2a5e75ed411f9104da055f15cf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aef1720c2334fc789b080add1bf45a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b274a97e2b24068b3012ecf4ed8441b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9deefce0024fbb945cc0b2933c8891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2dd3aa9c8d40bfb84f14b81a12dbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd81542d63d34102b8fed8c2d0ed5e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95a9d7928584d12b93ced83aac66ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4bb6fa025342b4909ec787cf211d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6acdf151a3c4d8c81551ebca7c15133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c96f4ef53c4459b4343b4ceeae5930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15e1ce6eb9349ca8c543f97f433f32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e5419004ee4089adfe0b45f52247bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17cd14e51ec46f6ad97f695ea7dd613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2421924d2e47bfbd30709f9e0c9f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a693c7086645ee911cae4d126ef2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff22b86ae5004eb2b4d518f637438b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20945e274dab442e9296bb8d26d3f3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f674fe342cd4a58bc3e9879c2560c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4f576e68124a8d883c7a51994944dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115a67267b1f416d9c7628a2fcaf03a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f01f59a1dd4b6cb55833419996cf11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c23d1a951144bdbe6c6f8389cd81be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4f4df5fad242a3b0ad5b3c8d2803ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8b47c41f584664bae0cfacabad6795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41b776f9a3d47cf8cfc93c3f6ae54ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e762872080457ca906d466cbfad569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc4a589d4f64ff8a1f86bfd9f8aa2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c66d31f1e54b08b598b89a4b2b1806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa05ba57196a4798ba1a0f945fef964e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe322e8764e04d12a241afbd2d73360f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e185607ba64a4bd9b2c3d0bc40967ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed95464aa5f405abfb79bbebe30aa51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2994b3ca75474a812f696cf2d9309a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b1201d40c54253aa8c6d7b70e8e294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a73ebf5b86d45479eb19f9f8e2d1577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371876162821483386d55ee5353eae3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b94ab71bea34bb8921a7facfa7651cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2d5de6e4f54d95af93bfe206dcbef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6750e68cf2834a76b8cbe2e2635f8a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344891182c1947a68136cb87530cc34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c054b93c20c4fa08ac095e1ef1c085d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53102c39311140cf98fab65f3b60e28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a9f7ea136f42dbb8d071d4380da75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89bc942d97244ebb230e5cb46b9abdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110612b9c96b4de5a1b7c27623831917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a977b4ce52d84e2aa50ada3ad134aec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7919d95171c14a1cb25d213dfb9a5940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02aa1f1b555946359fbef767f86b5d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a571cc70864fb7bb7b500ed42f9d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f5841eeba9405b99fdd445e89f66b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368be2c2744244369a9267312a47fcb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20787a7934d24a9db1859c792e5b11a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec4bc45e8a64bbc8cbd7aec5f53a425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db710261d7444248afa0a8c8ae2e4934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6423505c1c54498fa01d406a417d3066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b1a46f010945a88f7bba2e55e96335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e6c1f68d4e4ecd8ea16f28c07b39e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary     acc=0.0312  macroF1=0.0152  time=13909.3s\n",
      "\n",
      "================================================================================\n",
      "RESULTS:\n",
      "    Model   Condition  Accuracy  Macro-F1      Time(s)\n",
      "CNN-Image    Boundary  0.031250  0.015152 13909.254671\n",
      "CNN-Image       Clean  0.831057  0.865819    95.781290\n",
      "CNN-Image     Dropout  0.223566  0.185294   102.293325\n",
      "CNN-Image        FGSM  0.843750  0.305085     9.081110\n",
      "CNN-Image    Gaussian  0.701912  0.686563   111.717097\n",
      "CNN-Image HopSkipJump  0.000000  0.000000  9625.470649\n",
      "CNN-Image         PGD  0.640625  0.260317    66.627091\n",
      "CNN-Image Salt-Pepper  0.696645  0.689083   110.015060\n",
      "CNN-Image     Uniform  0.817401  0.848724   105.623443\n",
      "\n",
      "Saved: malevis_image_cnn_noise_adv_results.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "MaleVis (IMAGE SPACE):\n",
    "CNN classifier with:\n",
    "- Clean evaluation\n",
    "- Noise perturbations: Gaussian / Uniform / Dropout / Salt-Pepper\n",
    "- Adversarial attacks via ART: FGSM / PGD / HopSkipJump / BoundaryAttack\n",
    "\n",
    "Key features:\n",
    "- Uses local directory with train/ and val/\n",
    "- FIXED class mapping: val uses class_names from train (prevents label mismatch)\n",
    "- Streaming tf.data for training/eval (no ds_to_numpy)\n",
    "- Adversarial attacks run on a SMALL SUBSET (ADV_MAX_SAMPLES) to avoid RAM blowups\n",
    "- Uses ART TensorFlowV2Classifier (works with TF2 eager)\n",
    "\n",
    "Output:\n",
    "- Prints results table\n",
    "- Saves: malevis_image_cnn_noise_adv_results.csv\n",
    "\"\"\"\n",
    "\n",
    "import os, time, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = r\"C:\\Users\\ajayi\\Downloads\\malevis_train_val_300x300\\malevis_train_val_300x300\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n",
    "\n",
    "# If you insist on 300x300, keep it.\n",
    "# If RAM/GPU memory becomes painful, drop to 128 or 224.\n",
    "IMG_H, IMG_W, CH = 300, 300, 3\n",
    "\n",
    "# Training\n",
    "EPOCHS = 15\n",
    "BS     = 32\n",
    "LR     = 1e-3\n",
    "\n",
    "# Noise perturbations (image-space)\n",
    "NOISE_GAUSS_SIG = 0.10\n",
    "NOISE_UNIF_RNG  = 0.10\n",
    "NOISE_DROPOUT   = 0.20\n",
    "NOISE_SP_PROB   = 0.02\n",
    "\n",
    "# Adversarial subset (IMPORTANT for memory/time)\n",
    "ADV_MAX_SAMPLES = 64     # increase slowly (e.g., 128, 256) if you have RAM\n",
    "ADV_BATCH_SIZE  = 8      # keep small for 300x300\n",
    "\n",
    "# FGSM/PGD (Linf) eps in [0,1] pixel space\n",
    "FGSM_EPS = 8/255\n",
    "PGD_EPS  = 8/255\n",
    "PGD_ALPHA = 2/255\n",
    "PGD_ITERS = 10\n",
    "\n",
    "# HSJ / Boundary (black-box) — slow, keep small subset\n",
    "HSJ_MAX_ITER    = 20\n",
    "HSJ_MAX_EVAL    = 2000\n",
    "HSJ_INIT_EVAL   = 50\n",
    "HSJ_INIT_SIZE   = 50\n",
    "\n",
    "BOUNDARY_MAX_ITER = 200\n",
    "BOUNDARY_DELTA    = 0.01\n",
    "BOUNDARY_EPS      = 0.01\n",
    "\n",
    "# GPU memory growth (safe)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Dataset loading (FIXED mapping)\n",
    "# -----------------------------\n",
    "def make_ds_fixed(train_dir, val_dir, img_size, batch_size, seed):\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names = train_ds.class_names  # canonical mapping\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    def norm(x, y):\n",
    "        x = tf.cast(x, tf.float32) / 255.0\n",
    "        return x, y\n",
    "\n",
    "    train_ds = train_ds.map(norm, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = val_ds.map(norm,   num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds, class_names, len(class_names)\n",
    "\n",
    "print(\"Loading datasets with fixed label mapping...\")\n",
    "train_ds, val_ds, class_names, num_classes = make_ds_fixed(\n",
    "    TRAIN_DIR, VAL_DIR, (IMG_H, IMG_W), batch_size=BS, seed=SEED\n",
    ")\n",
    "print(\"Classes:\", num_classes)\n",
    "print(\"First 5 classes:\", class_names[:5])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) CNN model (logits output for stability)\n",
    "# -----------------------------\n",
    "def build_cnn(num_classes):\n",
    "    inp = layers.Input(shape=(IMG_H, IMG_W, CH))\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    logits = layers.Dense(num_classes, activation=None, name=\"logits\")(x)  # <-- logits\n",
    "    model = tf.keras.Model(inp, logits, name=\"malevis_cnn_logits\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(LR),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "cnn = build_cnn(num_classes)\n",
    "\n",
    "print(\"\\nTraining CNN...\")\n",
    "cnn.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "print(\"\\nSanity check (Keras evaluate on CLEAN val):\", cnn.evaluate(val_ds, verbose=0))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Noise perturbations (streaming, no numpy)\n",
    "# -----------------------------\n",
    "def clip01(x):\n",
    "    return tf.clip_by_value(x, 0.0, 1.0)\n",
    "\n",
    "def add_noise_batch(x, kind):\n",
    "    if kind == \"Gaussian\":\n",
    "        x2 = x + tf.random.normal(tf.shape(x), stddev=NOISE_GAUSS_SIG)\n",
    "    elif kind == \"Uniform\":\n",
    "        x2 = x + tf.random.uniform(tf.shape(x), minval=-NOISE_UNIF_RNG, maxval=NOISE_UNIF_RNG)\n",
    "    elif kind == \"Dropout\":\n",
    "        mask = tf.cast(tf.random.uniform(tf.shape(x)) > NOISE_DROPOUT, tf.float32)\n",
    "        x2 = x * mask\n",
    "    elif kind == \"Salt-Pepper\":\n",
    "        rnd = tf.random.uniform(tf.shape(x))\n",
    "        x2 = tf.where(rnd < (NOISE_SP_PROB/2.0), 0.0, x)\n",
    "        x2 = tf.where(rnd > 1.0 - (NOISE_SP_PROB/2.0), 1.0, x2)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown noise kind\")\n",
    "    return clip01(x2)\n",
    "\n",
    "@tf.function\n",
    "def predict_labels_logits(model, x):\n",
    "    logits = model(x, training=False)\n",
    "    return tf.argmax(logits, axis=1)\n",
    "\n",
    "def eval_streaming_condition(model, ds, condition):\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    t0 = time.time()\n",
    "    for xb, yb in ds:\n",
    "        if condition == \"Clean\":\n",
    "            x_eval = xb\n",
    "        else:\n",
    "            x_eval = add_noise_batch(xb, condition)\n",
    "        pred = predict_labels_logits(model, x_eval)\n",
    "        y_true_all.append(yb.numpy())\n",
    "        y_pred_all.append(pred.numpy())\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    mf1 = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    sec = float(time.time() - t0)\n",
    "    return acc, mf1, sec\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) ART setup (TF2 eager-safe) + attacks\n",
    "# -----------------------------\n",
    "def require_art():\n",
    "    try:\n",
    "        import art  # noqa: F401\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"ART is required for FGSM/PGD/HSJ/Boundary.\\n\"\n",
    "            \"Install it:\\n\"\n",
    "            \"  pip install adversarial-robustness-toolbox\\n\"\n",
    "            \"Then RESTART kernel / Python.\\n\"\n",
    "            f\"Original import error: {e}\"\n",
    "        )\n",
    "\n",
    "require_art()\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, HopSkipJump, BoundaryAttack\n",
    "\n",
    "# ART expects one-hot for CategoricalCrossentropy most reliably\n",
    "loss_obj = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "art_clf = TensorFlowV2Classifier(\n",
    "    model=cnn,\n",
    "    nb_classes=num_classes,\n",
    "    input_shape=(IMG_H, IMG_W, CH),\n",
    "    loss_object=loss_obj,\n",
    "    optimizer=optimizer,\n",
    "    clip_values=(0.0, 1.0),\n",
    ")\n",
    "\n",
    "fgsm = FastGradientMethod(estimator=art_clf, eps=FGSM_EPS)\n",
    "pgd  = ProjectedGradientDescent(\n",
    "    estimator=art_clf,\n",
    "    eps=PGD_EPS,\n",
    "    eps_step=PGD_ALPHA,\n",
    "    max_iter=PGD_ITERS,\n",
    "    targeted=False\n",
    ")\n",
    "\n",
    "# Handle ART API variations for HSJ/Boundary (estimator= vs classifier=)\n",
    "def make_hsj(estimator):\n",
    "    try:\n",
    "        return HopSkipJump(\n",
    "            estimator=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=HSJ_MAX_ITER,\n",
    "            max_eval=HSJ_MAX_EVAL,\n",
    "            init_eval=HSJ_INIT_EVAL,\n",
    "            init_size=HSJ_INIT_SIZE,\n",
    "        )\n",
    "    except TypeError:\n",
    "        return HopSkipJump(\n",
    "            classifier=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=HSJ_MAX_ITER,\n",
    "            max_eval=HSJ_MAX_EVAL,\n",
    "            init_eval=HSJ_INIT_EVAL,\n",
    "            init_size=HSJ_INIT_SIZE,\n",
    "        )\n",
    "\n",
    "def make_boundary(estimator):\n",
    "    try:\n",
    "        return BoundaryAttack(\n",
    "            estimator=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=BOUNDARY_MAX_ITER,\n",
    "            delta=BOUNDARY_DELTA,\n",
    "            epsilon=BOUNDARY_EPS,\n",
    "        )\n",
    "    except TypeError:\n",
    "        return BoundaryAttack(\n",
    "            classifier=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=BOUNDARY_MAX_ITER,\n",
    "            delta=BOUNDARY_DELTA,\n",
    "            epsilon=BOUNDARY_EPS,\n",
    "        )\n",
    "\n",
    "hsj = make_hsj(art_clf)\n",
    "bnd = make_boundary(art_clf)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Adversarial evaluation (subset -> numpy)\n",
    "# -----------------------------\n",
    "def take_numpy_subset(ds, max_samples):\n",
    "    Xs, Ys = [], []\n",
    "    n = 0\n",
    "    for xb, yb in ds:\n",
    "        Xs.append(xb.numpy().astype(np.float32))\n",
    "        Ys.append(yb.numpy().astype(np.int64))\n",
    "        n += len(yb)\n",
    "        if n >= max_samples:\n",
    "            break\n",
    "    X = np.concatenate(Xs, axis=0)[:max_samples]\n",
    "    y = np.concatenate(Ys, axis=0)[:max_samples]\n",
    "    y_oh = tf.keras.utils.to_categorical(y, num_classes=num_classes).astype(np.float32)\n",
    "    return X, y, y_oh\n",
    "\n",
    "def eval_attack_on_subset(attack, ds, max_samples, batch_size):\n",
    "    X, y, y_oh = take_numpy_subset(ds, max_samples=max_samples)\n",
    "    t0 = time.time()\n",
    "    # many ART attacks accept y optional; providing y usually helps\n",
    "    try:\n",
    "        X_adv = attack.generate(x=X, y=y_oh, batch_size=batch_size)\n",
    "    except Exception:\n",
    "        X_adv = attack.generate(x=X, y=y, batch_size=batch_size)\n",
    "\n",
    "    # evaluate with logits\n",
    "    logits = cnn(tf.convert_to_tensor(X_adv, dtype=tf.float32), training=False).numpy()\n",
    "    pred = logits.argmax(axis=1)\n",
    "    acc = float(accuracy_score(y, pred))\n",
    "    mf1 = float(f1_score(y, pred, average=\"macro\"))\n",
    "    sec = float(time.time() - t0)\n",
    "    return acc, mf1, sec\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Run evals + save\n",
    "# -----------------------------\n",
    "conditions_noise = [\"Clean\", \"Gaussian\", \"Uniform\", \"Dropout\", \"Salt-Pepper\"]\n",
    "conditions_adv   = [\"FGSM\", \"PGD\", \"HopSkipJump\", \"Boundary\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nEvaluating CNN (noise / clean) ...\")\n",
    "for cond in conditions_noise:\n",
    "    acc, mf1, sec = eval_streaming_condition(cnn, val_ds, cond)\n",
    "    results.append({\"Model\": \"CNN-Image\", \"Condition\": cond, \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "    print(f\"  {cond:12s}  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "print(f\"\\nEvaluating CNN (adversarial on first {ADV_MAX_SAMPLES} samples) ...\")\n",
    "acc, mf1, sec = eval_attack_on_subset(fgsm, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"CNN-Image\", \"Condition\": \"FGSM\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  FGSM         acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "acc, mf1, sec = eval_attack_on_subset(pgd, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"CNN-Image\", \"Condition\": \"PGD\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  PGD          acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "acc, mf1, sec = eval_attack_on_subset(hsj, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"CNN-Image\", \"Condition\": \"HopSkipJump\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  HopSkipJump  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "acc, mf1, sec = eval_attack_on_subset(bnd, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"CNN-Image\", \"Condition\": \"Boundary\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  Boundary     acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "df = pd.DataFrame(results).sort_values([\"Condition\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "out_csv = \"malevis_image_cnn_noise_adv_results.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f\"\\nSaved: {out_csv}\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b373048a-c877-4872-a5fb-6977a2f90f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with fixed label mapping...\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n",
      "Classes: 26\n",
      "First 5 classes: ['Adposhel', 'Agent', 'Allaple', 'Amonetize', 'Androm']\n",
      "\n",
      "Training VAE...\n",
      "WARNING:tensorflow:From C:\\Users\\ajayi\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 393ms/step - kl: 8969.8271 - loss: 2190.0793 - recon_sse: 2190.0793 - val_kl: 170633.8438 - val_loss: 1690.6217 - val_recon_sse: 1690.6217\n",
      "Epoch 2/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 385ms/step - kl: 1630.8685 - loss: 2017.8514 - recon_sse: 1854.7640 - val_kl: 212.5781 - val_loss: 1432.5139 - val_recon_sse: 1411.2562\n",
      "Epoch 3/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 382ms/step - kl: 158.3512 - loss: 1365.1257 - recon_sse: 1333.4557 - val_kl: 138.6490 - val_loss: 1320.5923 - val_recon_sse: 1292.8622\n",
      "Epoch 4/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 370ms/step - kl: 119.0207 - loss: 1238.3542 - recon_sse: 1202.6479 - val_kl: 113.9059 - val_loss: 1268.5648 - val_recon_sse: 1234.3928\n",
      "Epoch 5/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 360ms/step - kl: 101.3044 - loss: 1179.6324 - recon_sse: 1139.1108 - val_kl: 95.5915 - val_loss: 1247.0032 - val_recon_sse: 1208.7661\n",
      "Epoch 6/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 360ms/step - kl: 91.1229 - loss: 1180.1328 - recon_sse: 1134.5708 - val_kl: 89.1278 - val_loss: 1229.9264 - val_recon_sse: 1185.3618\n",
      "Epoch 7/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 359ms/step - kl: 80.5588 - loss: 1106.9248 - recon_sse: 1058.5894 - val_kl: 75.8159 - val_loss: 1201.3240 - val_recon_sse: 1155.8342\n",
      "Epoch 8/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 366ms/step - kl: 72.9677 - loss: 1090.0671 - recon_sse: 1038.9895 - val_kl: 66.9811 - val_loss: 1193.7557 - val_recon_sse: 1146.8688\n",
      "Epoch 9/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 363ms/step - kl: 68.8217 - loss: 1095.5912 - recon_sse: 1040.5337 - val_kl: 59.3831 - val_loss: 1177.7781 - val_recon_sse: 1130.2728\n",
      "Epoch 10/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 364ms/step - kl: 61.2454 - loss: 1058.6549 - recon_sse: 1003.5345 - val_kl: 57.3753 - val_loss: 1168.3721 - val_recon_sse: 1116.7338\n",
      "Epoch 11/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 363ms/step - kl: 57.5152 - loss: 1054.5958 - recon_sse: 997.0798 - val_kl: 57.0004 - val_loss: 1170.7644 - val_recon_sse: 1113.7645\n",
      "Epoch 12/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 364ms/step - kl: 57.5286 - loss: 1068.3269 - recon_sse: 1010.7983 - val_kl: 53.0178 - val_loss: 1162.2804 - val_recon_sse: 1109.2637\n",
      "Epoch 13/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 363ms/step - kl: 54.2856 - loss: 1027.3146 - recon_sse: 973.0287 - val_kl: 56.0650 - val_loss: 1160.2158 - val_recon_sse: 1104.1514\n",
      "Epoch 14/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 367ms/step - kl: 67.6363 - loss: 1243.7172 - recon_sse: 1176.0804 - val_kl: 51.9350 - val_loss: 1170.1538 - val_recon_sse: 1118.2190\n",
      "Epoch 15/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 389ms/step - kl: 55.0182 - loss: 1034.6598 - recon_sse: 979.6414 - val_kl: 49.3460 - val_loss: 1140.1073 - val_recon_sse: 1090.7622\n",
      "Epoch 16/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 433ms/step - kl: 52.2394 - loss: 1006.1642 - recon_sse: 953.9250 - val_kl: 47.2843 - val_loss: 1149.5930 - val_recon_sse: 1102.3097\n",
      "Epoch 17/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 416ms/step - kl: 50.6123 - loss: 994.4492 - recon_sse: 943.8369 - val_kl: 53.0648 - val_loss: 1142.3077 - val_recon_sse: 1089.2434\n",
      "Epoch 18/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 411ms/step - kl: 51.9390 - loss: 1002.5568 - recon_sse: 950.6176 - val_kl: 48.2222 - val_loss: 1127.0662 - val_recon_sse: 1078.8434\n",
      "Epoch 19/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 390ms/step - kl: 53.6870 - loss: 1030.4502 - recon_sse: 976.7631 - val_kl: 49.3694 - val_loss: 1119.6644 - val_recon_sse: 1070.2963\n",
      "Epoch 20/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 384ms/step - kl: 49.4702 - loss: 980.1037 - recon_sse: 930.6338 - val_kl: 49.0810 - val_loss: 1141.1161 - val_recon_sse: 1092.0352\n",
      "Epoch 21/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 457ms/step - kl: 48.2554 - loss: 972.4890 - recon_sse: 924.2337 - val_kl: 49.9964 - val_loss: 1125.3691 - val_recon_sse: 1075.3735\n",
      "Epoch 22/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 402ms/step - kl: 48.0267 - loss: 966.3640 - recon_sse: 918.3370 - val_kl: 49.8460 - val_loss: 1141.8293 - val_recon_sse: 1091.9838\n",
      "Epoch 23/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 372ms/step - kl: 47.9376 - loss: 974.0828 - recon_sse: 926.1455 - val_kl: 46.5603 - val_loss: 1105.1818 - val_recon_sse: 1058.6215\n",
      "Epoch 24/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 375ms/step - kl: 47.9108 - loss: 972.9204 - recon_sse: 925.0100 - val_kl: 44.4882 - val_loss: 1099.8792 - val_recon_sse: 1055.3907\n",
      "Epoch 25/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 396ms/step - kl: 46.6111 - loss: 953.0358 - recon_sse: 906.4248 - val_kl: 42.2766 - val_loss: 1095.0769 - val_recon_sse: 1052.8007\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n",
      "\n",
      "Latent clip bounds: -2.344692347049713 2.3626357209682456\n",
      "\n",
      "Training Latent CNN...\n",
      "Epoch 1/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 247ms/step - accuracy: 0.1678 - loss: 2.9982 - val_accuracy: 0.4218 - val_loss: 2.2652\n",
      "Epoch 2/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - accuracy: 0.5510 - loss: 1.7645 - val_accuracy: 0.5572 - val_loss: 1.5431\n",
      "Epoch 3/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 66ms/step - accuracy: 0.6730 - loss: 1.2233 - val_accuracy: 0.5813 - val_loss: 1.4479\n",
      "Epoch 4/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - accuracy: 0.7190 - loss: 1.0385 - val_accuracy: 0.6773 - val_loss: 1.1458\n",
      "Epoch 5/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 0.7540 - loss: 0.9041 - val_accuracy: 0.6937 - val_loss: 1.1406\n",
      "Epoch 6/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - accuracy: 0.7835 - loss: 0.8007 - val_accuracy: 0.7060 - val_loss: 1.0958\n",
      "Epoch 7/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 66ms/step - accuracy: 0.7928 - loss: 0.7499 - val_accuracy: 0.7095 - val_loss: 1.0792\n",
      "Epoch 8/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 73ms/step - accuracy: 0.8013 - loss: 0.7022 - val_accuracy: 0.6927 - val_loss: 1.1453\n",
      "Epoch 9/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 0.8102 - loss: 0.6574 - val_accuracy: 0.6789 - val_loss: 1.1965\n",
      "Epoch 10/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.8205 - loss: 0.6078 - val_accuracy: 0.7072 - val_loss: 1.0993\n",
      "Epoch 11/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - accuracy: 0.8281 - loss: 0.5919 - val_accuracy: 0.7058 - val_loss: 1.1218\n",
      "\n",
      "Training Latent RNN...\n",
      "Epoch 1/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 382ms/step - accuracy: 0.1276 - loss: 3.1239 - val_accuracy: 0.2587 - val_loss: 2.5750\n",
      "Epoch 2/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 386ms/step - accuracy: 0.3815 - loss: 2.2360 - val_accuracy: 0.3812 - val_loss: 2.1195\n",
      "Epoch 3/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 392ms/step - accuracy: 0.4860 - loss: 1.8193 - val_accuracy: 0.4372 - val_loss: 1.8974\n",
      "Epoch 4/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 430ms/step - accuracy: 0.5747 - loss: 1.5251 - val_accuracy: 0.5121 - val_loss: 1.6539\n",
      "Epoch 5/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 386ms/step - accuracy: 0.6458 - loss: 1.3169 - val_accuracy: 0.5057 - val_loss: 1.7163\n",
      "Epoch 6/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 387ms/step - accuracy: 0.6741 - loss: 1.1994 - val_accuracy: 0.5404 - val_loss: 1.5562\n",
      "Epoch 7/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 380ms/step - accuracy: 0.6999 - loss: 1.1305 - val_accuracy: 0.5470 - val_loss: 1.5251\n",
      "Epoch 8/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 380ms/step - accuracy: 0.7154 - loss: 1.0271 - val_accuracy: 0.6030 - val_loss: 1.3428\n",
      "Epoch 9/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 426ms/step - accuracy: 0.7349 - loss: 0.9477 - val_accuracy: 0.6247 - val_loss: 1.2523\n",
      "Epoch 10/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 413ms/step - accuracy: 0.7586 - loss: 0.8570 - val_accuracy: 0.6124 - val_loss: 1.3231\n",
      "Epoch 11/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 389ms/step - accuracy: 0.7652 - loss: 0.8201 - val_accuracy: 0.6151 - val_loss: 1.3006\n",
      "Epoch 12/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 412ms/step - accuracy: 0.7633 - loss: 0.8463 - val_accuracy: 0.6483 - val_loss: 1.1966\n",
      "Epoch 13/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 417ms/step - accuracy: 0.7778 - loss: 0.7848 - val_accuracy: 0.6561 - val_loss: 1.2030\n",
      "Epoch 14/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 418ms/step - accuracy: 0.7926 - loss: 0.7221 - val_accuracy: 0.6477 - val_loss: 1.2405\n",
      "Epoch 15/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 405ms/step - accuracy: 0.7842 - loss: 0.7371 - val_accuracy: 0.6629 - val_loss: 1.1926\n",
      "\n",
      "Evaluating Latent-CNN ...\n",
      "  Clean         acc=0.7095  macroF1=0.7716  time=1.6s\n",
      "  Gaussian      acc=0.6631  macroF1=0.7272  time=1.5s\n",
      "  Uniform       acc=0.6937  macroF1=0.7569  time=1.5s\n",
      "  Dropout       acc=0.4721  macroF1=0.5348  time=1.4s\n",
      "  Salt-Pepper   acc=0.4187  macroF1=0.4772  time=1.5s\n",
      "  FGSM          acc=0.0092  macroF1=0.0066  time=5.8s\n",
      "  PGD           acc=0.0000  macroF1=0.0000  time=45.6s\n",
      "\n",
      "Evaluating Latent-RNN ...\n",
      "  Clean         acc=0.6629  macroF1=0.7262  time=58.3s\n",
      "  Gaussian      acc=0.5195  macroF1=0.5732  time=59.8s\n",
      "  Uniform       acc=0.6091  macroF1=0.6674  time=59.4s\n",
      "  Dropout       acc=0.3568  macroF1=0.3927  time=57.9s\n",
      "  Salt-Pepper   acc=0.3391  macroF1=0.3741  time=58.9s\n",
      "  FGSM          acc=0.0154  macroF1=0.0094  time=175.2s\n",
      "  PGD           acc=0.0000  macroF1=0.0000  time=1400.4s\n",
      "\n",
      "================================================================================\n",
      "RESULTS:\n",
      "     Model   Condition  Accuracy  Macro-F1     Time(s)\n",
      "Latent-CNN       Clean  0.709520  0.771637    1.565470\n",
      "Latent-RNN       Clean  0.662895  0.726158   58.263085\n",
      "Latent-CNN     Dropout  0.472103  0.534799    1.430795\n",
      "Latent-RNN     Dropout  0.356808  0.392671   57.887914\n",
      "Latent-CNN        FGSM  0.009169  0.006569    5.789823\n",
      "Latent-RNN        FGSM  0.015412  0.009449  175.213934\n",
      "Latent-CNN    Gaussian  0.663090  0.727210    1.548250\n",
      "Latent-RNN    Gaussian  0.519508  0.573200   59.759185\n",
      "Latent-CNN         PGD  0.000000  0.000000   45.574011\n",
      "Latent-RNN         PGD  0.000000  0.000000 1400.442961\n",
      "Latent-CNN Salt-Pepper  0.418650  0.477169    1.518542\n",
      "Latent-RNN Salt-Pepper  0.339056  0.374086   58.929537\n",
      "Latent-CNN     Uniform  0.693718  0.756922    1.466306\n",
      "Latent-RNN     Uniform  0.609052  0.667399   59.413692\n",
      "\n",
      "Saved: malevis_latent_cnn_rnn_results.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "MaleVis: VAE Latent -> (CNN + RNN) with Clean + Perturbations + Adversarial (Latent Space)\n",
    "\n",
    "Key features:\n",
    "- Uses local directory with train/ and val/\n",
    "- FIXED class mapping: val uses class_names from train (prevents label mismatch)\n",
    "- Streaming tf.data (NO ds_to_numpy)\n",
    "- VAE uses KL warmup via model.beta (no missing 'kl' layer)\n",
    "- Classifiers use deterministic z_mean (stable)\n",
    "- Evaluates: Clean + Gaussian/Uniform/Dropout/SaltPepper + FGSM/PGD on latent\n",
    "\n",
    "Output:\n",
    "- Prints results table\n",
    "- Saves: malevis_latent_cnn_rnn_results.csv\n",
    "\"\"\"\n",
    "\n",
    "import os, gc, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = r\"C:\\Users\\ajayi\\Downloads\\malevis_train_val_300x300\\malevis_train_val_300x300\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n",
    "\n",
    "IMG_H, IMG_W, CH = 128, 128, 3\n",
    "LATENT_DIM = 128  # set to your chosen k (or best_k from sweep)\n",
    "\n",
    "# VAE\n",
    "VAE_EPOCHS = 25\n",
    "VAE_BS     = 8\n",
    "VAE_LR     = 1e-3\n",
    "\n",
    "# KL warmup\n",
    "BETA_MAX = 1.0\n",
    "BETA_WARMUP_EPOCHS = 10\n",
    "\n",
    "# Classifiers\n",
    "CLS_EPOCHS = 15\n",
    "CLS_BS     = 128\n",
    "CLS_LR     = 1e-3\n",
    "\n",
    "# Latent perturbations\n",
    "NOISE_GAUSS_SIG = 0.15\n",
    "NOISE_UNIF_RNG  = 0.15\n",
    "NOISE_DROPOUT   = 0.25\n",
    "NOISE_SP_PROB   = 0.02\n",
    "\n",
    "# Adversarial (latent, Linf)\n",
    "FGSM_EPS_LAT   = 0.25\n",
    "PGD_EPS_LAT    = 0.35\n",
    "PGD_ALPHA_LAT  = 0.06\n",
    "PGD_ITERS_LAT  = 10\n",
    "\n",
    "# GPU memory growth (safe)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Dataset loading (FIXED mapping)\n",
    "# -----------------------------\n",
    "def make_ds_fixed(train_dir, val_dir, img_size, batch_size, seed):\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names = train_ds.class_names  # canonical mapping\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_names=class_names   # <-- critical fix\n",
    "    )\n",
    "\n",
    "    def norm(x, y):\n",
    "        x = tf.cast(x, tf.float32) / 255.0\n",
    "        return x, y\n",
    "\n",
    "    # DON'T cache images in RAM for 300x300; just prefetch\n",
    "    train_ds = train_ds.map(norm, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = val_ds.map(norm,   num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds, class_names, len(class_names)\n",
    "\n",
    "print(\"Loading datasets with fixed label mapping...\")\n",
    "train_ds_vae, val_ds_vae, class_names, num_classes = make_ds_fixed(\n",
    "    TRAIN_DIR, VAL_DIR, (IMG_H, IMG_W), batch_size=VAE_BS, seed=SEED\n",
    ")\n",
    "print(\"Classes:\", num_classes)\n",
    "print(\"First 5 classes:\", class_names[:5])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Conv VAE (subclassed, beta warmup)\n",
    "# -----------------------------\n",
    "def _compute_decoder_grid(img_h, img_w, downsamples=4):\n",
    "    div = 2 ** downsamples\n",
    "    gh = int(math.ceil(img_h / div))\n",
    "    gw = int(math.ceil(img_w / div))\n",
    "    out_h = gh * div\n",
    "    out_w = gw * div\n",
    "    return gh, gw, out_h, out_w\n",
    "\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    inp = layers.Input(shape=input_shape, name=f\"enc_in_k{latent_dim}\")\n",
    "    x = layers.Conv2D(32, 4, strides=2, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.Conv2D(64, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(128, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(256, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=f\"z_mean_k{latent_dim}\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=f\"z_log_var_k{latent_dim}\")(x)\n",
    "    return tf.keras.Model(inp, [z_mean, z_log_var], name=f\"encoder_k{latent_dim}\")\n",
    "\n",
    "def build_decoder(output_shape, latent_dim):\n",
    "    img_h, img_w, ch = output_shape\n",
    "    gh, gw, out_h, out_w = _compute_decoder_grid(img_h, img_w, downsamples=4)\n",
    "\n",
    "    inp = layers.Input(shape=(latent_dim,), name=f\"dec_in_k{latent_dim}\")\n",
    "    x = layers.Dense(gh * gw * 256, activation=\"relu\")(inp)\n",
    "    x = layers.Reshape((gh, gw, 256))(x)\n",
    "    x = layers.Conv2DTranspose(256, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(128, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(64, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(32, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(ch, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "\n",
    "    crop_h = max(0, out_h - img_h)\n",
    "    crop_w = max(0, out_w - img_w)\n",
    "    if crop_h > 0 or crop_w > 0:\n",
    "        top = crop_h // 2\n",
    "        bot = crop_h - top\n",
    "        left = crop_w // 2\n",
    "        right = crop_w - left\n",
    "        x = layers.Cropping2D(cropping=((top, bot), (left, right)))(x)\n",
    "\n",
    "    return tf.keras.Model(inp, x, name=f\"decoder_k{latent_dim}\")\n",
    "\n",
    "class ConvVAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, logvar_clip=10.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = tf.Variable(0.0, trainable=False, dtype=tf.float32, name=\"beta\")\n",
    "        self.logvar_clip = float(logvar_clip)\n",
    "\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.recon_tracker = tf.keras.metrics.Mean(name=\"recon_sse\")\n",
    "        self.kl_tracker = tf.keras.metrics.Mean(name=\"kl\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.recon_tracker, self.kl_tracker]\n",
    "\n",
    "    def _sample(self, z_mean, z_log_var):\n",
    "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(x, training=True)\n",
    "            z_log_var = tf.clip_by_value(z_log_var, -self.logvar_clip, self.logvar_clip)\n",
    "            z = self._sample(z_mean, z_log_var)\n",
    "            x_hat = self.decoder(z, training=True)\n",
    "\n",
    "            recon_sse = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=[1,2,3]))\n",
    "            kl = tf.reduce_mean(\n",
    "                -0.5 * tf.reduce_sum(1.0 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "            )\n",
    "            loss = recon_sse + self.beta * kl\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.recon_tracker.update_state(recon_sse)\n",
    "        self.kl_tracker.update_state(kl)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x = data\n",
    "        z_mean, z_log_var = self.encoder(x, training=False)\n",
    "        z_log_var = tf.clip_by_value(z_log_var, -self.logvar_clip, self.logvar_clip)\n",
    "        z = self._sample(z_mean, z_log_var)\n",
    "        x_hat = self.decoder(z, training=False)\n",
    "\n",
    "        recon_sse = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=[1,2,3]))\n",
    "        kl = tf.reduce_mean(\n",
    "            -0.5 * tf.reduce_sum(1.0 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        loss = recon_sse + self.beta * kl\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.recon_tracker.update_state(recon_sse)\n",
    "        self.kl_tracker.update_state(kl)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def encode_mean(self, x):\n",
    "        z_mean, _ = self.encoder(x, training=False)\n",
    "        return z_mean\n",
    "\n",
    "class BetaWarmup(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, beta_max=1.0, warmup_epochs=10):\n",
    "        super().__init__()\n",
    "        self.beta_max = float(beta_max)\n",
    "        self.warmup_epochs = int(max(1, warmup_epochs))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        beta = self.beta_max * min(1.0, epoch / float(self.warmup_epochs))\n",
    "        self.model.beta.assign(beta)\n",
    "\n",
    "\n",
    "# Train VAE (images only)\n",
    "print(\"\\nTraining VAE...\")\n",
    "tf.keras.backend.clear_session()\n",
    "encoder = build_encoder((IMG_H, IMG_W, CH), LATENT_DIM)\n",
    "decoder = build_decoder((IMG_H, IMG_W, CH), LATENT_DIM)\n",
    "vae = ConvVAE(encoder, decoder, name=f\"vae_k{LATENT_DIM}\")\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(VAE_LR))\n",
    "\n",
    "vae_train_img = train_ds_vae.map(lambda x, y: x)\n",
    "vae_val_img   = val_ds_vae.map(lambda x, y: x)\n",
    "\n",
    "vae.fit(\n",
    "    vae_train_img,\n",
    "    validation_data=vae_val_img,\n",
    "    epochs=VAE_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        BetaWarmup(beta_max=BETA_MAX, warmup_epochs=BETA_WARMUP_EPOCHS),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build latent datasets (cache latents, safe)\n",
    "# -----------------------------\n",
    "def make_image_ds(dir_path, class_names, img_size, batch_size, shuffle, seed):\n",
    "    ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dir_path,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=seed if shuffle else None,\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    ds = ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds_cls = make_image_ds(TRAIN_DIR, class_names, (IMG_H, IMG_W), CLS_BS, shuffle=True,  seed=SEED)\n",
    "val_ds_cls   = make_image_ds(VAL_DIR,   class_names, (IMG_H, IMG_W), CLS_BS, shuffle=False, seed=SEED)\n",
    "\n",
    "def make_latent_ds(image_ds, vae, cache_latents=True, shuffle=False):\n",
    "    def to_latent(x, y):\n",
    "        z = vae.encode_mean(x)          # (B,k)\n",
    "        z = tf.expand_dims(z, axis=-1)  # (B,k,1)\n",
    "        return z, y\n",
    "\n",
    "    out = image_ds.map(to_latent, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if cache_latents:\n",
    "        out = out.cache()\n",
    "    if shuffle:\n",
    "        out = out.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
    "    return out.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "lat_train_ds = make_latent_ds(train_ds_cls, vae, cache_latents=True, shuffle=True)\n",
    "lat_val_ds   = make_latent_ds(val_ds_cls,   vae, cache_latents=True, shuffle=False)\n",
    "\n",
    "def estimate_latent_bounds(latent_ds, q_lo=0.5, q_hi=99.5, take_batches=50):\n",
    "    zs = []\n",
    "    for zb, _ in latent_ds.take(take_batches):\n",
    "        zs.append(tf.reshape(zb, [-1]).numpy())\n",
    "    zs = np.concatenate(zs, axis=0)\n",
    "    return float(np.percentile(zs, q_lo)), float(np.percentile(zs, q_hi))\n",
    "\n",
    "z_min, z_max = estimate_latent_bounds(lat_train_ds)\n",
    "print(\"\\nLatent clip bounds:\", z_min, z_max)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build latent CNN + latent RNN\n",
    "# -----------------------------\n",
    "def build_latent_cnn(latent_dim, n_classes):\n",
    "    inp = layers.Input(shape=(latent_dim, 1))\n",
    "    x = layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    m = tf.keras.Model(inp, out, name=\"latent_cnn\")\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(CLS_LR),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "def build_latent_rnn(latent_dim, n_classes):\n",
    "    inp = layers.Input(shape=(latent_dim, 1))\n",
    "    x = layers.Bidirectional(layers.LSTM(96, return_sequences=False))(inp)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(192, activation=\"relu\")(x)\n",
    "    out = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    m = tf.keras.Model(inp, out, name=\"latent_rnn\")\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(CLS_LR),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "cnn_lat = build_latent_cnn(LATENT_DIM, num_classes)\n",
    "rnn_lat = build_latent_rnn(LATENT_DIM, num_classes)\n",
    "\n",
    "print(\"\\nTraining Latent CNN...\")\n",
    "cnn_lat.fit(\n",
    "    lat_train_ds,\n",
    "    validation_data=lat_val_ds,\n",
    "    epochs=CLS_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Latent RNN...\")\n",
    "rnn_lat.fit(\n",
    "    lat_train_ds,\n",
    "    validation_data=lat_val_ds,\n",
    "    epochs=CLS_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Latent perturbations + attacks\n",
    "# -----------------------------\n",
    "def clip_latent(z):\n",
    "    return tf.clip_by_value(z, z_min, z_max)\n",
    "\n",
    "def latent_noise(z, kind):\n",
    "    if kind == \"Gaussian\":\n",
    "        z2 = z + tf.random.normal(tf.shape(z), stddev=NOISE_GAUSS_SIG)\n",
    "    elif kind == \"Uniform\":\n",
    "        z2 = z + tf.random.uniform(tf.shape(z), minval=-NOISE_UNIF_RNG, maxval=NOISE_UNIF_RNG)\n",
    "    elif kind == \"Dropout\":\n",
    "        mask = tf.cast(tf.random.uniform(tf.shape(z)) > NOISE_DROPOUT, tf.float32)\n",
    "        z2 = z * mask\n",
    "    elif kind == \"Salt-Pepper\":\n",
    "        rnd = tf.random.uniform(tf.shape(z))\n",
    "        z2 = tf.where(rnd < (NOISE_SP_PROB/2.0), tf.cast(z_min, tf.float32), z)\n",
    "        z2 = tf.where(rnd > 1.0 - (NOISE_SP_PROB/2.0), tf.cast(z_max, tf.float32), z2)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown noise kind\")\n",
    "    return clip_latent(z2)\n",
    "\n",
    "cce_sparse = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def fgsm_latent(model, z, y, eps):\n",
    "    z = tf.identity(z)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(z)\n",
    "        probs = model(z, training=False)\n",
    "        loss = cce_sparse(y, probs)\n",
    "    grad = tape.gradient(loss, z)\n",
    "    z_adv = z + eps * tf.sign(grad)\n",
    "    return clip_latent(z_adv)\n",
    "\n",
    "def pgd_latent(model, z0, y, eps, alpha, iters):\n",
    "    z_adv = z0 + tf.random.uniform(tf.shape(z0), minval=-eps, maxval=eps)\n",
    "    z_adv = clip_latent(z_adv)\n",
    "    for _ in range(iters):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(z_adv)\n",
    "            probs = model(z_adv, training=False)\n",
    "            loss = cce_sparse(y, probs)\n",
    "        grad = tape.gradient(loss, z_adv)\n",
    "        z_adv = z_adv + alpha * tf.sign(grad)\n",
    "\n",
    "        delta = tf.clip_by_value(z_adv - z0, -eps, eps)\n",
    "        z_adv = clip_latent(z0 + delta)\n",
    "    return z_adv\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Evaluation on Clean + Perturbations + Adversarial\n",
    "# -----------------------------\n",
    "def eval_model_on_condition(model, latent_ds, condition):\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for zb, yb in latent_ds:\n",
    "        if condition == \"Clean\":\n",
    "            z_eval = zb\n",
    "        elif condition in [\"Gaussian\", \"Uniform\", \"Dropout\", \"Salt-Pepper\"]:\n",
    "            z_eval = latent_noise(zb, condition)\n",
    "        elif condition == \"FGSM\":\n",
    "            z_eval = fgsm_latent(model, zb, yb, FGSM_EPS_LAT)\n",
    "        elif condition == \"PGD\":\n",
    "            z_eval = pgd_latent(model, zb, yb, PGD_EPS_LAT, PGD_ALPHA_LAT, PGD_ITERS_LAT)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown condition\")\n",
    "\n",
    "        probs = model(z_eval, training=False)\n",
    "        pred = tf.argmax(probs, axis=1)\n",
    "\n",
    "        y_true_all.append(yb.numpy())\n",
    "        y_pred_all.append(pred.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    macro_f1 = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    elapsed = float(time.time() - t0)\n",
    "\n",
    "    return acc, macro_f1, elapsed\n",
    "\n",
    "conditions = [\"Clean\", \"Gaussian\", \"Uniform\", \"Dropout\", \"Salt-Pepper\", \"FGSM\", \"PGD\"]\n",
    "\n",
    "results = []\n",
    "for mname, model in [(\"Latent-CNN\", cnn_lat), (\"Latent-RNN\", rnn_lat)]:\n",
    "    print(f\"\\nEvaluating {mname} ...\")\n",
    "    for cond in conditions:\n",
    "        acc, mf1, sec = eval_model_on_condition(model, lat_val_ds, cond)\n",
    "        results.append({\"Model\": mname, \"Condition\": cond, \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "        print(f\"  {cond:12s}  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "df = pd.DataFrame(results).sort_values([\"Condition\", \"Model\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "df.to_csv(\"malevis_latent_cnn_rnn_results.csv\", index=False)\n",
    "print(\"\\nSaved: malevis_latent_cnn_rnn_results.csv\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5ed93-52ec-4370-9105-7a561273f18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with fixed label mapping...\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n",
      "Classes: 26\n",
      "First 5 classes: ['Adposhel', 'Agent', 'Allaple', 'Amonetize', 'Androm']\n",
      "\n",
      "Training RNN...\n",
      "Epoch 1/15\n",
      "\u001b[1m569/569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 1s/step - accuracy: 0.8012 - loss: 0.8306 - val_accuracy: 0.8125 - val_loss: 0.6828\n",
      "Epoch 2/15\n",
      "\u001b[1m569/569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m578s\u001b[0m 1s/step - accuracy: 0.9799 - loss: 0.0733 - val_accuracy: 0.8303 - val_loss: 0.8345\n",
      "Epoch 3/15\n",
      "\u001b[1m569/569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m643s\u001b[0m 1s/step - accuracy: 0.9953 - loss: 0.0164 - val_accuracy: 0.8266 - val_loss: 1.0461\n",
      "Epoch 4/15\n",
      "\u001b[1m569/569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 1s/step - accuracy: 0.9909 - loss: 0.0273 - val_accuracy: 0.8311 - val_loss: 1.1151\n",
      "Epoch 5/15\n",
      "\u001b[1m569/569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m721s\u001b[0m 1s/step - accuracy: 0.9910 - loss: 0.0330 - val_accuracy: 0.8301 - val_loss: 1.1815\n",
      "Epoch 6/15\n",
      "\u001b[1m569/569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 1s/step - accuracy: 0.9919 - loss: 0.0285 - val_accuracy: 0.8275 - val_loss: 1.2596\n",
      "Epoch 7/15\n",
      "\u001b[1m569/569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m792s\u001b[0m 1s/step - accuracy: 0.9969 - loss: 0.0116 - val_accuracy: 0.8459 - val_loss: 1.2092\n",
      "Epoch 8/15\n",
      "\u001b[1m569/569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m780s\u001b[0m 1s/step - accuracy: 0.9939 - loss: 0.0180 - val_accuracy: 0.8439 - val_loss: 1.3147\n",
      "Epoch 9/15\n",
      "\u001b[1m569/569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m790s\u001b[0m 1s/step - accuracy: 0.9952 - loss: 0.0182 - val_accuracy: 0.8373 - val_loss: 1.3814\n",
      "Epoch 10/15\n",
      "\u001b[1m569/569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m780s\u001b[0m 1s/step - accuracy: 0.9928 - loss: 0.0244 - val_accuracy: 0.8402 - val_loss: 1.3043\n",
      "Epoch 11/15\n",
      "\u001b[1m569/569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 1s/step - accuracy: 0.9947 - loss: 0.0193 - val_accuracy: 0.8420 - val_loss: 1.5365\n",
      "\n",
      "Sanity check (Keras evaluate on CLEAN val): [1.2092128992080688, 0.8458837270736694]\n",
      "\n",
      "Evaluating RNN (noise / clean) ...\n",
      "  Clean         acc=0.8459  macroF1=0.8932  time=71.5s\n",
      "  Gaussian      acc=0.8309  macroF1=0.8770  time=81.6s\n",
      "  Uniform       acc=0.8408  macroF1=0.8863  time=72.9s\n",
      "  Dropout       acc=0.7710  macroF1=0.8158  time=72.4s\n",
      "  Salt-Pepper   acc=0.8291  macroF1=0.8757  time=83.6s\n",
      "\n",
      "Evaluating RNN (adversarial on first 64 samples) ...\n",
      "  FGSM         acc=0.0000  macroF1=0.0000  time=67.6s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82c94997fc94d14975f17faa41842af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PGD          acc=0.0000  macroF1=0.0000  time=612.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daab4356f36745f19154b95e14c33b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HopSkipJump:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "MaleVis (IMAGE SPACE) - RNN classifier:\n",
    "- Clean evaluation\n",
    "- Noise perturbations: Gaussian / Uniform / Dropout / Salt-Pepper\n",
    "- Adversarial attacks via ART: FGSM / PGD / HopSkipJump / BoundaryAttack\n",
    "\n",
    "Key features:\n",
    "- Uses local directory with train/ and val/\n",
    "- FIXED class mapping: val uses class_names from train (prevents label mismatch)\n",
    "- Streaming tf.data for training/eval (no ds_to_numpy full)\n",
    "- Adversarial attacks run on a SMALL SUBSET (ADV_MAX_SAMPLES) to avoid RAM blowups\n",
    "- Uses ART TensorFlowV2Classifier (works with TF2 eager)\n",
    "\n",
    "Output:\n",
    "- Prints results table\n",
    "- Saves: malevis_image_rnn_noise_adv_results.csv\n",
    "\"\"\"\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = r\"C:\\Users\\ajayi\\Downloads\\malevis_train_val_300x300\\malevis_train_val_300x300\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n",
    "\n",
    "IMG_H, IMG_W, CH = 300, 300, 3\n",
    "\n",
    "# Training\n",
    "EPOCHS = 15\n",
    "BS     = 16          # RNN is heavier; 16 is safer than 32 for 300x300\n",
    "LR     = 1e-3\n",
    "\n",
    "# Noise perturbations (image-space)\n",
    "NOISE_GAUSS_SIG = 0.10\n",
    "NOISE_UNIF_RNG  = 0.10\n",
    "NOISE_DROPOUT   = 0.20\n",
    "NOISE_SP_PROB   = 0.02\n",
    "\n",
    "# Adversarial subset (IMPORTANT for memory/time)\n",
    "ADV_MAX_SAMPLES = 64\n",
    "ADV_BATCH_SIZE  = 4        # RNN + 300x300 -> keep small\n",
    "\n",
    "# FGSM/PGD (Linf) eps in [0,1] pixel space\n",
    "FGSM_EPS  = 8/255\n",
    "PGD_EPS   = 8/255\n",
    "PGD_ALPHA = 2/255\n",
    "PGD_ITERS = 10\n",
    "\n",
    "# HSJ / Boundary (black-box) — slow, keep small subset\n",
    "HSJ_MAX_ITER    = 20\n",
    "HSJ_MAX_EVAL    = 2000\n",
    "HSJ_INIT_EVAL   = 50\n",
    "HSJ_INIT_SIZE   = 50\n",
    "\n",
    "BOUNDARY_MAX_ITER = 200\n",
    "BOUNDARY_DELTA    = 0.01\n",
    "BOUNDARY_EPS      = 0.01\n",
    "\n",
    "# GPU memory growth (safe)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Dataset loading (FIXED mapping)\n",
    "# -----------------------------\n",
    "def make_ds_fixed(train_dir, val_dir, img_size, batch_size, seed):\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    def norm(x, y):\n",
    "        x = tf.cast(x, tf.float32) / 255.0\n",
    "        return x, y\n",
    "\n",
    "    train_ds = train_ds.map(norm, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = val_ds.map(norm,   num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds, class_names, len(class_names)\n",
    "\n",
    "print(\"Loading datasets with fixed label mapping...\")\n",
    "train_ds, val_ds, class_names, num_classes = make_ds_fixed(\n",
    "    TRAIN_DIR, VAL_DIR, (IMG_H, IMG_W), batch_size=BS, seed=SEED\n",
    ")\n",
    "print(\"Classes:\", num_classes)\n",
    "print(\"First 5 classes:\", class_names[:5])\n",
    "\n",
    "# -----------------------------\n",
    "# 2) RNN model (logits output for stability)\n",
    "# -----------------------------\n",
    "def build_rnn(num_classes):\n",
    "    inp = layers.Input(shape=(IMG_H, IMG_W, CH), name=\"img_in\")\n",
    "\n",
    "    # Treat image as sequence of rows: timesteps=IMG_H, features=IMG_W*CH\n",
    "    x = layers.Reshape((IMG_H, IMG_W * CH), name=\"to_seq\")(inp)\n",
    "\n",
    "    # Normalize sequence features (helps RNN stability)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # RNN backbone\n",
    "    # (GRU is faster; LSTM sometimes slightly better. Pick one.)\n",
    "    x = layers.Bidirectional(layers.GRU(128, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    x = layers.Bidirectional(layers.GRU(128, return_sequences=False))(x)\n",
    "\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.30)(x)\n",
    "\n",
    "    logits = layers.Dense(num_classes, activation=None, name=\"logits\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, logits, name=\"malevis_rnn_logits\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(LR),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "rnn = build_rnn(num_classes)\n",
    "\n",
    "print(\"\\nTraining RNN...\")\n",
    "rnn.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "print(\"\\nSanity check (Keras evaluate on CLEAN val):\", rnn.evaluate(val_ds, verbose=0))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Noise perturbations (streaming, no numpy)\n",
    "# -----------------------------\n",
    "def clip01(x):\n",
    "    return tf.clip_by_value(x, 0.0, 1.0)\n",
    "\n",
    "def add_noise_batch(x, kind):\n",
    "    if kind == \"Gaussian\":\n",
    "        x2 = x + tf.random.normal(tf.shape(x), stddev=NOISE_GAUSS_SIG)\n",
    "    elif kind == \"Uniform\":\n",
    "        x2 = x + tf.random.uniform(tf.shape(x), minval=-NOISE_UNIF_RNG, maxval=NOISE_UNIF_RNG)\n",
    "    elif kind == \"Dropout\":\n",
    "        mask = tf.cast(tf.random.uniform(tf.shape(x)) > NOISE_DROPOUT, tf.float32)\n",
    "        x2 = x * mask\n",
    "    elif kind == \"Salt-Pepper\":\n",
    "        rnd = tf.random.uniform(tf.shape(x))\n",
    "        x2 = tf.where(rnd < (NOISE_SP_PROB/2.0), 0.0, x)\n",
    "        x2 = tf.where(rnd > 1.0 - (NOISE_SP_PROB/2.0), 1.0, x2)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown noise kind\")\n",
    "    return clip01(x2)\n",
    "\n",
    "@tf.function\n",
    "def predict_labels_logits(model, x):\n",
    "    logits = model(x, training=False)\n",
    "    return tf.argmax(logits, axis=1)\n",
    "\n",
    "def eval_streaming_condition(model, ds, condition):\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    t0 = time.time()\n",
    "    for xb, yb in ds:\n",
    "        x_eval = xb if condition == \"Clean\" else add_noise_batch(xb, condition)\n",
    "        pred = predict_labels_logits(model, x_eval)\n",
    "        y_true_all.append(yb.numpy())\n",
    "        y_pred_all.append(pred.numpy())\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    mf1 = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    sec = float(time.time() - t0)\n",
    "    return acc, mf1, sec\n",
    "\n",
    "# -----------------------------\n",
    "# 4) ART setup (TF2 eager-safe) + attacks\n",
    "# -----------------------------\n",
    "def require_art():\n",
    "    try:\n",
    "        import art  # noqa: F401\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"ART is required for FGSM/PGD/HSJ/Boundary.\\n\"\n",
    "            \"Install it:\\n\"\n",
    "            \"  pip install adversarial-robustness-toolbox\\n\"\n",
    "            \"Then RESTART kernel / Python.\\n\"\n",
    "            f\"Original import error: {e}\"\n",
    "        )\n",
    "\n",
    "require_art()\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, HopSkipJump, BoundaryAttack\n",
    "\n",
    "loss_obj = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "art_clf = TensorFlowV2Classifier(\n",
    "    model=rnn,\n",
    "    nb_classes=num_classes,\n",
    "    input_shape=(IMG_H, IMG_W, CH),\n",
    "    loss_object=loss_obj,\n",
    "    optimizer=optimizer,\n",
    "    clip_values=(0.0, 1.0),\n",
    ")\n",
    "\n",
    "fgsm = FastGradientMethod(estimator=art_clf, eps=FGSM_EPS)\n",
    "pgd  = ProjectedGradientDescent(\n",
    "    estimator=art_clf,\n",
    "    eps=PGD_EPS,\n",
    "    eps_step=PGD_ALPHA,\n",
    "    max_iter=PGD_ITERS,\n",
    "    targeted=False\n",
    ")\n",
    "\n",
    "def make_hsj(estimator):\n",
    "    try:\n",
    "        return HopSkipJump(\n",
    "            estimator=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=HSJ_MAX_ITER,\n",
    "            max_eval=HSJ_MAX_EVAL,\n",
    "            init_eval=HSJ_INIT_EVAL,\n",
    "            init_size=HSJ_INIT_SIZE,\n",
    "        )\n",
    "    except TypeError:\n",
    "        return HopSkipJump(\n",
    "            classifier=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=HSJ_MAX_ITER,\n",
    "            max_eval=HSJ_MAX_EVAL,\n",
    "            init_eval=HSJ_INIT_EVAL,\n",
    "            init_size=HSJ_INIT_SIZE,\n",
    "        )\n",
    "\n",
    "def make_boundary(estimator):\n",
    "    try:\n",
    "        return BoundaryAttack(\n",
    "            estimator=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=BOUNDARY_MAX_ITER,\n",
    "            delta=BOUNDARY_DELTA,\n",
    "            epsilon=BOUNDARY_EPS,\n",
    "        )\n",
    "    except TypeError:\n",
    "        return BoundaryAttack(\n",
    "            classifier=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=BOUNDARY_MAX_ITER,\n",
    "            delta=BOUNDARY_DELTA,\n",
    "            epsilon=BOUNDARY_EPS,\n",
    "        )\n",
    "\n",
    "hsj = make_hsj(art_clf)\n",
    "bnd = make_boundary(art_clf)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Adversarial evaluation (subset -> numpy)\n",
    "# -----------------------------\n",
    "def take_numpy_subset(ds, max_samples):\n",
    "    Xs, Ys = [], []\n",
    "    n = 0\n",
    "    for xb, yb in ds:\n",
    "        Xs.append(xb.numpy().astype(np.float32))\n",
    "        Ys.append(yb.numpy().astype(np.int64))\n",
    "        n += len(yb)\n",
    "        if n >= max_samples:\n",
    "            break\n",
    "    X = np.concatenate(Xs, axis=0)[:max_samples]\n",
    "    y = np.concatenate(Ys, axis=0)[:max_samples]\n",
    "    y_oh = tf.keras.utils.to_categorical(y, num_classes=num_classes).astype(np.float32)\n",
    "    return X, y, y_oh\n",
    "\n",
    "def eval_attack_on_subset(attack, ds, max_samples, batch_size):\n",
    "    X, y, y_oh = take_numpy_subset(ds, max_samples=max_samples)\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        X_adv = attack.generate(x=X, y=y_oh, batch_size=batch_size)\n",
    "    except Exception:\n",
    "        X_adv = attack.generate(x=X, y=y, batch_size=batch_size)\n",
    "\n",
    "    logits = rnn(tf.convert_to_tensor(X_adv, dtype=tf.float32), training=False).numpy()\n",
    "    pred = logits.argmax(axis=1)\n",
    "    acc = float(accuracy_score(y, pred))\n",
    "    mf1 = float(f1_score(y, pred, average=\"macro\"))\n",
    "    sec = float(time.time() - t0)\n",
    "    return acc, mf1, sec\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Run evals + save\n",
    "# -----------------------------\n",
    "conditions_noise = [\"Clean\", \"Gaussian\", \"Uniform\", \"Dropout\", \"Salt-Pepper\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nEvaluating RNN (noise / clean) ...\")\n",
    "for cond in conditions_noise:\n",
    "    acc, mf1, sec = eval_streaming_condition(rnn, val_ds, cond)\n",
    "    results.append({\"Model\": \"RNN-Image\", \"Condition\": cond, \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "    print(f\"  {cond:12s}  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "print(f\"\\nEvaluating RNN (adversarial on first {ADV_MAX_SAMPLES} samples) ...\")\n",
    "acc, mf1, sec = eval_attack_on_subset(fgsm, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"RNN-Image\", \"Condition\": \"FGSM\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  FGSM         acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "acc, mf1, sec = eval_attack_on_subset(pgd, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"RNN-Image\", \"Condition\": \"PGD\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  PGD          acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "acc, mf1, sec = eval_attack_on_subset(hsj, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"RNN-Image\", \"Condition\": \"HopSkipJump\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  HopSkipJump  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "acc, mf1, sec = eval_attack_on_subset(bnd, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"RNN-Image\", \"Condition\": \"Boundary\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  Boundary     acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "df = pd.DataFrame(results).sort_values([\"Condition\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "out_csv = \"malevis_image_rnn_noise_adv_results.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f\"\\nSaved: {out_csv}\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09fa885-eef5-4e6d-8f33-0cd873dfd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "MaleVis (IMAGE SPACE) - RNN classifier:\n",
    "- Clean evaluation\n",
    "- Noise perturbations: Gaussian / Uniform / Dropout / Salt-Pepper\n",
    "- Adversarial attacks via ART: FGSM / PGD / HopSkipJump / BoundaryAttack\n",
    "\n",
    "Key features:\n",
    "- Uses local directory with train/ and val/\n",
    "- FIXED class mapping: val uses class_names from train (prevents label mismatch)\n",
    "- Streaming tf.data for training/eval (no ds_to_numpy full)\n",
    "- Adversarial attacks run on a SMALL SUBSET (ADV_MAX_SAMPLES) to avoid RAM blowups\n",
    "- Uses ART TensorFlowV2Classifier (works with TF2 eager)\n",
    "\n",
    "Output:\n",
    "- Prints results table\n",
    "- Saves: malevis_image_rnn_noise_adv_results.csv\n",
    "\"\"\"\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = r\"C:\\Users\\ajayi\\Downloads\\malevis_train_val_300x300\\malevis_train_val_300x300\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n",
    "\n",
    "IMG_H, IMG_W, CH = 300, 300, 3\n",
    "\n",
    "# Training\n",
    "EPOCHS = 15\n",
    "BS     = 16          # RNN is heavier; 16 is safer than 32 for 300x300\n",
    "LR     = 1e-3\n",
    "\n",
    "# Noise perturbations (image-space)\n",
    "NOISE_GAUSS_SIG = 0.10\n",
    "NOISE_UNIF_RNG  = 0.10\n",
    "NOISE_DROPOUT   = 0.20\n",
    "NOISE_SP_PROB   = 0.02\n",
    "\n",
    "# Adversarial subset (IMPORTANT for memory/time)\n",
    "ADV_MAX_SAMPLES = 64\n",
    "ADV_BATCH_SIZE  = 4        # RNN + 300x300 -> keep small\n",
    "\n",
    "# FGSM/PGD (Linf) eps in [0,1] pixel space\n",
    "FGSM_EPS  = 8/255\n",
    "PGD_EPS   = 8/255\n",
    "PGD_ALPHA = 2/255\n",
    "PGD_ITERS = 10\n",
    "\n",
    "# HSJ / Boundary (black-box) — slow, keep small subset\n",
    "HSJ_MAX_ITER    = 20\n",
    "HSJ_MAX_EVAL    = 2000\n",
    "HSJ_INIT_EVAL   = 50\n",
    "HSJ_INIT_SIZE   = 50\n",
    "\n",
    "BOUNDARY_MAX_ITER = 200\n",
    "BOUNDARY_DELTA    = 0.01\n",
    "BOUNDARY_EPS      = 0.01\n",
    "\n",
    "# GPU memory growth (safe)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Dataset loading (FIXED mapping)\n",
    "# -----------------------------\n",
    "def make_ds_fixed(train_dir, val_dir, img_size, batch_size, seed):\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    def norm(x, y):\n",
    "        x = tf.cast(x, tf.float32) / 255.0\n",
    "        return x, y\n",
    "\n",
    "    train_ds = train_ds.map(norm, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = val_ds.map(norm,   num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds, class_names, len(class_names)\n",
    "\n",
    "print(\"Loading datasets with fixed label mapping...\")\n",
    "train_ds, val_ds, class_names, num_classes = make_ds_fixed(\n",
    "    TRAIN_DIR, VAL_DIR, (IMG_H, IMG_W), batch_size=BS, seed=SEED\n",
    ")\n",
    "print(\"Classes:\", num_classes)\n",
    "print(\"First 5 classes:\", class_names[:5])\n",
    "\n",
    "# -----------------------------\n",
    "# 2) RNN model (logits output for stability)\n",
    "# -----------------------------\n",
    "def build_rnn(num_classes):\n",
    "    inp = layers.Input(shape=(IMG_H, IMG_W, CH), name=\"img_in\")\n",
    "\n",
    "    # Treat image as sequence of rows: timesteps=IMG_H, features=IMG_W*CH\n",
    "    x = layers.Reshape((IMG_H, IMG_W * CH), name=\"to_seq\")(inp)\n",
    "\n",
    "    # Normalize sequence features (helps RNN stability)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # RNN backbone\n",
    "    # (GRU is faster; LSTM sometimes slightly better. Pick one.)\n",
    "    x = layers.Bidirectional(layers.GRU(128, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    x = layers.Bidirectional(layers.GRU(128, return_sequences=False))(x)\n",
    "\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.30)(x)\n",
    "\n",
    "    logits = layers.Dense(num_classes, activation=None, name=\"logits\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, logits, name=\"malevis_rnn_logits\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(LR),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "rnn = build_rnn(num_classes)\n",
    "\n",
    "print(\"\\nTraining RNN...\")\n",
    "rnn.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "print(\"\\nSanity check (Keras evaluate on CLEAN val):\", rnn.evaluate(val_ds, verbose=0))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Noise perturbations (streaming, no numpy)\n",
    "# -----------------------------\n",
    "def clip01(x):\n",
    "    return tf.clip_by_value(x, 0.0, 1.0)\n",
    "\n",
    "def add_noise_batch(x, kind):\n",
    "    if kind == \"Gaussian\":\n",
    "        x2 = x + tf.random.normal(tf.shape(x), stddev=NOISE_GAUSS_SIG)\n",
    "    elif kind == \"Uniform\":\n",
    "        x2 = x + tf.random.uniform(tf.shape(x), minval=-NOISE_UNIF_RNG, maxval=NOISE_UNIF_RNG)\n",
    "    elif kind == \"Dropout\":\n",
    "        mask = tf.cast(tf.random.uniform(tf.shape(x)) > NOISE_DROPOUT, tf.float32)\n",
    "        x2 = x * mask\n",
    "    elif kind == \"Salt-Pepper\":\n",
    "        rnd = tf.random.uniform(tf.shape(x))\n",
    "        x2 = tf.where(rnd < (NOISE_SP_PROB/2.0), 0.0, x)\n",
    "        x2 = tf.where(rnd > 1.0 - (NOISE_SP_PROB/2.0), 1.0, x2)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown noise kind\")\n",
    "    return clip01(x2)\n",
    "\n",
    "@tf.function\n",
    "def predict_labels_logits(model, x):\n",
    "    logits = model(x, training=False)\n",
    "    return tf.argmax(logits, axis=1)\n",
    "\n",
    "def eval_streaming_condition(model, ds, condition):\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    t0 = time.time()\n",
    "    for xb, yb in ds:\n",
    "        x_eval = xb if condition == \"Clean\" else add_noise_batch(xb, condition)\n",
    "        pred = predict_labels_logits(model, x_eval)\n",
    "        y_true_all.append(yb.numpy())\n",
    "        y_pred_all.append(pred.numpy())\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    mf1 = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    sec = float(time.time() - t0)\n",
    "    return acc, mf1, sec\n",
    "\n",
    "# -----------------------------\n",
    "# 4) ART setup (TF2 eager-safe) + attacks\n",
    "# -----------------------------\n",
    "def require_art():\n",
    "    try:\n",
    "        import art  # noqa: F401\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"ART is required for FGSM/PGD/HSJ/Boundary.\\n\"\n",
    "            \"Install it:\\n\"\n",
    "            \"  pip install adversarial-robustness-toolbox\\n\"\n",
    "            \"Then RESTART kernel / Python.\\n\"\n",
    "            f\"Original import error: {e}\"\n",
    "        )\n",
    "\n",
    "require_art()\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, HopSkipJump, BoundaryAttack\n",
    "\n",
    "loss_obj = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "art_clf = TensorFlowV2Classifier(\n",
    "    model=rnn,\n",
    "    nb_classes=num_classes,\n",
    "    input_shape=(IMG_H, IMG_W, CH),\n",
    "    loss_object=loss_obj,\n",
    "    optimizer=optimizer,\n",
    "    clip_values=(0.0, 1.0),\n",
    ")\n",
    "\n",
    "fgsm = FastGradientMethod(estimator=art_clf, eps=FGSM_EPS)\n",
    "pgd  = ProjectedGradientDescent(\n",
    "    estimator=art_clf,\n",
    "    eps=PGD_EPS,\n",
    "    eps_step=PGD_ALPHA,\n",
    "    max_iter=PGD_ITERS,\n",
    "    targeted=False\n",
    ")\n",
    "\n",
    "def make_hsj(estimator):\n",
    "    try:\n",
    "        return HopSkipJump(\n",
    "            estimator=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=HSJ_MAX_ITER,\n",
    "            max_eval=HSJ_MAX_EVAL,\n",
    "            init_eval=HSJ_INIT_EVAL,\n",
    "            init_size=HSJ_INIT_SIZE,\n",
    "        )\n",
    "    except TypeError:\n",
    "        return HopSkipJump(\n",
    "            classifier=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=HSJ_MAX_ITER,\n",
    "            max_eval=HSJ_MAX_EVAL,\n",
    "            init_eval=HSJ_INIT_EVAL,\n",
    "            init_size=HSJ_INIT_SIZE,\n",
    "        )\n",
    "\n",
    "def make_boundary(estimator):\n",
    "    try:\n",
    "        return BoundaryAttack(\n",
    "            estimator=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=BOUNDARY_MAX_ITER,\n",
    "            delta=BOUNDARY_DELTA,\n",
    "            epsilon=BOUNDARY_EPS,\n",
    "        )\n",
    "    except TypeError:\n",
    "        return BoundaryAttack(\n",
    "            classifier=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=BOUNDARY_MAX_ITER,\n",
    "            delta=BOUNDARY_DELTA,\n",
    "            epsilon=BOUNDARY_EPS,\n",
    "        )\n",
    "\n",
    "hsj = make_hsj(art_clf)\n",
    "bnd = make_boundary(art_clf)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Adversarial evaluation (subset -> numpy)\n",
    "# -----------------------------\n",
    "def take_numpy_subset(ds, max_samples):\n",
    "    Xs, Ys = [], []\n",
    "    n = 0\n",
    "    for xb, yb in ds:\n",
    "        Xs.append(xb.numpy().astype(np.float32))\n",
    "        Ys.append(yb.numpy().astype(np.int64))\n",
    "        n += len(yb)\n",
    "        if n >= max_samples:\n",
    "            break\n",
    "    X = np.concatenate(Xs, axis=0)[:max_samples]\n",
    "    y = np.concatenate(Ys, axis=0)[:max_samples]\n",
    "    y_oh = tf.keras.utils.to_categorical(y, num_classes=num_classes).astype(np.float32)\n",
    "    return X, y, y_oh\n",
    "\n",
    "def eval_attack_on_subset(attack, ds, max_samples, batch_size):\n",
    "    X, y, y_oh = take_numpy_subset(ds, max_samples=max_samples)\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        X_adv = attack.generate(x=X, y=y_oh, batch_size=batch_size)\n",
    "    except Exception:\n",
    "        X_adv = attack.generate(x=X, y=y, batch_size=batch_size)\n",
    "\n",
    "    logits = rnn(tf.convert_to_tensor(X_adv, dtype=tf.float32), training=False).numpy()\n",
    "    pred = logits.argmax(axis=1)\n",
    "    acc = float(accuracy_score(y, pred))\n",
    "    mf1 = float(f1_score(y, pred, average=\"macro\"))\n",
    "    sec = float(time.time() - t0)\n",
    "    return acc, mf1, sec\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Run evals + save\n",
    "# -----------------------------\n",
    "conditions_noise = [\"Clean\", \"Gaussian\", \"Uniform\", \"Dropout\", \"Salt-Pepper\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nEvaluating RNN (noise / clean) ...\")\n",
    "for cond in conditions_noise:\n",
    "    acc, mf1, sec = eval_streaming_condition(rnn, val_ds, cond)\n",
    "    results.append({\"Model\": \"RNN-Image\", \"Condition\": cond, \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "    print(f\"  {cond:12s}  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "print(f\"\\nEvaluating RNN (adversarial on first {ADV_MAX_SAMPLES} samples) ...\")\n",
    "acc, mf1, sec = eval_attack_on_subset(fgsm, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"RNN-Image\", \"Condition\": \"FGSM\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  FGSM         acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "acc, mf1, sec = eval_attack_on_subset(pgd, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"RNN-Image\", \"Condition\": \"PGD\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  PGD          acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "acc, mf1, sec = eval_attack_on_subset(hsj, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"RNN-Image\", \"Condition\": \"HopSkipJump\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  HopSkipJump  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "acc, mf1, sec = eval_attack_on_subset(bnd, val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "results.append({\"Model\": \"RNN-Image\", \"Condition\": \"Boundary\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "print(f\"  Boundary     acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "df = pd.DataFrame(results).sort_values([\"Condition\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "out_csv = \"malevis_image_rnn_noise_adv_results.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f\"\\nSaved: {out_csv}\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf88895-95dc-4591-bdc2-a3282e2cbebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with fixed label mapping...\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n",
      "Classes: 26\n",
      "First 5 classes: ['Adposhel', 'Agent', 'Allaple', 'Amonetize', 'Androm']\n",
      "\n",
      "Training VAE...\n",
      "Epoch 1/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2723s\u001b[0m 2s/step - kl: 4001.5256 - loss: 18044.8242 - recon_sse: 18044.8242 - val_kl: 1092.2385 - val_loss: 12539.2002 - val_recon_sse: 12539.2002\n",
      "Epoch 2/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2669s\u001b[0m 2s/step - kl: 1550.4702 - loss: 12558.0996 - recon_sse: 12542.5947 - val_kl: 2041.9601 - val_loss: 11394.0459 - val_recon_sse: 11373.6270\n",
      "Epoch 3/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2701s\u001b[0m 2s/step - kl: 1618.0955 - loss: 11021.3242 - recon_sse: 10988.9570 - val_kl: 814.4783 - val_loss: 10902.7148 - val_recon_sse: 10886.4268\n",
      "Epoch 4/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2681s\u001b[0m 2s/step - kl: 757.3937 - loss: 10515.5283 - recon_sse: 10492.8027 - val_kl: 584.9694 - val_loss: 10580.4258 - val_recon_sse: 10562.8887\n",
      "Epoch 5/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2703s\u001b[0m 2s/step - kl: 549.2943 - loss: 10079.7334 - recon_sse: 10057.7617 - val_kl: 411.5385 - val_loss: 10328.7148 - val_recon_sse: 10312.2578\n",
      "Epoch 6/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2683s\u001b[0m 2s/step - kl: 431.2830 - loss: 9906.3779 - recon_sse: 9884.8125 - val_kl: 346.6379 - val_loss: 10219.8545 - val_recon_sse: 10202.5215\n",
      "Epoch 7/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2690s\u001b[0m 2s/step - kl: 372.3071 - loss: 9610.0859 - recon_sse: 9587.7500 - val_kl: 326.1579 - val_loss: 10104.9092 - val_recon_sse: 10085.3467\n",
      "Epoch 8/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2715s\u001b[0m 2s/step - kl: 332.5174 - loss: 9516.8369 - recon_sse: 9493.5625 - val_kl: 297.2525 - val_loss: 10002.9287 - val_recon_sse: 9982.1211\n",
      "Epoch 9/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2744s\u001b[0m 2s/step - kl: 294.6786 - loss: 9407.6475 - recon_sse: 9384.0752 - val_kl: 261.2662 - val_loss: 9878.7188 - val_recon_sse: 9857.8086\n",
      "Epoch 10/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2704s\u001b[0m 2s/step - kl: 268.6117 - loss: 9248.1143 - recon_sse: 9223.9463 - val_kl: 245.7726 - val_loss: 9880.4014 - val_recon_sse: 9858.2812\n",
      "Epoch 11/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2699s\u001b[0m 2s/step - kl: 252.0473 - loss: 9180.8486 - recon_sse: 9155.6445 - val_kl: 234.0091 - val_loss: 9831.8125 - val_recon_sse: 9808.4141\n",
      "Epoch 12/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2699s\u001b[0m 2s/step - kl: 248.1944 - loss: 9106.8867 - recon_sse: 9082.0684 - val_kl: 235.3804 - val_loss: 9825.7930 - val_recon_sse: 9802.2500\n",
      "Epoch 13/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2754s\u001b[0m 2s/step - kl: 240.0953 - loss: 9055.6338 - recon_sse: 9031.6270 - val_kl: 229.0643 - val_loss: 9770.0654 - val_recon_sse: 9747.1582\n",
      "Epoch 14/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2612s\u001b[0m 2s/step - kl: 232.9409 - loss: 9015.2471 - recon_sse: 8991.9521 - val_kl: 226.1027 - val_loss: 9745.2363 - val_recon_sse: 9722.6279\n",
      "Epoch 15/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2533s\u001b[0m 2s/step - kl: 230.1502 - loss: 8980.0771 - recon_sse: 8957.0596 - val_kl: 236.1014 - val_loss: 9673.2500 - val_recon_sse: 9649.6318\n",
      "Epoch 16/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2563s\u001b[0m 2s/step - kl: 229.4714 - loss: 8913.7910 - recon_sse: 8890.8477 - val_kl: 225.5129 - val_loss: 9676.6455 - val_recon_sse: 9654.0947\n",
      "Epoch 17/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2705s\u001b[0m 2s/step - kl: 230.1392 - loss: 8856.6621 - recon_sse: 8833.6494 - val_kl: 226.7668 - val_loss: 9646.2090 - val_recon_sse: 9623.5303\n",
      "Epoch 18/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2732s\u001b[0m 2s/step - kl: 231.8332 - loss: 8869.6104 - recon_sse: 8846.4209 - val_kl: 237.8408 - val_loss: 9804.1162 - val_recon_sse: 9780.3271\n",
      "Epoch 19/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2729s\u001b[0m 2s/step - kl: 230.0333 - loss: 8822.0166 - recon_sse: 8799.0107 - val_kl: 234.4663 - val_loss: 9591.9248 - val_recon_sse: 9568.4746\n",
      "Epoch 20/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2730s\u001b[0m 2s/step - kl: 230.9256 - loss: 8794.1562 - recon_sse: 8771.0654 - val_kl: 219.8742 - val_loss: 9587.2080 - val_recon_sse: 9565.2188\n",
      "Epoch 21/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2726s\u001b[0m 2s/step - kl: 232.9637 - loss: 8813.6621 - recon_sse: 8790.3672 - val_kl: 234.0811 - val_loss: 9593.5498 - val_recon_sse: 9570.1562\n",
      "Epoch 22/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2728s\u001b[0m 2s/step - kl: 232.7845 - loss: 8769.5244 - recon_sse: 8746.2471 - val_kl: 240.4915 - val_loss: 9601.5508 - val_recon_sse: 9577.4941\n",
      "Epoch 23/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2732s\u001b[0m 2s/step - kl: 239.9313 - loss: 8736.3623 - recon_sse: 8712.3730 - val_kl: 233.0747 - val_loss: 9607.3213 - val_recon_sse: 9584.0137\n",
      "Epoch 24/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2739s\u001b[0m 2s/step - kl: 235.6867 - loss: 8692.7637 - recon_sse: 8669.1943 - val_kl: 226.1659 - val_loss: 9564.8203 - val_recon_sse: 9542.2041\n",
      "Epoch 25/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2734s\u001b[0m 2s/step - kl: 235.3781 - loss: 8678.2012 - recon_sse: 8654.6631 - val_kl: 227.2568 - val_loss: 9560.9697 - val_recon_sse: 9538.2412\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "MaleVis: IMAGE -> ConvVAE -> LATENT -> Transformer classifier\n",
    "Evaluates on:\n",
    "- Clean latent\n",
    "- Latent noise perturbations: Gaussian / Uniform / Dropout / Salt-Pepper\n",
    "- Latent adversarial attacks via ART (TF2 eager-safe): FGSM / PGD / HopSkipJump / BoundaryAttack\n",
    "\n",
    "Fixes included:\n",
    "- FIXED class mapping (val uses class_names from train)\n",
    "- Streaming tf.data (no ds_to_numpy on full sets)\n",
    "- VAE KL warmup via model.beta (no missing 'kl' layer)\n",
    "- Latent augmentation mixing uses tf.searchsorted + switch_case (NO exclusive=True overlap bug)\n",
    "- ART uses TensorFlowV2Classifier (works with TF2 eager; no \"disable eager\" error)\n",
    "\n",
    "NOTES:\n",
    "1) Install ART (same env) then restart kernel:\n",
    "   pip install adversarial-robustness-toolbox\n",
    "\n",
    "2) HSJ/Boundary are slow; we run them on a small subset (ADV_MAX_SAMPLES).\n",
    "\n",
    "Outputs:\n",
    "- Prints results table\n",
    "- Saves: malevis_latent_transformer_noise_adv_results.csv\n",
    "\"\"\"\n",
    "\n",
    "import os, time, math, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = r\"C:\\Users\\ajayi\\Downloads\\malevis_train_val_300x300\\malevis_train_val_300x300\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n",
    "\n",
    "# 300x300 is heavy; if you hit OOM/slow, try 224 or 128\n",
    "IMG_H, IMG_W, CH = 300, 300, 3\n",
    "\n",
    "LATENT_DIM = 64\n",
    "\n",
    "# VAE (unsupervised)\n",
    "VAE_EPOCHS = 25\n",
    "VAE_BS     = 8\n",
    "VAE_LR     = 1e-3\n",
    "\n",
    "# KL warmup (keep small for better, less-collapsed latents)\n",
    "BETA_MAX = 0.10\n",
    "BETA_WARMUP_EPOCHS = 10\n",
    "\n",
    "# Transformer classifier (latent)\n",
    "CLS_EPOCHS = 20\n",
    "CLS_BS     = 256\n",
    "CLS_LR     = 1e-3\n",
    "\n",
    "# Transformer hyperparams\n",
    "D_MODEL   = 128\n",
    "NUM_HEADS = 4\n",
    "FF_DIM    = 256\n",
    "N_LAYERS  = 2\n",
    "DROPOUT   = 0.20\n",
    "\n",
    "# Latent perturbations (training augmentation + eval)\n",
    "NOISE_GAUSS_SIG = 0.15\n",
    "NOISE_UNIF_RNG  = 0.15\n",
    "NOISE_DROPOUT   = 0.25\n",
    "NOISE_SP_PROB   = 0.02\n",
    "\n",
    "# Mixture probabilities for latent augmentation during training\n",
    "# (must be non-negative; will be normalized automatically)\n",
    "P_CLEAN = 0.40\n",
    "P_GAUSS = 0.20\n",
    "P_UNIF  = 0.20\n",
    "P_DROP  = 0.10\n",
    "P_SP    = 0.10\n",
    "\n",
    "# Adversarial subset (IMPORTANT for memory/time)\n",
    "ADV_MAX_SAMPLES = 64     # raise slowly (128/256) if your machine can handle it\n",
    "ADV_BATCH_SIZE  = 8      # keep small\n",
    "\n",
    "# FGSM/PGD in latent space (values depend on latent scale; tune if needed)\n",
    "FGSM_EPS_LAT = 0.25\n",
    "PGD_EPS_LAT  = 0.35\n",
    "PGD_ALPHA    = 0.06\n",
    "PGD_ITERS    = 10\n",
    "\n",
    "# HSJ / Boundary (black-box) — very slow\n",
    "HSJ_MAX_ITER    = 20\n",
    "HSJ_MAX_EVAL    = 2000\n",
    "HSJ_INIT_EVAL   = 50\n",
    "HSJ_INIT_SIZE   = 50\n",
    "\n",
    "BOUNDARY_MAX_ITER = 200\n",
    "BOUNDARY_DELTA    = 0.01\n",
    "BOUNDARY_EPS      = 0.01\n",
    "\n",
    "CACHE_LATENTS = True  # latents are small; safe\n",
    "\n",
    "# GPU memory growth (safe)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Dataset loading (FIXED mapping)\n",
    "# -----------------------------\n",
    "def make_ds_fixed(train_dir, val_dir, img_size, batch_size, seed):\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names = train_ds.class_names  # canonical mapping from TRAIN\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    def norm(x, y):\n",
    "        x = tf.cast(x, tf.float32) / 255.0\n",
    "        return x, y\n",
    "\n",
    "    # Don't cache 300x300 images in RAM; prefetch only\n",
    "    train_ds = train_ds.map(norm, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = val_ds.map(norm,   num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds, class_names, len(class_names)\n",
    "\n",
    "print(\"Loading datasets with fixed label mapping...\")\n",
    "train_ds_vae, val_ds_vae, class_names, num_classes = make_ds_fixed(\n",
    "    TRAIN_DIR, VAL_DIR, (IMG_H, IMG_W), batch_size=VAE_BS, seed=SEED\n",
    ")\n",
    "print(\"Classes:\", num_classes)\n",
    "print(\"First 5 classes:\", class_names[:5])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Conv VAE (subclassed, beta warmup)\n",
    "# -----------------------------\n",
    "def _compute_decoder_grid(img_h, img_w, downsamples=4):\n",
    "    div = 2 ** downsamples\n",
    "    gh = int(math.ceil(img_h / div))\n",
    "    gw = int(math.ceil(img_w / div))\n",
    "    out_h = gh * div\n",
    "    out_w = gw * div\n",
    "    return gh, gw, out_h, out_w\n",
    "\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    inp = layers.Input(shape=input_shape, name=f\"enc_in_k{latent_dim}\")\n",
    "    x = layers.Conv2D(32, 4, strides=2, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.Conv2D(64, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(128, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(256, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=f\"z_mean_k{latent_dim}\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=f\"z_log_var_k{latent_dim}\")(x)\n",
    "    return tf.keras.Model(inp, [z_mean, z_log_var], name=f\"encoder_k{latent_dim}\")\n",
    "\n",
    "def build_decoder(output_shape, latent_dim):\n",
    "    img_h, img_w, ch = output_shape\n",
    "    gh, gw, out_h, out_w = _compute_decoder_grid(img_h, img_w, downsamples=4)\n",
    "\n",
    "    inp = layers.Input(shape=(latent_dim,), name=f\"dec_in_k{latent_dim}\")\n",
    "    x = layers.Dense(gh * gw * 256, activation=\"relu\")(inp)\n",
    "    x = layers.Reshape((gh, gw, 256))(x)\n",
    "    x = layers.Conv2DTranspose(256, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(128, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(64, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(32, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(ch, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "\n",
    "    crop_h = max(0, out_h - img_h)\n",
    "    crop_w = max(0, out_w - img_w)\n",
    "    if crop_h > 0 or crop_w > 0:\n",
    "        top = crop_h // 2\n",
    "        bot = crop_h - top\n",
    "        left = crop_w // 2\n",
    "        right = crop_w - left\n",
    "        x = layers.Cropping2D(cropping=((top, bot), (left, right)))(x)\n",
    "\n",
    "    return tf.keras.Model(inp, x, name=f\"decoder_k{latent_dim}\")\n",
    "\n",
    "class ConvVAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, logvar_clip=10.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = tf.Variable(0.0, trainable=False, dtype=tf.float32, name=\"beta\")\n",
    "        self.logvar_clip = float(logvar_clip)\n",
    "\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.recon_tracker = tf.keras.metrics.Mean(name=\"recon_sse\")\n",
    "        self.kl_tracker = tf.keras.metrics.Mean(name=\"kl\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.recon_tracker, self.kl_tracker]\n",
    "\n",
    "    def _sample(self, z_mean, z_log_var):\n",
    "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(x, training=True)\n",
    "            z_log_var = tf.clip_by_value(z_log_var, -self.logvar_clip, self.logvar_clip)\n",
    "            z = self._sample(z_mean, z_log_var)\n",
    "            x_hat = self.decoder(z, training=True)\n",
    "\n",
    "            recon_sse = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=[1,2,3]))\n",
    "            kl = tf.reduce_mean(\n",
    "                -0.5 * tf.reduce_sum(1.0 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "            )\n",
    "            loss = recon_sse + self.beta * kl\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.recon_tracker.update_state(recon_sse)\n",
    "        self.kl_tracker.update_state(kl)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x = data\n",
    "        z_mean, z_log_var = self.encoder(x, training=False)\n",
    "        z_log_var = tf.clip_by_value(z_log_var, -self.logvar_clip, self.logvar_clip)\n",
    "        z = self._sample(z_mean, z_log_var)\n",
    "        x_hat = self.decoder(z, training=False)\n",
    "\n",
    "        recon_sse = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=[1,2,3]))\n",
    "        kl = tf.reduce_mean(\n",
    "            -0.5 * tf.reduce_sum(1.0 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        loss = recon_sse + self.beta * kl\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.recon_tracker.update_state(recon_sse)\n",
    "        self.kl_tracker.update_state(kl)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def encode_mean(self, x):\n",
    "        z_mean, _ = self.encoder(x, training=False)\n",
    "        return z_mean\n",
    "\n",
    "class BetaWarmup(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, beta_max=1.0, warmup_epochs=10):\n",
    "        super().__init__()\n",
    "        self.beta_max = float(beta_max)\n",
    "        self.warmup_epochs = int(max(1, warmup_epochs))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        beta = self.beta_max * min(1.0, epoch / float(self.warmup_epochs))\n",
    "        self.model.beta.assign(beta)\n",
    "\n",
    "print(\"\\nTraining VAE...\")\n",
    "tf.keras.backend.clear_session()\n",
    "encoder = build_encoder((IMG_H, IMG_W, CH), LATENT_DIM)\n",
    "decoder = build_decoder((IMG_H, IMG_W, CH), LATENT_DIM)\n",
    "vae = ConvVAE(encoder, decoder, name=f\"vae_k{LATENT_DIM}\")\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(VAE_LR))\n",
    "\n",
    "vae_train_img = train_ds_vae.map(lambda x, y: x)\n",
    "vae_val_img   = val_ds_vae.map(lambda x, y: x)\n",
    "\n",
    "vae.fit(\n",
    "    vae_train_img,\n",
    "    validation_data=vae_val_img,\n",
    "    epochs=VAE_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        BetaWarmup(beta_max=BETA_MAX, warmup_epochs=BETA_WARMUP_EPOCHS),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build latent datasets (safe to cache latents)\n",
    "# -----------------------------\n",
    "def make_image_ds(dir_path, class_names, img_size, batch_size, shuffle, seed):\n",
    "    ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dir_path,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=seed if shuffle else None,\n",
    "        class_names=class_names\n",
    "    )\n",
    "    ds = ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds_cls = make_image_ds(TRAIN_DIR, class_names, (IMG_H, IMG_W), CLS_BS, shuffle=True,  seed=SEED)\n",
    "val_ds_cls   = make_image_ds(VAL_DIR,   class_names, (IMG_H, IMG_W), CLS_BS, shuffle=False, seed=SEED)\n",
    "\n",
    "def make_latent_ds(image_ds, vae, cache_latents=True, shuffle=False):\n",
    "    def to_latent(x, y):\n",
    "        z = vae.encode_mean(x)           # (B,k)\n",
    "        z = tf.expand_dims(z, axis=-1)   # (B,k,1)\n",
    "        return z, y\n",
    "    out = image_ds.map(to_latent, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if cache_latents:\n",
    "        out = out.cache()\n",
    "    if shuffle:\n",
    "        out = out.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
    "    return out.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "lat_train_ds = make_latent_ds(train_ds_cls, vae, cache_latents=CACHE_LATENTS, shuffle=True)\n",
    "lat_val_ds   = make_latent_ds(val_ds_cls,   vae, cache_latents=CACHE_LATENTS, shuffle=False)\n",
    "\n",
    "def estimate_latent_bounds(latent_ds, q_lo=0.5, q_hi=99.5, take_batches=50):\n",
    "    zs = []\n",
    "    for zb, _ in latent_ds.take(take_batches):\n",
    "        zs.append(tf.reshape(zb, [-1]).numpy())\n",
    "    zs = np.concatenate(zs, axis=0)\n",
    "    return float(np.percentile(zs, q_lo)), float(np.percentile(zs, q_hi))\n",
    "\n",
    "z_min, z_max = estimate_latent_bounds(lat_train_ds)\n",
    "print(\"\\nLatent clip bounds:\", z_min, z_max)\n",
    "\n",
    "def latent_std_check(latent_ds, take_batches=10):\n",
    "    zs = []\n",
    "    for zb, _ in latent_ds.take(take_batches):\n",
    "        z = tf.squeeze(zb, axis=-1).numpy()  # (B,k)\n",
    "        zs.append(z)\n",
    "    Z = np.concatenate(zs, axis=0)\n",
    "    per_dim_std = Z.std(axis=0)\n",
    "    print(\"Latent std: mean=\", float(per_dim_std.mean()),\n",
    "          \"min=\", float(per_dim_std.min()),\n",
    "          \"max=\", float(per_dim_std.max()))\n",
    "latent_std_check(lat_train_ds)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Latent perturbations + mixed augmentation (FIXED)\n",
    "# -----------------------------\n",
    "def clip_latent(z):\n",
    "    return tf.clip_by_value(z, z_min, z_max)\n",
    "\n",
    "def latent_noise(z, kind):\n",
    "    if kind == \"Gaussian\":\n",
    "        z2 = z + tf.random.normal(tf.shape(z), stddev=NOISE_GAUSS_SIG)\n",
    "    elif kind == \"Uniform\":\n",
    "        z2 = z + tf.random.uniform(tf.shape(z), minval=-NOISE_UNIF_RNG, maxval=NOISE_UNIF_RNG)\n",
    "    elif kind == \"Dropout\":\n",
    "        mask = tf.cast(tf.random.uniform(tf.shape(z)) > NOISE_DROPOUT, tf.float32)\n",
    "        z2 = z * mask\n",
    "    elif kind == \"Salt-Pepper\":\n",
    "        rnd = tf.random.uniform(tf.shape(z))\n",
    "        z2 = tf.where(rnd < (NOISE_SP_PROB/2.0), tf.cast(z_min, tf.float32), z)\n",
    "        z2 = tf.where(rnd > 1.0 - (NOISE_SP_PROB/2.0), tf.cast(z_max, tf.float32), z2)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown noise kind\")\n",
    "    return clip_latent(z2)\n",
    "\n",
    "# ---- fixed mixed perturb sampler (no overlapping conditions) ----\n",
    "_probs = np.array([P_CLEAN, P_GAUSS, P_UNIF, P_DROP, P_SP], dtype=np.float32)\n",
    "_probs = _probs / max(_probs.sum(), 1e-8)\n",
    "P_CLEAN, P_GAUSS, P_UNIF, P_DROP, P_SP = _probs.tolist()\n",
    "\n",
    "PROBS_T   = tf.constant([P_CLEAN, P_GAUSS, P_UNIF, P_DROP, P_SP], dtype=tf.float32)\n",
    "CUMPROB_T = tf.cumsum(PROBS_T)\n",
    "\n",
    "def apply_mixed_perturb(z):\n",
    "    r = tf.random.uniform([], 0.0, 1.0, dtype=tf.float32)\n",
    "    idx = tf.searchsorted(CUMPROB_T, r, side=\"right\")  # 0..5 possible\n",
    "    idx = tf.clip_by_value(idx, 0, 4)\n",
    "\n",
    "    def f0(): return z\n",
    "    def f1(): return latent_noise(z, \"Gaussian\")\n",
    "    def f2(): return latent_noise(z, \"Uniform\")\n",
    "    def f3(): return latent_noise(z, \"Dropout\")\n",
    "    def f4(): return latent_noise(z, \"Salt-Pepper\")\n",
    "\n",
    "    return tf.switch_case(idx, branch_fns={0: f0, 1: f1, 2: f2, 3: f3, 4: f4})\n",
    "\n",
    "lat_train_aug = lat_train_ds.map(\n",
    "    lambda z, y: (apply_mixed_perturb(z), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Latent Transformer classifier (LOGITS)\n",
    "# -----------------------------\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(d_model),\n",
    "        ])\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.drop1 = layers.Dropout(dropout)\n",
    "        self.drop2 = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        attn = self.att(x, x, training=training)\n",
    "        attn = self.drop1(attn, training=training)\n",
    "        x = self.ln1(x + attn)\n",
    "\n",
    "        f = self.ffn(x, training=training)\n",
    "        f = self.drop2(f, training=training)\n",
    "        return self.ln2(x + f)\n",
    "\n",
    "def build_latent_transformer(latent_dim, num_classes,\n",
    "                            d_model=128, num_heads=4, ff_dim=256, n_layers=2, dropout=0.2):\n",
    "    inp = layers.Input(shape=(latent_dim, 1), name=\"z_seq\")\n",
    "\n",
    "    # project 1 -> d_model\n",
    "    x = layers.Dense(d_model)(inp)\n",
    "\n",
    "    # positional embedding (length = latent_dim)\n",
    "    positions = tf.range(start=0, limit=latent_dim, delta=1)\n",
    "    pos_emb = layers.Embedding(input_dim=latent_dim, output_dim=d_model)(positions)\n",
    "    pos_emb = tf.expand_dims(pos_emb, axis=0)  # (1, L, d_model)\n",
    "    x = x + pos_emb\n",
    "\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    for _ in range(n_layers):\n",
    "        x = TransformerBlock(d_model, num_heads, ff_dim, dropout)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    logits = layers.Dense(num_classes, activation=None, name=\"logits\")(x)  # logits\n",
    "    model = tf.keras.Model(inp, logits, name=\"latent_transformer_logits\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(CLS_LR),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "clf = build_latent_transformer(\n",
    "    LATENT_DIM, num_classes,\n",
    "    d_model=D_MODEL, num_heads=NUM_HEADS, ff_dim=FF_DIM, n_layers=N_LAYERS, dropout=DROPOUT\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Latent-Transformer...\")\n",
    "clf.fit(\n",
    "    lat_train_aug,\n",
    "    validation_data=lat_val_ds,\n",
    "    epochs=CLS_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "print(\"\\nSanity check (Keras evaluate on CLEAN latent val):\", clf.evaluate(lat_val_ds, verbose=0))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Eval: Clean + latent perturbations (streaming)\n",
    "# -----------------------------\n",
    "@tf.function\n",
    "def predict_labels_logits(model, z):\n",
    "    logits = model(z, training=False)\n",
    "    return tf.argmax(logits, axis=1)\n",
    "\n",
    "def eval_streaming_latent(model, ds, condition):\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    t0 = time.time()\n",
    "    for zb, yb in ds:\n",
    "        if condition == \"Clean\":\n",
    "            z_eval = zb\n",
    "        else:\n",
    "            z_eval = latent_noise(zb, condition)\n",
    "        pred = predict_labels_logits(model, z_eval)\n",
    "        y_true_all.append(yb.numpy())\n",
    "        y_pred_all.append(pred.numpy())\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    return (\n",
    "        float(accuracy_score(y_true, y_pred)),\n",
    "        float(f1_score(y_true, y_pred, average=\"macro\")),\n",
    "        float(time.time() - t0),\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) ART (TF2 eager-safe) + latent adversarial attacks\n",
    "# -----------------------------\n",
    "def require_art():\n",
    "    try:\n",
    "        import art  # noqa: F401\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"ART is required for FGSM/PGD/HSJ/Boundary.\\n\"\n",
    "            \"Install it:\\n\"\n",
    "            \"  pip install adversarial-robustness-toolbox\\n\"\n",
    "            \"Then RESTART kernel / Python.\\n\"\n",
    "            f\"Original import error: {e}\"\n",
    "        )\n",
    "\n",
    "require_art()\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, HopSkipJump, BoundaryAttack\n",
    "\n",
    "# ART wants one-hot loss most reliably\n",
    "loss_obj = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "opt_obj  = tf.keras.optimizers.Adam(learning_rate=CLS_LR)\n",
    "\n",
    "art_clf = TensorFlowV2Classifier(\n",
    "    model=clf,\n",
    "    nb_classes=num_classes,\n",
    "    input_shape=(LATENT_DIM, 1),\n",
    "    loss_object=loss_obj,\n",
    "    optimizer=opt_obj,\n",
    "    clip_values=(z_min, z_max),\n",
    ")\n",
    "\n",
    "fgsm = FastGradientMethod(estimator=art_clf, eps=FGSM_EPS_LAT)\n",
    "pgd  = ProjectedGradientDescent(\n",
    "    estimator=art_clf,\n",
    "    eps=PGD_EPS_LAT,\n",
    "    eps_step=PGD_ALPHA,\n",
    "    max_iter=PGD_ITERS,\n",
    "    targeted=False\n",
    ")\n",
    "\n",
    "def make_hsj(estimator):\n",
    "    try:\n",
    "        return HopSkipJump(\n",
    "            estimator=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=HSJ_MAX_ITER,\n",
    "            max_eval=HSJ_MAX_EVAL,\n",
    "            init_eval=HSJ_INIT_EVAL,\n",
    "            init_size=HSJ_INIT_SIZE,\n",
    "        )\n",
    "    except TypeError:\n",
    "        return HopSkipJump(\n",
    "            classifier=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=HSJ_MAX_ITER,\n",
    "            max_eval=HSJ_MAX_EVAL,\n",
    "            init_eval=HSJ_INIT_EVAL,\n",
    "            init_size=HSJ_INIT_SIZE,\n",
    "        )\n",
    "\n",
    "def make_boundary(estimator):\n",
    "    try:\n",
    "        return BoundaryAttack(\n",
    "            estimator=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=BOUNDARY_MAX_ITER,\n",
    "            delta=BOUNDARY_DELTA,\n",
    "            epsilon=BOUNDARY_EPS,\n",
    "        )\n",
    "    except TypeError:\n",
    "        return BoundaryAttack(\n",
    "            classifier=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=BOUNDARY_MAX_ITER,\n",
    "            delta=BOUNDARY_DELTA,\n",
    "            epsilon=BOUNDARY_EPS,\n",
    "        )\n",
    "\n",
    "hsj = make_hsj(art_clf)\n",
    "bnd = make_boundary(art_clf)\n",
    "\n",
    "def take_latent_numpy_subset(latent_ds, max_samples):\n",
    "    Zs, Ys = [], []\n",
    "    n = 0\n",
    "    for zb, yb in latent_ds:\n",
    "        Zs.append(zb.numpy().astype(np.float32))\n",
    "        Ys.append(yb.numpy().astype(np.int64))\n",
    "        n += len(yb)\n",
    "        if n >= max_samples:\n",
    "            break\n",
    "    Z = np.concatenate(Zs, axis=0)[:max_samples]  # (N,k,1)\n",
    "    y = np.concatenate(Ys, axis=0)[:max_samples]  # (N,)\n",
    "    y_oh = tf.keras.utils.to_categorical(y, num_classes=num_classes).astype(np.float32)\n",
    "    # clip to ART bounds\n",
    "    Z = np.clip(Z, z_min, z_max).astype(np.float32)\n",
    "    return Z, y, y_oh\n",
    "\n",
    "def eval_attack_latent_on_subset(attack, latent_ds, max_samples, batch_size, name=\"Attack\"):\n",
    "    Z, y, y_oh = take_latent_numpy_subset(latent_ds, max_samples=max_samples)\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        try:\n",
    "            Z_adv = attack.generate(x=Z, y=y_oh, batch_size=batch_size)\n",
    "        except Exception:\n",
    "            Z_adv = attack.generate(x=Z, y=y, batch_size=batch_size)\n",
    "        Z_adv = np.clip(Z_adv, z_min, z_max).astype(np.float32)\n",
    "\n",
    "        logits = clf(tf.convert_to_tensor(Z_adv, dtype=tf.float32), training=False).numpy()\n",
    "        pred = logits.argmax(axis=1)\n",
    "        acc = float(accuracy_score(y, pred))\n",
    "        mf1 = float(f1_score(y, pred, average=\"macro\"))\n",
    "        sec = float(time.time() - t0)\n",
    "        return acc, mf1, sec, None\n",
    "    except Exception as e:\n",
    "        sec = float(time.time() - t0)\n",
    "        return float(\"nan\"), float(\"nan\"), sec, str(e)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Run evals + save\n",
    "# -----------------------------\n",
    "conditions_noise = [\"Clean\", \"Gaussian\", \"Uniform\", \"Dropout\", \"Salt-Pepper\"]\n",
    "results = []\n",
    "\n",
    "print(\"\\nEvaluating Latent-Transformer (Clean + noise perturbations) ...\")\n",
    "for cond in conditions_noise:\n",
    "    acc, mf1, sec = eval_streaming_latent(clf, lat_val_ds, cond)\n",
    "    results.append({\"Model\": \"Latent-Transformer\", \"Condition\": cond, \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec, \"Error\": \"\"})\n",
    "    print(f\"  {cond:12s}  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "print(f\"\\nEvaluating Latent-Transformer (adversarial on first {ADV_MAX_SAMPLES} samples) ...\")\n",
    "\n",
    "acc, mf1, sec, err = eval_attack_latent_on_subset(fgsm, lat_val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE, \"FGSM\")\n",
    "results.append({\"Model\": \"Latent-Transformer\", \"Condition\": \"FGSM\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec, \"Error\": err or \"\"})\n",
    "print(f\"  FGSM         acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\" + (f\"  [ERR: {err}]\" if err else \"\"))\n",
    "\n",
    "acc, mf1, sec, err = eval_attack_latent_on_subset(pgd, lat_val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE, \"PGD\")\n",
    "results.append({\"Model\": \"Latent-Transformer\", \"Condition\": \"PGD\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec, \"Error\": err or \"\"})\n",
    "print(f\"  PGD          acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\" + (f\"  [ERR: {err}]\" if err else \"\"))\n",
    "\n",
    "acc, mf1, sec, err = eval_attack_latent_on_subset(hsj, lat_val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE, \"HopSkipJump\")\n",
    "results.append({\"Model\": \"Latent-Transformer\", \"Condition\": \"HopSkipJump\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec, \"Error\": err or \"\"})\n",
    "print(f\"  HopSkipJump  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\" + (f\"  [ERR: {err}]\" if err else \"\"))\n",
    "\n",
    "acc, mf1, sec, err = eval_attack_latent_on_subset(bnd, lat_val_ds, ADV_MAX_SAMPLES, ADV_BATCH_SIZE, \"Boundary\")\n",
    "results.append({\"Model\": \"Latent-Transformer\", \"Condition\": \"Boundary\", \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec, \"Error\": err or \"\"})\n",
    "print(f\"  Boundary     acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\" + (f\"  [ERR: {err}]\" if err else \"\"))\n",
    "\n",
    "df = pd.DataFrame(results).sort_values([\"Condition\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "out_csv = \"malevis_latent_transformer_noise_adv_results.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f\"\\nSaved: {out_csv}\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c30dbf8-9b07-419e-9385-c8e359337a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Found 9100 files belonging to 26 classes.\n",
      "Found 5126 files belonging to 26 classes.\n",
      "Found 26 classes.\n",
      "First 5 classes: ['Adposhel', 'Agent', 'Allaple', 'Amonetize', 'Androm']\n",
      "\n",
      "Training VAE...\n",
      "WARNING:tensorflow:From C:\\Users\\ajayi\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 380ms/step - kl: 7292.0933 - loss: 2153.6316 - recon_sse: 2153.6316 - val_kl: 32166.6582 - val_loss: 1675.5875 - val_recon_sse: 1675.5875\n",
      "Epoch 2/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 403ms/step - kl: 674.1396 - loss: 1892.6906 - recon_sse: 1825.2753 - val_kl: 182.0198 - val_loss: 1424.3887 - val_recon_sse: 1406.1868\n",
      "Epoch 3/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 392ms/step - kl: 137.8045 - loss: 1441.4247 - recon_sse: 1413.8641 - val_kl: 118.7917 - val_loss: 1346.3872 - val_recon_sse: 1322.6282\n",
      "Epoch 4/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 367ms/step - kl: 102.0763 - loss: 1276.3553 - recon_sse: 1245.7322 - val_kl: 97.4291 - val_loss: 1290.7113 - val_recon_sse: 1261.4829\n",
      "Epoch 5/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 366ms/step - kl: 89.1308 - loss: 1238.1476 - recon_sse: 1202.4957 - val_kl: 74.8185 - val_loss: 1247.9330 - val_recon_sse: 1218.0068\n",
      "Epoch 6/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 367ms/step - kl: 76.7712 - loss: 1173.1827 - recon_sse: 1134.7975 - val_kl: 69.2799 - val_loss: 1224.7887 - val_recon_sse: 1190.1476\n",
      "Epoch 7/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 369ms/step - kl: 68.6497 - loss: 1117.7806 - recon_sse: 1076.5906 - val_kl: 66.2265 - val_loss: 1219.5767 - val_recon_sse: 1179.8402\n",
      "Epoch 8/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 369ms/step - kl: 65.5469 - loss: 1124.1141 - recon_sse: 1078.2306 - val_kl: 60.0481 - val_loss: 1186.5900 - val_recon_sse: 1144.5563\n",
      "Epoch 9/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 400ms/step - kl: 58.9251 - loss: 1078.1801 - recon_sse: 1031.0396 - val_kl: 58.3651 - val_loss: 1237.4225 - val_recon_sse: 1190.7312\n",
      "Epoch 10/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 377ms/step - kl: 54.7230 - loss: 1071.8125 - recon_sse: 1022.5613 - val_kl: 50.4947 - val_loss: 1190.3712 - val_recon_sse: 1144.9257\n",
      "Epoch 11/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 372ms/step - kl: 60.8546 - loss: 1199.3597 - recon_sse: 1138.5048 - val_kl: 50.7810 - val_loss: 1178.4199 - val_recon_sse: 1127.6404\n",
      "Epoch 12/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 351ms/step - kl: 50.7226 - loss: 1044.6899 - recon_sse: 993.9675 - val_kl: 50.9505 - val_loss: 1177.1001 - val_recon_sse: 1126.1484\n",
      "Epoch 13/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 339ms/step - kl: 48.7020 - loss: 1025.9304 - recon_sse: 977.2286 - val_kl: 47.3946 - val_loss: 1170.6338 - val_recon_sse: 1123.2401\n",
      "Epoch 14/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 340ms/step - kl: 56.1201 - loss: 1167.0452 - recon_sse: 1110.9250 - val_kl: 47.0165 - val_loss: 1154.9070 - val_recon_sse: 1107.8910\n",
      "Epoch 15/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 355ms/step - kl: 47.9408 - loss: 1016.8591 - recon_sse: 968.9188 - val_kl: 46.2533 - val_loss: 1148.7463 - val_recon_sse: 1102.4932\n",
      "Epoch 16/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 342ms/step - kl: 46.6126 - loss: 1005.1119 - recon_sse: 958.4993 - val_kl: 45.6035 - val_loss: 1140.8635 - val_recon_sse: 1095.2594\n",
      "Epoch 17/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 341ms/step - kl: 46.2658 - loss: 997.6463 - recon_sse: 951.3801 - val_kl: 42.2146 - val_loss: 1128.5005 - val_recon_sse: 1086.2860\n",
      "Epoch 18/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 340ms/step - kl: 45.5396 - loss: 992.7695 - recon_sse: 947.2295 - val_kl: 46.7372 - val_loss: 1135.4537 - val_recon_sse: 1088.7163\n",
      "Epoch 19/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 341ms/step - kl: 44.2951 - loss: 980.8831 - recon_sse: 936.5878 - val_kl: 44.3385 - val_loss: 1120.1848 - val_recon_sse: 1075.8452\n",
      "Epoch 20/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 341ms/step - kl: 44.0315 - loss: 981.6013 - recon_sse: 937.5698 - val_kl: 42.1793 - val_loss: 1123.2047 - val_recon_sse: 1081.0258\n",
      "Epoch 21/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 338ms/step - kl: 43.2916 - loss: 976.1201 - recon_sse: 932.8289 - val_kl: 41.8917 - val_loss: 1115.6930 - val_recon_sse: 1073.8005\n",
      "Epoch 22/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 338ms/step - kl: 43.2675 - loss: 972.7249 - recon_sse: 929.4574 - val_kl: 41.3014 - val_loss: 1109.8286 - val_recon_sse: 1068.5275\n",
      "Epoch 23/25\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 337ms/step - kl: 42.7617 - loss: 969.3327 - recon_sse: 926.5712 - val_kl: 41.5554 - val_loss: 1111.5564 - val_recon_sse: 1070.0017\n",
      "Epoch 24/25\n",
      "\u001b[1m 585/1138\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2:42\u001b[0m 294ms/step - kl: 41.6438 - loss: 949.1204 - recon_sse: 907.4763"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f4ca2788fd4e208d065de613da1bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21ffab4374c4fbdabb8905ec6b4a4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a15ac1c73434130803512602db798cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46c2468a7be46f3a540a22a03d949ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7b5e06ae664d43abeead59e825db05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4297d217aa484f83941ba4e96dfc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cd0278349e46f0bb836cdb1284c3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c947a9de9d014cdcb6dbece09894bb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c608c6b2445b453fa8fd6ee294981219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75af359c02d47b4a647cea4a00a2ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff39edcbaeef420d828f3ff3fb1bc468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613424f1bab741e7a8c4aab3b9d94c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c910d9fdd60451faf373e09117471b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338026945ee1431f9aa92ad8e154e084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240b74e07dde4c3eb1d49f580d22d47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fa474ab9964cd0858ffd4b8ff7d181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2b79ee4ba84f5e853fcbbe95377596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8040cb53fe794c529879e494ee8b0969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7ccea01cb645989271528e64ba56ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab6e44110df4625a385082aec7ff541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b224e0c68fb44c0e93ee5a7ff79b8844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6c892f0ece46d2812dbcbcff5c7cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497208a48f954991bef52319490e7f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6409b7c20f4440a899d00f8fc53b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bdb4a246424a199337b8230e9e12b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a426298e685c4810803ac27a239f5b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c5eff527ed4ec487e0ce3fe2f6992d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2a4fa266ca4507820a63100e48fde2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38586da101f442eaaaffc849eabc470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524bb46aa9dc4d28938bde4c66058285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a92bd92083420a8d386e40145b4a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b24cd08802f46d29d22d49eafaa2edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8bc45070024891b1e3824dc647d0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df2ced296d4480f8bec62f81c61a564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad36a83321d41d5886a9292f6280dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7078be8a814c7da0642c979ea5d839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17d1ec4f0fc487db89fd80c8c35e04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52de874bdd2343f89efa3e05966671cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783693f45b8a4c6c80811b7f5447585c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1916fad9e6094f0d89557427395e6e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ff2be2bbe847d99d0840e374223f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ef9f4a7f7a4f3589147c38821d9fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6621c440f4414be4bae6e01bfbc3d417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f858c567164acbaec971f102e05359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02f832efa614a1e8c2928e577e9532e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a951cfa14b8c406da2620c67498fb33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b2cd5bdca54bc4806ac2e2505775a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d1257bf97e4af888fdc4f76a190cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf6cab9b146408e9aeeb56a03070d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358f440d8b6f4c9d942954c4200b4a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edad1afb57349faaec18989c994585d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4050d6fe8f5d415cbefcce1edd336d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f336e48c89ee4b2c938a78bba3343e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58d65f6003f4cc1989cff5b825c72c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b171a2c3984253a72460932678925e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b2d4903b524005bc11e2d50aa8ef90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c623c9a6e174609ba753d15399a93ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3ea699c6404600a4f7a19e0b2eb12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd249c807e5a4090ab2c49ebf6846613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021110a0c9714536846baad23d895549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340476aaed5248cfbc9f0ad4da4cbb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f51b47c945f452fb1f9bcfc4b971099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9f2d9f811645299db2a2978a41e09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e8de835c164b5b8447c254a29fbc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a817357ffb6540119a4f05da11f5a1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed736fe16b246b38fb8bdc29ddd1c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee562d94ffb94fe4ab3e82122e46957a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737605d0ccb949d6b2a0a0de6e780b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1e12f62720498b8196e443c9d730f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c513ad4aec4cc28d21a85f1a963f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdec32718c24577aadbe2a2f4a564b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b9ce503cf84c44b9751ba09324c76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f619834393564ab3a02ffbbb2dc79cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8163131531a44b45b3cc7335e7495f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b921f85afaba49b193d9f7cfc4ae47ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d64b55a4376466ea337555100edf2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5705b1e1840d4fbabdca70659035aa3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5c423f10624411b0410549ccbf01e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12353415f9034875995ec842d230e438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c3f4e08fb24c2087d9ccf861797d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a44d62c815042d08d6691f7b4a89f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f642b3d58ce448c9c469c657081a022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4550faa4d6b04b8cae7aabbf4e2e6b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180fb6074f0143ffbc6dfc0b3c4c4e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d74a29f32654c7a93c562587165e341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1450768d1f4546d0aba25b29a22b04dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23d51fc0a7e4e2caaddc7e1630db45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eba2bac2bf144d0a9de014da5da5a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fb71fe88594ae8bdc28cf210cacc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a3ccc69bd14f4fae7896c023ab0992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724fde56d5234378aaaef845b2e12004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71545012eb14083a84f323c1aba7251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156cae10cfee475bb774544c999fc171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdfbb9dd5ce4862aed6963fe1c9f302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69c8ab04bc44632becbe21f28571c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0850f8b4aa234deaa5e2071cab66c685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f5ddfa048949089fa11a118d20fa68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aba82425b9d43278b5f0e5bae7389b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bbee9e1bb34c5e81d10f662311f39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26325119672748789b1a4fd640ba6395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370cf15d12964bb5995cd65c3fcf470a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318760965f394d8baeed855dc31ad82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cd5cbbd89849b289667887dfe151c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2df93eb74742e6b2eb3bc814027918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb384a66df94e979050c7fd9d06ffe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d730577776b40c1bef2d885a5e48f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc904de2a7964afb9cf3f470e41183ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0c2a79556942f2902fb17721b77b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e753d370d6b46e8a0dbe3a4dd984187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7079c436af4e1e84766bb3f41fd218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9096581e3424d5897fe920d497c61af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729971c200bf4f96aeda828a13662497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1c005bd24444dcb04cfab37e4ee8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff318dd8823463d854033d478942162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18221a841d684f91a89705a0f4cc4e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c54e292a0274f678bdf52c1d15dd61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e1e841f6ac458b8ca11ce6d471ef60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5975fee3f24d229a35f53dd1640ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36af7255438446d6a8e2d748e6f90d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed73af6bd4344fff8e681bdd5d7a2f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0216cc973e0d4d33bff7dbce557def34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Boundary attack - iterations:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary      acc=0.0234  macroF1=0.0050  time=11904.8s\n",
      "\n",
      "================================================================================\n",
      "RESULTS:\n",
      "             Model   Condition  Accuracy  Macro-F1      Time(s)\n",
      "Latent-Transformer    Boundary  0.023438  0.004965 11904.796096\n",
      "Latent-Transformer       Clean  0.715763  0.739484     6.223179\n",
      "Latent-Transformer     Dropout  0.607491  0.632248     6.350265\n",
      "Latent-Transformer        FGSM  0.000000  0.000000     2.975788\n",
      "Latent-Transformer    Gaussian  0.692353  0.718183     6.250838\n",
      "Latent-Transformer HopSkipJump  0.007812  0.001769  6546.147692\n",
      "Latent-Transformer         PGD  0.000000  0.000000    48.408241\n",
      "Latent-Transformer Salt-Pepper  0.667382  0.686902     6.435498\n",
      "Latent-Transformer     Uniform  0.712641  0.737240     6.222651\n",
      "\n",
      "Saved: malevis_latent_transformer_noise_adv_results.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "MaleVis: VAE -> Latent Transformer (TF2)\n",
    "+ Latent perturbations (Gaussian/Uniform/Dropout/SaltPepper)\n",
    "+ Adversarial attacks (FGSM/PGD/HopSkipJump/Boundary) via ART TensorFlowV2Classifier\n",
    "\n",
    "Key points:\n",
    "- FIXED label mapping: val uses class_names from train.\n",
    "- Streaming tf.data (NO ds_to_numpy for full dataset).\n",
    "- Latents are cached (small) to avoid recomputing encoder.\n",
    "- Augmentations are applied AFTER caching so they're NOT frozen.\n",
    "- ART uses TensorFlowV2Classifier (works with TF2 eager).\n",
    "- HSJ/Boundary run on ADV_MAX_SAMPLES only (black-box attacks are slow).\n",
    "\n",
    "Install once (same env) then restart kernel:\n",
    "  pip install adversarial-robustness-toolbox\n",
    "\"\"\"\n",
    "\n",
    "import os, time, math, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# =========================================================\n",
    "# 0) Config\n",
    "# =========================================================\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = r\"C:\\Users\\ajayi\\Downloads\\malevis_train_val_300x300\\malevis_train_val_300x300\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n",
    "\n",
    "IMG_H, IMG_W, CH = 128, 128, 3   # keep 300 if you want; 128/224 is faster\n",
    "LATENT_DIM = 64\n",
    "\n",
    "# VAE\n",
    "VAE_EPOCHS = 25\n",
    "VAE_BS     = 8\n",
    "VAE_LR     = 1e-3\n",
    "BETA_MAX = 1.0\n",
    "BETA_WARMUP_EPOCHS = 10\n",
    "\n",
    "# Latent classifier (Transformer)\n",
    "CLS_EPOCHS = 20\n",
    "CLS_BS     = 128\n",
    "CLS_LR     = 1e-3\n",
    "\n",
    "# Latent noise (evaluation)\n",
    "NOISE_GAUSS_SIG = 0.15\n",
    "NOISE_UNIF_RNG  = 0.15\n",
    "NOISE_DROPOUT   = 0.25\n",
    "NOISE_SP_PROB   = 0.02\n",
    "\n",
    "# Latent augmentation mixture during classifier training\n",
    "# (probabilities for [Clean, Gaussian, Uniform, Dropout, SaltPepper])\n",
    "P_CLEAN = 0.35\n",
    "P_GAUSS = 0.20\n",
    "P_UNIF  = 0.20\n",
    "P_DROP  = 0.15\n",
    "P_SP    = 0.10\n",
    "\n",
    "# Adversarial subset sizes (IMPORTANT)\n",
    "ADV_MAX_SAMPLES = 256     # increase slowly\n",
    "ADV_BATCH_SIZE  = 32\n",
    "\n",
    "# FGSM/PGD (latent L∞)\n",
    "FGSM_EPS_LAT  = 0.35\n",
    "PGD_EPS_LAT   = 0.50\n",
    "PGD_ALPHA_LAT = 0.08\n",
    "PGD_ITERS_LAT = 15\n",
    "\n",
    "# HSJ / Boundary (black-box) — slow\n",
    "HSJ_MAX_ITER    = 20\n",
    "HSJ_MAX_EVAL    = 2000\n",
    "HSJ_INIT_EVAL   = 50\n",
    "HSJ_INIT_SIZE   = 50\n",
    "\n",
    "BOUNDARY_MAX_ITER = 200\n",
    "BOUNDARY_DELTA    = 0.01\n",
    "BOUNDARY_EPS      = 0.01\n",
    "\n",
    "# Transformer size\n",
    "D_MODEL   = 128\n",
    "NUM_HEADS = 4\n",
    "FF_DIM    = 256\n",
    "N_LAYERS  = 2\n",
    "DROPOUT   = 0.15\n",
    "\n",
    "# GPU memory growth\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) Dataset loading (FIXED mapping)\n",
    "# =========================================================\n",
    "def make_ds_fixed(train_dir, val_dir, img_size, batch_size, seed):\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    def norm(x, y):\n",
    "        x = tf.cast(x, tf.float32) / 255.0\n",
    "        return x, y\n",
    "\n",
    "    train_ds = train_ds.map(norm, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = val_ds.map(norm,   num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds, class_names, len(class_names)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_ds_vae, val_ds_vae, class_names, num_classes = make_ds_fixed(\n",
    "    TRAIN_DIR, VAL_DIR, (IMG_H, IMG_W), batch_size=VAE_BS, seed=SEED\n",
    ")\n",
    "print(f\"Found {len(class_names)} classes.\")\n",
    "print(\"First 5 classes:\", class_names[:5])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) Conv VAE (subclassed, KL warmup)\n",
    "# =========================================================\n",
    "def _compute_decoder_grid(img_h, img_w, downsamples=4):\n",
    "    div = 2 ** downsamples\n",
    "    gh = int(math.ceil(img_h / div))\n",
    "    gw = int(math.ceil(img_w / div))\n",
    "    out_h = gh * div\n",
    "    out_w = gw * div\n",
    "    return gh, gw, out_h, out_w\n",
    "\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 4, strides=2, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.Conv2D(64, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(128, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(256, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    return tf.keras.Model(inp, [z_mean, z_log_var], name=\"encoder\")\n",
    "\n",
    "def build_decoder(output_shape, latent_dim):\n",
    "    img_h, img_w, ch = output_shape\n",
    "    gh, gw, out_h, out_w = _compute_decoder_grid(img_h, img_w, downsamples=4)\n",
    "\n",
    "    inp = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(gh * gw * 256, activation=\"relu\")(inp)\n",
    "    x = layers.Reshape((gh, gw, 256))(x)\n",
    "    x = layers.Conv2DTranspose(256, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(128, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(64, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(32, 4, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(ch, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "\n",
    "    crop_h = max(0, out_h - img_h)\n",
    "    crop_w = max(0, out_w - img_w)\n",
    "    if crop_h > 0 or crop_w > 0:\n",
    "        top = crop_h // 2\n",
    "        bot = crop_h - top\n",
    "        left = crop_w // 2\n",
    "        right = crop_w - left\n",
    "        x = layers.Cropping2D(cropping=((top, bot), (left, right)))(x)\n",
    "\n",
    "    return tf.keras.Model(inp, x, name=\"decoder\")\n",
    "\n",
    "class ConvVAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, logvar_clip=10.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
    "        self.logvar_clip = float(logvar_clip)\n",
    "\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.recon_tracker = tf.keras.metrics.Mean(name=\"recon_sse\")\n",
    "        self.kl_tracker = tf.keras.metrics.Mean(name=\"kl\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.recon_tracker, self.kl_tracker]\n",
    "\n",
    "    def _sample(self, z_mean, z_log_var):\n",
    "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(x, training=True)\n",
    "            z_log_var = tf.clip_by_value(z_log_var, -self.logvar_clip, self.logvar_clip)\n",
    "            z = self._sample(z_mean, z_log_var)\n",
    "            x_hat = self.decoder(z, training=True)\n",
    "\n",
    "            recon_sse = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=[1,2,3]))\n",
    "            kl = tf.reduce_mean(\n",
    "                -0.5 * tf.reduce_sum(1.0 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "            )\n",
    "            loss = recon_sse + self.beta * kl\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.recon_tracker.update_state(recon_sse)\n",
    "        self.kl_tracker.update_state(kl)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x = data\n",
    "        z_mean, z_log_var = self.encoder(x, training=False)\n",
    "        z_log_var = tf.clip_by_value(z_log_var, -self.logvar_clip, self.logvar_clip)\n",
    "        z = self._sample(z_mean, z_log_var)\n",
    "        x_hat = self.decoder(z, training=False)\n",
    "\n",
    "        recon_sse = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=[1,2,3]))\n",
    "        kl = tf.reduce_mean(\n",
    "            -0.5 * tf.reduce_sum(1.0 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        loss = recon_sse + self.beta * kl\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.recon_tracker.update_state(recon_sse)\n",
    "        self.kl_tracker.update_state(kl)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def encode_mean(self, x):\n",
    "        z_mean, _ = self.encoder(x, training=False)\n",
    "        return z_mean\n",
    "\n",
    "class BetaWarmup(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, beta_max=1.0, warmup_epochs=10):\n",
    "        super().__init__()\n",
    "        self.beta_max = float(beta_max)\n",
    "        self.warmup_epochs = int(max(1, warmup_epochs))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        beta = self.beta_max * min(1.0, epoch / float(self.warmup_epochs))\n",
    "        self.model.beta.assign(beta)\n",
    "\n",
    "print(\"\\nTraining VAE...\")\n",
    "tf.keras.backend.clear_session()\n",
    "encoder = build_encoder((IMG_H, IMG_W, CH), LATENT_DIM)\n",
    "decoder = build_decoder((IMG_H, IMG_W, CH), LATENT_DIM)\n",
    "vae = ConvVAE(encoder, decoder, name=\"vae\")\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(VAE_LR))\n",
    "\n",
    "vae_train_img = train_ds_vae.map(lambda x, y: x)\n",
    "vae_val_img   = val_ds_vae.map(lambda x, y: x)\n",
    "\n",
    "vae.fit(\n",
    "    vae_train_img,\n",
    "    validation_data=vae_val_img,\n",
    "    epochs=VAE_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        BetaWarmup(beta_max=BETA_MAX, warmup_epochs=BETA_WARMUP_EPOCHS),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Freeze encoder for classification\n",
    "vae.encoder.trainable = False\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) Latent datasets (cache clean latents)\n",
    "# =========================================================\n",
    "def make_image_ds(dir_path, class_names, img_size, batch_size, shuffle, seed):\n",
    "    ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dir_path,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=seed if shuffle else None,\n",
    "        class_names=class_names\n",
    "    )\n",
    "    ds = ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_img_cls = make_image_ds(TRAIN_DIR, class_names, (IMG_H, IMG_W), CLS_BS, shuffle=True, seed=SEED)\n",
    "val_img_cls   = make_image_ds(VAL_DIR,   class_names, (IMG_H, IMG_W), CLS_BS, shuffle=False, seed=SEED)\n",
    "\n",
    "def make_latent_ds(image_ds, vae, cache=True, shuffle=False):\n",
    "    def to_latent(x, y):\n",
    "        z = vae.encode_mean(x)          # (B,k)\n",
    "        z = tf.expand_dims(z, axis=-1)  # (B,k,1)\n",
    "        return z, y\n",
    "    out = image_ds.map(to_latent, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if cache:\n",
    "        out = out.cache()\n",
    "    if shuffle:\n",
    "        out = out.shuffle(4096, seed=SEED, reshuffle_each_iteration=True)\n",
    "    return out.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "lat_train_clean = make_latent_ds(train_img_cls, vae, cache=True, shuffle=True)\n",
    "lat_val_clean   = make_latent_ds(val_img_cls,   vae, cache=True, shuffle=False)\n",
    "\n",
    "def estimate_latent_bounds(latent_ds, q_lo=0.5, q_hi=99.5, take_batches=50):\n",
    "    zs = []\n",
    "    for zb, _ in latent_ds.take(take_batches):\n",
    "        zs.append(tf.reshape(zb, [-1]).numpy())\n",
    "    zs = np.concatenate(zs, axis=0)\n",
    "    return float(np.percentile(zs, q_lo)), float(np.percentile(zs, q_hi))\n",
    "\n",
    "z_min, z_max = estimate_latent_bounds(lat_train_clean)\n",
    "print(\"\\nLatent clip bounds:\", z_min, z_max)\n",
    "\n",
    "def latent_std_check(latent_ds, take_batches=10):\n",
    "    zs = []\n",
    "    for zb, _ in latent_ds.take(take_batches):\n",
    "        z = tf.squeeze(zb, axis=-1).numpy()\n",
    "        zs.append(z)\n",
    "    Z = np.concatenate(zs, axis=0)\n",
    "    per_dim_std = Z.std(axis=0)\n",
    "    print(\"Latent std: mean=\", float(per_dim_std.mean()), \"min=\", float(per_dim_std.min()), \"max=\", float(per_dim_std.max()))\n",
    "\n",
    "latent_std_check(lat_train_clean)\n",
    "\n",
    "def clip_latent(z):\n",
    "    return tf.clip_by_value(z, z_min, z_max)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4) Latent perturbations + training-time mixture augmentation (SAFE)\n",
    "# =========================================================\n",
    "def latent_noise(z, kind):\n",
    "    if kind == \"Gaussian\":\n",
    "        z2 = z + tf.random.normal(tf.shape(z), stddev=NOISE_GAUSS_SIG)\n",
    "    elif kind == \"Uniform\":\n",
    "        z2 = z + tf.random.uniform(tf.shape(z), minval=-NOISE_UNIF_RNG, maxval=NOISE_UNIF_RNG)\n",
    "    elif kind == \"Dropout\":\n",
    "        mask = tf.cast(tf.random.uniform(tf.shape(z)) > NOISE_DROPOUT, tf.float32)\n",
    "        z2 = z * mask\n",
    "    elif kind == \"Salt-Pepper\":\n",
    "        rnd = tf.random.uniform(tf.shape(z))\n",
    "        z2 = tf.where(rnd < (NOISE_SP_PROB/2.0), tf.cast(z_min, tf.float32), z)\n",
    "        z2 = tf.where(rnd > 1.0 - (NOISE_SP_PROB/2.0), tf.cast(z_max, tf.float32), z2)\n",
    "    else:\n",
    "        z2 = z\n",
    "    return clip_latent(z2)\n",
    "\n",
    "# ---- fixed probabilities block (NO __probs typo)\n",
    "_probs = np.array([P_CLEAN, P_GAUSS, P_UNIF, P_DROP, P_SP], dtype=np.float32)\n",
    "_probs = _probs / max(_probs.sum(), 1e-8)\n",
    "PROBS_T = tf.constant(_probs, dtype=tf.float32)\n",
    "LOGITS_T = tf.math.log(PROBS_T + 1e-8)  # safe log\n",
    "\n",
    "# Apply exactly ONE condition per batch (no overlapping tf.case conditions)\n",
    "@tf.function\n",
    "def augment_latent_batch(z, y):\n",
    "    # sample one augmentation index for the whole batch\n",
    "    idx = tf.random.categorical(LOGITS_T[None, :], 1)[0, 0]  # 0..4\n",
    "    idx = tf.cast(idx, tf.int32)\n",
    "\n",
    "    def do_clean():\n",
    "        return z\n",
    "\n",
    "    def do_gauss():\n",
    "        return latent_noise(z, \"Gaussian\")\n",
    "\n",
    "    def do_unif():\n",
    "        return latent_noise(z, \"Uniform\")\n",
    "\n",
    "    def do_drop():\n",
    "        return latent_noise(z, \"Dropout\")\n",
    "\n",
    "    def do_sp():\n",
    "        return latent_noise(z, \"Salt-Pepper\")\n",
    "\n",
    "    z_aug = tf.switch_case(\n",
    "        idx,\n",
    "        branch_fns=[do_clean, do_gauss, do_unif, do_drop, do_sp]\n",
    "    )\n",
    "    return z_aug, y\n",
    "\n",
    "# IMPORTANT: augment AFTER caching so augmentation isn't frozen\n",
    "lat_train_aug = lat_train_clean.map(augment_latent_batch, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5) Transformer classifier on latent (logits output)\n",
    "# =========================================================\n",
    "def transformer_block(x, num_heads, key_dim, ff_dim, dropout):\n",
    "    attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=dropout)(x, x)\n",
    "    x = layers.Add()([x, attn])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    ff = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    ff = layers.Dropout(dropout)(ff)\n",
    "    ff = layers.Dense(x.shape[-1])(ff)\n",
    "\n",
    "    x = layers.Add()([x, ff])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x\n",
    "\n",
    "def build_latent_transformer(latent_dim, num_classes, d_model=128, num_heads=4, ff_dim=256, n_layers=2, dropout=0.1):\n",
    "    inp = layers.Input(shape=(latent_dim, 1), name=\"latent_in\")  # (B,64,1)\n",
    "\n",
    "    # project 1 -> d_model per token\n",
    "    x = layers.Dense(d_model)(inp)  # (B,64,d_model)\n",
    "\n",
    "    # positional embedding\n",
    "    pos = tf.range(start=0, limit=latent_dim, delta=1)\n",
    "    pos_emb = layers.Embedding(input_dim=latent_dim, output_dim=d_model)(pos)  # (64,d_model)\n",
    "    x = x + pos_emb[None, :, :]  # broadcast\n",
    "\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    key_dim = max(1, d_model // num_heads)\n",
    "    for _ in range(n_layers):\n",
    "        x = transformer_block(x, num_heads=num_heads, key_dim=key_dim, ff_dim=ff_dim, dropout=dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    logits = layers.Dense(num_classes, activation=None, name=\"logits\")(x)\n",
    "    model = tf.keras.Model(inp, logits, name=\"latent_transformer_logits\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(CLS_LR),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "clf = build_latent_transformer(\n",
    "    LATENT_DIM, num_classes,\n",
    "    d_model=D_MODEL, num_heads=NUM_HEADS, ff_dim=FF_DIM, n_layers=N_LAYERS, dropout=DROPOUT\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Latent-Transformer...\")\n",
    "clf.fit(\n",
    "    lat_train_aug,\n",
    "    validation_data=lat_val_clean,\n",
    "    epochs=CLS_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "print(\"\\nSanity check (Keras evaluate on CLEAN latent val):\", clf.evaluate(lat_val_clean, verbose=0))\n",
    "\n",
    "# quick prediction sanity: are we collapsed to one class?\n",
    "def pred_sanity(model, ds, take_batches=3):\n",
    "    preds = []\n",
    "    ys = []\n",
    "    for zb, yb in ds.take(take_batches):\n",
    "        logits = model(zb, training=False)\n",
    "        p = tf.argmax(logits, axis=1).numpy()\n",
    "        preds.append(p)\n",
    "        ys.append(yb.numpy())\n",
    "    p = np.concatenate(preds)\n",
    "    y = np.concatenate(ys)\n",
    "    vals, counts = np.unique(p, return_counts=True)\n",
    "    print(\"Pred sanity: unique predicted classes =\", len(vals), \" / \", num_classes)\n",
    "    print(\"Top predicted counts:\", sorted(counts, reverse=True)[:5])\n",
    "    print(\"Batch acc (rough):\", float((p == y).mean()))\n",
    "\n",
    "pred_sanity(clf, lat_val_clean)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6) Evaluation: Clean + perturbations (streaming)\n",
    "# =========================================================\n",
    "def eval_latent_condition(model, latent_ds, condition):\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    t0 = time.time()\n",
    "    for zb, yb in latent_ds:\n",
    "        if condition == \"Clean\":\n",
    "            z_eval = zb\n",
    "        else:\n",
    "            z_eval = latent_noise(zb, condition)\n",
    "        logits = model(z_eval, training=False)\n",
    "        pred = tf.argmax(logits, axis=1)\n",
    "        y_true_all.append(yb.numpy())\n",
    "        y_pred_all.append(pred.numpy())\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    mf1 = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    sec = float(time.time() - t0)\n",
    "    return acc, mf1, sec\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) ART attacks (FGSM / PGD / HSJ / Boundary) in latent space\n",
    "# =========================================================\n",
    "def require_art():\n",
    "    try:\n",
    "        import art  # noqa: F401\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"ART is required. Install then restart kernel:\\n\"\n",
    "            \"  pip install adversarial-robustness-toolbox\\n\"\n",
    "            f\"Original error: {e}\"\n",
    "        )\n",
    "\n",
    "require_art()\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, HopSkipJump, BoundaryAttack\n",
    "\n",
    "# ART prefers one-hot for categorical loss\n",
    "loss_obj = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=CLS_LR)\n",
    "\n",
    "art_clf = TensorFlowV2Classifier(\n",
    "    model=clf,\n",
    "    nb_classes=num_classes,\n",
    "    input_shape=(LATENT_DIM, 1),\n",
    "    loss_object=loss_obj,\n",
    "    optimizer=optimizer,\n",
    "    clip_values=(z_min, z_max),\n",
    ")\n",
    "\n",
    "fgsm = FastGradientMethod(estimator=art_clf, eps=FGSM_EPS_LAT)\n",
    "pgd  = ProjectedGradientDescent(\n",
    "    estimator=art_clf,\n",
    "    eps=PGD_EPS_LAT,\n",
    "    eps_step=PGD_ALPHA_LAT,\n",
    "    max_iter=PGD_ITERS_LAT,\n",
    "    targeted=False\n",
    ")\n",
    "\n",
    "def make_hsj(estimator):\n",
    "    try:\n",
    "        return HopSkipJump(\n",
    "            estimator=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=HSJ_MAX_ITER,\n",
    "            max_eval=HSJ_MAX_EVAL,\n",
    "            init_eval=HSJ_INIT_EVAL,\n",
    "            init_size=HSJ_INIT_SIZE,\n",
    "        )\n",
    "    except TypeError:\n",
    "        return HopSkipJump(\n",
    "            classifier=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=HSJ_MAX_ITER,\n",
    "            max_eval=HSJ_MAX_EVAL,\n",
    "            init_eval=HSJ_INIT_EVAL,\n",
    "            init_size=HSJ_INIT_SIZE,\n",
    "        )\n",
    "\n",
    "def make_boundary(estimator):\n",
    "    try:\n",
    "        return BoundaryAttack(\n",
    "            estimator=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=BOUNDARY_MAX_ITER,\n",
    "            delta=BOUNDARY_DELTA,\n",
    "            epsilon=BOUNDARY_EPS,\n",
    "        )\n",
    "    except TypeError:\n",
    "        return BoundaryAttack(\n",
    "            classifier=estimator,\n",
    "            targeted=False,\n",
    "            max_iter=BOUNDARY_MAX_ITER,\n",
    "            delta=BOUNDARY_DELTA,\n",
    "            epsilon=BOUNDARY_EPS,\n",
    "        )\n",
    "\n",
    "hsj = make_hsj(art_clf)\n",
    "bnd = make_boundary(art_clf)\n",
    "\n",
    "def take_latent_numpy_subset(latent_ds, max_samples):\n",
    "    Zs, Ys = [], []\n",
    "    n = 0\n",
    "    for zb, yb in latent_ds:\n",
    "        Zs.append(zb.numpy().astype(np.float32))\n",
    "        Ys.append(yb.numpy().astype(np.int64))\n",
    "        n += len(yb)\n",
    "        if n >= max_samples:\n",
    "            break\n",
    "    Z = np.concatenate(Zs, axis=0)[:max_samples]\n",
    "    y = np.concatenate(Ys, axis=0)[:max_samples]\n",
    "    y_oh = tf.keras.utils.to_categorical(y, num_classes=num_classes).astype(np.float32)\n",
    "    return Z, y, y_oh\n",
    "\n",
    "def eval_attack_subset(attack, latent_ds, max_samples, batch_size):\n",
    "    Z, y, y_oh = take_latent_numpy_subset(latent_ds, max_samples)\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        Z_adv = attack.generate(x=Z, y=y_oh, batch_size=batch_size)\n",
    "    except Exception:\n",
    "        Z_adv = attack.generate(x=Z, y=y, batch_size=batch_size)\n",
    "\n",
    "    logits = clf(tf.convert_to_tensor(Z_adv, dtype=tf.float32), training=False).numpy()\n",
    "    pred = logits.argmax(axis=1)\n",
    "\n",
    "    acc = float(accuracy_score(y, pred))\n",
    "    mf1 = float(f1_score(y, pred, average=\"macro\"))\n",
    "    sec = float(time.time() - t0)\n",
    "    return acc, mf1, sec\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8) Run all evals + save\n",
    "# =========================================================\n",
    "results = []\n",
    "\n",
    "print(\"\\nEvaluating Latent-Transformer (Clean + Noise)...\")\n",
    "for cond in [\"Clean\", \"Gaussian\", \"Uniform\", \"Dropout\", \"Salt-Pepper\"]:\n",
    "    acc, mf1, sec = eval_latent_condition(clf, lat_val_clean, cond)\n",
    "    results.append({\"Model\": \"Latent-Transformer\", \"Condition\": cond, \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "    print(f\"  {cond:12s}  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "print(f\"\\nEvaluating Latent-Transformer (Adversarial on first {ADV_MAX_SAMPLES})...\")\n",
    "for name, attack in [(\"FGSM\", fgsm), (\"PGD\", pgd), (\"HopSkipJump\", hsj), (\"Boundary\", bnd)]:\n",
    "    acc, mf1, sec = eval_attack_subset(attack, lat_val_clean, ADV_MAX_SAMPLES, ADV_BATCH_SIZE)\n",
    "    results.append({\"Model\": \"Latent-Transformer\", \"Condition\": name, \"Accuracy\": acc, \"Macro-F1\": mf1, \"Time(s)\": sec})\n",
    "    print(f\"  {name:12s}  acc={acc:.4f}  macroF1={mf1:.4f}  time={sec:.1f}s\")\n",
    "\n",
    "df = pd.DataFrame(results).sort_values([\"Condition\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "out_csv = \"malevis_latent_transformer_noise_adv_results.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f\"\\nSaved: {out_csv}\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b5d0e-cbc8-4f69-983d-83722a877965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
